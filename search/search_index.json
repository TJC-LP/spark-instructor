{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#spark-instructor","title":"Spark Instructor","text":"<p><code>spark-instructor</code> combines the capabilities of Apache Spark and the <code>instructor</code> library to enable AI-powered structured data generation within Spark SQL DataFrames.</p>"},{"location":"#overview","title":"Overview","text":"<p>This project aims to bridge the gap between large-scale data processing with Apache Spark and AI-driven content generation. By leveraging the <code>instructor</code> library's ability to work with various AI models (such as OpenAI, Anthropic, and Databricks), Spark Instructor allows users to create User-Defined Functions (UDFs) that generate structured, AI-powered columns in Spark SQL DataFrames.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>AI-Powered UDFs: Create Spark UDFs that utilize AI models to generate structured data.</li> <li>Multi-Provider Support: Work with various AI providers including OpenAI, Anthropic, and Databricks. Support for other providers will be added on an as-needed basis. Contributions for added provider support are highly encouraged.</li> <li>Type-Safe Responses: Utilize Pydantic models to ensure type safety and data validation for AI-generated content.</li> <li>Seamless Integration: Easily incorporate AI-generated columns into your existing Spark SQL workflows.</li> <li>Scalable Processing: Leverage Spark's distributed computing capabilities for processing large datasets with AI augmentation.</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Enhance datasets with AI-generated insights or summaries.</li> <li>Perform large-scale text classification or entity extraction.</li> <li>Generate structured metadata for unstructured text data.</li> <li>Create synthetic datasets for testing or machine learning purposes.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Run <code>pip install spark-instructor</code>, or <code>pip install spark-instructor[anthropic]</code> for Anthropic SDK support.</li> <li><code>spark-instructor</code> must be installed on the Spark driver and workers to generate working UDFs.</li> <li>Add the necessary environment variables and config to your spark environment (recommended). See the Spark documentation for more details.</li> <li>For OpenAI support, add <code>OPENAI_API_KEY=&lt;openai-api-key&gt;</code></li> <li>For Databricks support, add <code>DATABRICKS_HOST=&lt;databricks-host&gt;</code> and <code>DATABRICKS_TOKEN=&lt;databricks-token&gt;</code></li> <li>For Anthropic support, make sure <code>spark-instructor[anthropic]</code> is installed and add <code>ANTHROPIC_API_KEY=&lt;anthropic-api-key&gt;</code></li> <li>For Ollama support, run <code>init/init-ollama.sh</code> as an init script to install <code>ollama</code> on all nodes</li> </ol>"},{"location":"#examples","title":"Examples","text":"<p>The following example demonstrates a sample run using the provided <code>User</code> model. The <code>User</code> model is defined below: <pre><code>from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    \"\"\"A user.\"\"\"\n\n    name: str\n    age: int\n</code></pre> For proper Spark serialization, we import our Pydantic model so that it is accessible on the Spark workers. See Limitations for more details. <pre><code>import pyspark.sql.functions as f\nfrom pyspark.sql import SparkSession\n\nfrom spark_instructor import create_chat_completion_messages, instruct\nfrom spark_instructor.response_models import User\n\n# Create spark session \nspark = SparkSession.builder.getOrCreate()\n\n# Create a sample DataFrame\ndata = [\n    (\"Extract Jason is 25 years old.\",),\n    (\"Extract Emma is 30 years old.\",),\n    (\"Extract Michael is 42 years old.\",),\n]\n\n# Format content as chat messages\ndf = (\n   spark.createDataFrame(data, [\"content\"])\n   .withColumn(\n      \"messages\", \n      create_chat_completion_messages(\n         [\n            {\"role\": f.lit(\"system\"), \"content\": f.lit(\"You are a helpful assistant\")},\n            # We can pass columns or names as map values. Here, `\"content\"` represents the original `content` column.\n            {\"role\": f.lit(\"user\"), \"content\": \"content\"}\n         ]\n      )\n   )\n   .withColumn(\n      \"response\", \n      instruct(response_model=User, default_model=\"gpt-4o\")(f.col(\"messages\"))\n   )\n)\n\n# Run the parser\nresult_df = df.select(\n    \"content\",\n    \"messages\",\n    f.col(\"response.*\")\n)\nresult_df.show(truncate=False)\n</code></pre> <pre><code>+--------------------------------+---------------------------------------------------------------------------------------------------------------------------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|content                         |messages                                                                                                                         |user         |chat_completion                                                                                                                                                                                                                                               |\n+--------------------------------+---------------------------------------------------------------------------------------------------------------------------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Extract Jason is 25 years old.  |[{system, You are a helpful assistant, NULL, NULL, NULL, NULL}, {user, Extract Jason is 25 years old., NULL, NULL, NULL, NULL}]  |{Jason, 25}  |{chatcmpl-9uP7OLHFfh5Ykqn0WWe73PKPT53Os, [{stop, 0, NULL, {NULL, NULL, assistant, NULL, [{call_KWemzBDaMeJ4IUIlV0O615R7, {{\"name\":\"Jason\",\"age\":25}, User}, function}]}}], 1723229950, gpt-4o-2024-05-13, chat.completion, NULL, fp_3aa7262c27, {9, 70, 79}}  |\n|Extract Emma is 30 years old.   |[{system, You are a helpful assistant, NULL, NULL, NULL, NULL}, {user, Extract Emma is 30 years old., NULL, NULL, NULL, NULL}]   |{Emma, 30}   |{chatcmpl-9uP7PkPacKjBH6pf4IVGeKauaMvPg, [{stop, 0, NULL, {NULL, NULL, assistant, NULL, [{call_JPnuSpto99mOJMDdzaRtQtiw, {{\"name\":\"Emma\",\"age\":30}, User}, function}]}}], 1723229951, gpt-4o-2024-05-13, chat.completion, NULL, fp_3aa7262c27, {9, 70, 79}}   |\n|Extract Michael is 42 years old.|[{system, You are a helpful assistant, NULL, NULL, NULL, NULL}, {user, Extract Michael is 42 years old., NULL, NULL, NULL, NULL}]|{Michael, 42}|{chatcmpl-9uP7QUbUlY3ksplUHpDXQhYUPa3Lu, [{stop, 0, NULL, {NULL, NULL, assistant, NULL, [{call_ABCEAljoRqtTeKwMh6YfnfnC, {{\"name\":\"Michael\",\"age\":42}, User}, function}]}}], 1723229952, gpt-4o-2024-05-13, chat.completion, NULL, fp_3aa7262c27, {9, 70, 79}}|\n+--------------------------------+---------------------------------------------------------------------------------------------------------------------------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> Note that setting <code>response_model</code> is optional, and omitting the argument will result in a standard message. Furthermore, we can pass configuration parameters as PySpark columns. <pre><code>import pyspark.sql.functions as f\nfrom pyspark.sql import SparkSession\n\nfrom spark_instructor import create_chat_completion_messages, instruct\n\n# Create spark session\nspark = SparkSession.builder.getOrCreate()\n\n# Create a sample DataFrame\ndata = [\n    (\"Extract Jason is 25 years old.\",),\n    (\"Extract Emma is 30 years old.\",),\n    (\"Extract Michael is 42 years old.\",),\n]\n\n# Format content as chat messages\ndf = (\n    spark.createDataFrame(data, [\"content\"])\n    .withColumn(\n        \"messages\",\n        create_chat_completion_messages(\n            [\n                {\n                    \"role\": f.lit(\"system\"),\n                    \"content\": f.lit(\"You are a helpful assistant who speaks in latin\"),\n                },\n                {\"role\": f.lit(\"user\"), \"content\": \"content\"},\n            ]\n        ),\n    )\n    .withColumn(\n        \"response\",\n        instruct()(\n            f.col(\"messages\"), \n            # Pick OpenAI or Anthropic randomly (this could incorporate custom routing logic)\n            model=f.when(f.rand(seed=42) &gt; f.lit(0.5), f.lit(\"claude-3-5-sonnet-20240620\")).otherwise(f.lit(\"gpt-4o\")), \n            max_tokens=f.lit(400),\n            temperature=f.lit(1)\n        ),\n    )\n)\n\n# Run the parser\nresult_df = df.select(\"content\", \"messages\", f.col(\"response.*\"))\nresult_df.show(truncate=False)\n</code></pre> <pre><code>+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|content                         |messages                                                                                                                                             |chat_completion                                                                                                                                                                                                      |\n+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Extract Jason is 25 years old.  |[{system, You are a helpful assistant who speaks in latin, NULL, NULL, NULL, NULL}, {user, Extract Jason is 25 years old., NULL, NULL, NULL, NULL}]  |{msg_01Fd1w3FVa2PFHD2egVMsYNu, [{stop, 0, NULL, {Iason viginti quinque annos natus est., NULL, assistant, NULL, NULL}}], 1723245768, claude-3-5-sonnet-20240620, chat.completion, NULL, NULL, {16, 25, 41}}          |\n|Extract Emma is 30 years old.   |[{system, You are a helpful assistant who speaks in latin, NULL, NULL, NULL, NULL}, {user, Extract Emma is 30 years old., NULL, NULL, NULL, NULL}]   |{msg_01Le4ChgsBc4vpBEmJytN3wg, [{stop, 0, NULL, {Ecce extractum:\\n\\nEmma triginta annos nata est., NULL, assistant, NULL, NULL}}], 1723245770, claude-3-5-sonnet-20240620, chat.completion, NULL, NULL, {19, 25, 44}}|\n|Extract Michael is 42 years old.|[{system, You are a helpful assistant who speaks in latin, NULL, NULL, NULL, NULL}, {user, Extract Michael is 42 years old., NULL, NULL, NULL, NULL}]|{chatcmpl-9uTEXQRNSJZTfacsPD2XYfphvtMkj, [{stop, 0, NULL, {Extractum Micha\u00eblis est XLII annorum., NULL, assistant, NULL, NULL}}], 1723245769, gpt-4o-2024-05-13, chat.completion, NULL, fp_3aa7262c27, {11, 28, 39}} |\n+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre></p>"},{"location":"#limitations","title":"Limitations","text":"<p>Serialization of raw Python code in Spark is tricky. Spark UDFs attempt to pickle everything associated with functions, and it is known that Pydantic objects do not serialize well when defined on the driver level. However, when Pydantic classes are imported from another source, these seem to serialize perfectly fine. See this example for more details.</p> <p>Even if a Pydantic model is installed at the cluster level, it may not have a well-defined Spark schema. Try to avoid using <code>typing.Any</code> or <code>typing.Union</code> fields. Very complex types are not guaranteed to work.</p> <p>We plan to continue adding generalizable examples to the <code>response_models.py</code> module, but users should consider building private Pydantic libraries where necessary, and install those at the cluster level. We will continue to investigate workarounds in the meantime.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions to Spark Instructor! Please see our contributing guidelines for more information on how to get involved.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project builds upon the excellent work of the Apache Spark community, the creators of the <code>instructor</code> library, and the creators of the <code>sparkdantic</code> library.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We're excited that you're interested in contributing to Spark Instructor! This document outlines the process for contributing to this project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork this repository and clone locally </li> <li>Install poetry</li> <li>Run <code>poetry install</code></li> <li>Run <code>poetry run pre-commit install</code></li> <li>Set up your IDE environment using the created poetry environment</li> <li>Run <code>poetry run lint</code> to run a full linting suite as well as tests</li> <li>Run <code>poetry run mkdocs build</code> to build documentation locally</li> </ol>"},{"location":"contributing/#guidelines","title":"Guidelines","text":"<ul> <li>Create meaningful branches for all PRs (e.g. <code>update-openai-schema</code> or <code>add-email-response-model</code>)</li> <li>Make sure to add relevant tests for code changes</li> <li>Add sufficient documentation with Google-style docstrings</li> <li>Keep it simple</li> </ul>"},{"location":"license/","title":"MIT License","text":""},{"location":"license/#copyright-c-2024-tjc-lp","title":"Copyright (c) 2024 TJC L.P.","text":"<p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>client<ul> <li>base</li> </ul> </li> <li>completions<ul> <li>anthropic_completions</li> <li>base</li> <li>databricks_completions</li> <li>ollama_completions</li> <li>openai_completions</li> </ul> </li> <li>factory<ul> <li>anthropic_factory</li> <li>base</li> <li>databricks_factory</li> <li>ollama_factory</li> <li>openai_factory</li> </ul> </li> <li>registry</li> <li>response_models</li> <li>types<ul> <li>base</li> <li>openai_types</li> </ul> </li> <li>udf<ul> <li>instruct</li> <li>message_router</li> </ul> </li> <li>utils<ul> <li>env</li> <li>image</li> <li>prompt</li> <li>types</li> </ul> </li> </ul>"},{"location":"reference/registry/","title":"registry","text":"<p>Module for defining registries of clients.</p>"},{"location":"reference/registry/#registry.ClientRegistry","title":"<code>ClientRegistry</code>  <code>dataclass</code>","text":"<p>A registry for managing and accessing different client factories for various AI models.</p> <p>This class serves as a central repository for client factories, allowing easy registration, retrieval, and mapping of model names to their respective factories. It provides a flexible way to manage multiple AI model providers in a single application.</p> <p>Attributes:</p> Name Type Description <code>factories</code> <code>Dict[str, Type[ClientFactory]]</code> <p>A dictionary mapping model class names to their corresponding ClientFactory classes.</p> <code>model_map</code> <code>Dict[str, str]</code> <p>A dictionary mapping model name patterns to their corresponding model class names.</p> <p>The default configuration includes factories for Anthropic, OpenAI, Databricks, and Ollama, with mappings for common model name patterns.</p> Usage <pre><code>registry = ClientRegistry()\nopenai_factory = registry.get_factory_from_model(\"gpt-3.5-turbo\")\n</code></pre> Source code in <code>spark_instructor/registry.py</code> <pre><code>@dataclass\nclass ClientRegistry:\n    \"\"\"A registry for managing and accessing different client factories for various AI models.\n\n    This class serves as a central repository for client factories, allowing easy registration,\n    retrieval, and mapping of model names to their respective factories. It provides a flexible\n    way to manage multiple AI model providers in a single application.\n\n    Attributes:\n        factories (Dict[str, Type[ClientFactory]]): A dictionary mapping model class names\n            to their corresponding ClientFactory classes.\n        model_map (Dict[str, str]): A dictionary mapping model name patterns to their\n            corresponding model class names.\n\n    The default configuration includes factories for Anthropic, OpenAI, Databricks, and Ollama,\n    with mappings for common model name patterns.\n\n    Usage:\n        ```python\n        registry = ClientRegistry()\n        openai_factory = registry.get_factory_from_model(\"gpt-3.5-turbo\")\n        ```\n    \"\"\"\n\n    factories: Dict[str, Type[ClientFactory]] = field(default_factory=get_default_factories)\n    model_map: Dict[str, str] = field(\n        default_factory=lambda: {\n            \"databricks\": \"databricks\",\n            \"gpt\": \"openai\",\n            \"claude\": \"anthropic\",\n            \"llama\": \"ollama\",\n            \"o1\": \"o1\",\n        }\n    )\n\n    def register(self, model_class: str, factory_class: Type[ClientFactory]):\n        \"\"\"Register a new factory for a given model class.\n\n        This method allows adding new factory classes to the registry or overriding\n        existing ones.\n\n        Args:\n            model_class (str): The name of the model class (e.g., \"openai\", \"anthropic\").\n            factory_class (Type[ClientFactory]): The ClientFactory subclass to be registered.\n\n        Note:\n            The model_class is converted to lowercase before registration to ensure\n            case-insensitive lookups.\n\n        Example:\n            ```python\n            registry.register(\"custom_model\", CustomModelFactory)\n            ```\n        \"\"\"\n        self.factories[model_class.lower()] = factory_class\n\n    def get_factory(self, model_class: str) -&gt; Type[ClientFactory]:\n        \"\"\"Retrieve a factory class for a given model class.\n\n        This method looks up and returns the appropriate ClientFactory subclass\n        for the specified model class.\n\n        Args:\n            model_class (str): The name of the model class to look up.\n\n        Returns:\n            Type[ClientFactory]: The corresponding ClientFactory subclass.\n\n        Raises:\n            ValueError: If no factory is registered for the given model class.\n\n        Example:\n            ```python\n            openai_factory = registry.get_factory(\"openai\")\n            ```\n        \"\"\"\n        factory_class = self.factories.get(model_class.lower())\n        if not factory_class:\n            raise ValueError(f\"No factory registered for model class: {model_class}\")\n        return factory_class\n\n    def get_factory_from_model(self, model: str) -&gt; Type[ClientFactory]:\n        \"\"\"Retrieve a factory class based on a model name.\n\n        This method uses the model_map to determine the appropriate model class\n        for a given model name, then returns the corresponding factory.\n\n        Args:\n            model (str): The name of the model (e.g., \"gpt-4o\", \"claude-3-5-sonnet-20240620\").\n\n        Returns:\n            Type[ClientFactory]: The corresponding ClientFactory subclass.\n\n        Raises:\n            ValueError: If the model name doesn't match any known patterns in the model_map.\n\n        Example:\n            ```python\n            claude_factory = registry.get_factory_from_model(\"claude-3-5-sonnet-20240620\")\n            ```\n\n        Note:\n            This method performs a partial match on the model name. For example,\n            any model name containing \"gpt\" will be mapped to the OpenAI factory.\n        \"\"\"\n        for key, val in self.model_map.items():\n            if key in model:\n                return self.get_factory(val)\n        raise ValueError(f\"`{model}` does not have a mapped factory. Consider updating the `model_map`.\")\n</code></pre>"},{"location":"reference/registry/#registry.ClientRegistry.get_factory","title":"<code>get_factory(model_class)</code>","text":"<p>Retrieve a factory class for a given model class.</p> <p>This method looks up and returns the appropriate ClientFactory subclass for the specified model class.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>str</code> <p>The name of the model class to look up.</p> required <p>Returns:</p> Type Description <code>Type[ClientFactory]</code> <p>Type[ClientFactory]: The corresponding ClientFactory subclass.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no factory is registered for the given model class.</p> Example <pre><code>openai_factory = registry.get_factory(\"openai\")\n</code></pre> Source code in <code>spark_instructor/registry.py</code> <pre><code>def get_factory(self, model_class: str) -&gt; Type[ClientFactory]:\n    \"\"\"Retrieve a factory class for a given model class.\n\n    This method looks up and returns the appropriate ClientFactory subclass\n    for the specified model class.\n\n    Args:\n        model_class (str): The name of the model class to look up.\n\n    Returns:\n        Type[ClientFactory]: The corresponding ClientFactory subclass.\n\n    Raises:\n        ValueError: If no factory is registered for the given model class.\n\n    Example:\n        ```python\n        openai_factory = registry.get_factory(\"openai\")\n        ```\n    \"\"\"\n    factory_class = self.factories.get(model_class.lower())\n    if not factory_class:\n        raise ValueError(f\"No factory registered for model class: {model_class}\")\n    return factory_class\n</code></pre>"},{"location":"reference/registry/#registry.ClientRegistry.get_factory_from_model","title":"<code>get_factory_from_model(model)</code>","text":"<p>Retrieve a factory class based on a model name.</p> <p>This method uses the model_map to determine the appropriate model class for a given model name, then returns the corresponding factory.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model (e.g., \"gpt-4o\", \"claude-3-5-sonnet-20240620\").</p> required <p>Returns:</p> Type Description <code>Type[ClientFactory]</code> <p>Type[ClientFactory]: The corresponding ClientFactory subclass.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model name doesn't match any known patterns in the model_map.</p> Example <pre><code>claude_factory = registry.get_factory_from_model(\"claude-3-5-sonnet-20240620\")\n</code></pre> Note <p>This method performs a partial match on the model name. For example, any model name containing \"gpt\" will be mapped to the OpenAI factory.</p> Source code in <code>spark_instructor/registry.py</code> <pre><code>def get_factory_from_model(self, model: str) -&gt; Type[ClientFactory]:\n    \"\"\"Retrieve a factory class based on a model name.\n\n    This method uses the model_map to determine the appropriate model class\n    for a given model name, then returns the corresponding factory.\n\n    Args:\n        model (str): The name of the model (e.g., \"gpt-4o\", \"claude-3-5-sonnet-20240620\").\n\n    Returns:\n        Type[ClientFactory]: The corresponding ClientFactory subclass.\n\n    Raises:\n        ValueError: If the model name doesn't match any known patterns in the model_map.\n\n    Example:\n        ```python\n        claude_factory = registry.get_factory_from_model(\"claude-3-5-sonnet-20240620\")\n        ```\n\n    Note:\n        This method performs a partial match on the model name. For example,\n        any model name containing \"gpt\" will be mapped to the OpenAI factory.\n    \"\"\"\n    for key, val in self.model_map.items():\n        if key in model:\n            return self.get_factory(val)\n    raise ValueError(f\"`{model}` does not have a mapped factory. Consider updating the `model_map`.\")\n</code></pre>"},{"location":"reference/registry/#registry.ClientRegistry.register","title":"<code>register(model_class, factory_class)</code>","text":"<p>Register a new factory for a given model class.</p> <p>This method allows adding new factory classes to the registry or overriding existing ones.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>str</code> <p>The name of the model class (e.g., \"openai\", \"anthropic\").</p> required <code>factory_class</code> <code>Type[ClientFactory]</code> <p>The ClientFactory subclass to be registered.</p> required Note <p>The model_class is converted to lowercase before registration to ensure case-insensitive lookups.</p> Example <pre><code>registry.register(\"custom_model\", CustomModelFactory)\n</code></pre> Source code in <code>spark_instructor/registry.py</code> <pre><code>def register(self, model_class: str, factory_class: Type[ClientFactory]):\n    \"\"\"Register a new factory for a given model class.\n\n    This method allows adding new factory classes to the registry or overriding\n    existing ones.\n\n    Args:\n        model_class (str): The name of the model class (e.g., \"openai\", \"anthropic\").\n        factory_class (Type[ClientFactory]): The ClientFactory subclass to be registered.\n\n    Note:\n        The model_class is converted to lowercase before registration to ensure\n        case-insensitive lookups.\n\n    Example:\n        ```python\n        registry.register(\"custom_model\", CustomModelFactory)\n        ```\n    \"\"\"\n    self.factories[model_class.lower()] = factory_class\n</code></pre>"},{"location":"reference/registry/#registry.get_default_factories","title":"<code>get_default_factories()</code>","text":"<p>Get default factories based on whether anthropic is available.</p> Source code in <code>spark_instructor/registry.py</code> <pre><code>def get_default_factories() -&gt; Dict[str, Type[ClientFactory]]:\n    \"\"\"Get default factories based on whether anthropic is available.\"\"\"\n    if is_anthropic_available():\n        from spark_instructor.factory import AnthropicFactory\n\n        return {\n            \"anthropic\": AnthropicFactory,\n            \"openai\": OpenAIFactory,\n            \"databricks\": DatabricksFactory,\n            \"ollama\": OllamaFactory,\n            \"o1\": O1Factory,\n        }\n    return {\"openai\": OpenAIFactory, \"databricks\": DatabricksFactory, \"ollama\": OllamaFactory, \"o1\": O1Factory}\n</code></pre>"},{"location":"reference/response_models/","title":"response_models","text":"<p>Sample response models for parsing.</p>"},{"location":"reference/response_models/#response_models.TextResponse","title":"<code>TextResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A text response.</p> Source code in <code>spark_instructor/response_models.py</code> <pre><code>class TextResponse(BaseModel):\n    \"\"\"A text response.\"\"\"\n\n    text: str\n</code></pre>"},{"location":"reference/response_models/#response_models.User","title":"<code>User</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A user.</p> Source code in <code>spark_instructor/response_models.py</code> <pre><code>class User(BaseModel):\n    \"\"\"A user.\"\"\"\n\n    name: str\n    age: int\n</code></pre>"},{"location":"reference/client/","title":"client","text":"<p>A package for defining instructor clients.</p>"},{"location":"reference/client/#client.ModelClass","title":"<code>ModelClass</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of available model classes.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>class ModelClass(str, Enum):\n    \"\"\"Enumeration of available model classes.\"\"\"\n\n    OPENAI = \"openai\"\n    ANTHROPIC = \"anthropic\"\n    DATABRICKS = \"databricks\"\n    OLLAMA = \"ollama\"\n</code></pre>"},{"location":"reference/client/#client.get_anthropic_aclient","title":"<code>get_anthropic_aclient(mode=instructor.Mode.ANTHROPIC_JSON, base_url=None, api_key=None, enable_prompt_caching=False)</code>  <code>cached</code>","text":"<p>Get the async Anthropic client.</p> <p>Ensure that <code>ANTHROPIC_API_KEY</code> is set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_anthropic_aclient(\n    mode: instructor.Mode = instructor.Mode.ANTHROPIC_JSON,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    enable_prompt_caching: bool = False,\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the async Anthropic client.\n\n    Ensure that ``ANTHROPIC_API_KEY`` is set.\n    \"\"\"\n    try:\n        from anthropic import AsyncAnthropic\n    except ImportError:\n        raise ImportError(\n            \"Please install ``anthropic`` by running ``pip install anthropic`` \"\n            \"or ``pip install spark-instructor[anthropic]``\"\n        )\n\n    if not api_key:\n        assert_env_is_set(\"ANTHROPIC_API_KEY\")\n        return instructor.from_anthropic(AsyncAnthropic(), mode=mode, enable_prompt_caching=enable_prompt_caching)\n    return instructor.from_anthropic(\n        AsyncAnthropic(api_key=api_key, base_url=base_url), mode=mode, enable_prompt_caching=enable_prompt_caching\n    )\n</code></pre>"},{"location":"reference/client/#client.get_anthropic_client","title":"<code>get_anthropic_client(mode=instructor.Mode.ANTHROPIC_JSON, base_url=None, api_key=None, enable_prompt_caching=False)</code>  <code>cached</code>","text":"<p>Get the Anthropic client.</p> <p>Ensure that <code>ANTHROPIC_API_KEY</code> is set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_anthropic_client(\n    mode: instructor.Mode = instructor.Mode.ANTHROPIC_JSON,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    enable_prompt_caching: bool = False,\n) -&gt; instructor.Instructor:\n    \"\"\"Get the Anthropic client.\n\n    Ensure that ``ANTHROPIC_API_KEY`` is set.\n    \"\"\"\n    try:\n        from anthropic import Anthropic\n    except ImportError:\n        raise ImportError(\n            \"Please install ``anthropic`` by running ``pip install anthropic`` \"\n            \"or ``pip install spark-instructor[anthropic]``\"\n        )\n    if not api_key:\n        assert_env_is_set(\"ANTHROPIC_API_KEY\")\n        return instructor.from_anthropic(Anthropic(), mode=mode, enable_prompt_caching=enable_prompt_caching)\n    return instructor.from_anthropic(\n        Anthropic(api_key=api_key, base_url=base_url), mode=mode, enable_prompt_caching=enable_prompt_caching\n    )\n</code></pre>"},{"location":"reference/client/#client.get_async_instructor","title":"<code>get_async_instructor(model_class=None, mode=None, api_key=None, base_url=None)</code>","text":"<p>Get the instructor client based on the model class and mode.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>def get_async_instructor(\n    model_class: Optional[ModelClass] = None,\n    mode: Optional[instructor.Mode] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the instructor client based on the model class and mode.\"\"\"\n    if model_class is None:\n        # Use OpenAI by default\n        model_class = ModelClass.OPENAI\n    kwargs: Dict[str, Any] = dict(api_key=api_key, base_url=base_url)\n    if mode:\n        kwargs |= dict(mode=mode)\n    return MODEL_CLASS_ROUTE_ASYNC[model_class](**kwargs)\n</code></pre>"},{"location":"reference/client/#client.get_databricks_aclient","title":"<code>get_databricks_aclient(mode=instructor.Mode.MD_JSON, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the async databricks client.</p> <p>Unless passed as arguments, ensure that the <code>DATABRICKS_HOST</code> and <code>DATABRICKS_TOKEN</code> environment variables are set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_databricks_aclient(\n    mode: instructor.Mode = instructor.Mode.MD_JSON, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the async databricks client.\n\n    Unless passed as arguments,\n    ensure that the ``DATABRICKS_HOST`` and ``DATABRICKS_TOKEN`` environment variables are set.\n    \"\"\"\n    if base_url is None:\n        base_url = f\"{get_env_variable('DATABRICKS_HOST')}/serving-endpoints\"\n    if api_key is None:\n        api_key = get_env_variable(\"DATABRICKS_TOKEN\")\n    return instructor.from_openai(\n        AsyncOpenAI(api_key=api_key, base_url=base_url),\n        mode=mode,\n    )\n</code></pre>"},{"location":"reference/client/#client.get_databricks_client","title":"<code>get_databricks_client(mode=instructor.Mode.MD_JSON, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the databricks client.</p> <p>Unless passed as arguments, ensure that the <code>DATABRICKS_HOST</code> and <code>DATABRICKS_TOKEN</code> environment variables are set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_databricks_client(\n    mode: instructor.Mode = instructor.Mode.MD_JSON, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.Instructor:\n    \"\"\"Get the databricks client.\n\n    Unless passed as arguments,\n    ensure that the ``DATABRICKS_HOST`` and ``DATABRICKS_TOKEN`` environment variables are set.\n    \"\"\"\n    if base_url is None:\n        base_url = f\"{get_env_variable('DATABRICKS_HOST')}/serving-endpoints\"\n    if api_key is None:\n        api_key = get_env_variable(\"DATABRICKS_TOKEN\")\n    return instructor.from_openai(\n        OpenAI(api_key=api_key, base_url=base_url),\n        mode=mode,\n    )\n</code></pre>"},{"location":"reference/client/#client.get_instructor","title":"<code>get_instructor(model_class=None, mode=None, api_key=None, base_url=None)</code>","text":"<p>Get the instructor client based on the model class and mode.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>def get_instructor(\n    model_class: Optional[ModelClass] = None,\n    mode: Optional[instructor.Mode] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n) -&gt; instructor.Instructor:\n    \"\"\"Get the instructor client based on the model class and mode.\"\"\"\n    if model_class is None:\n        # Use OpenAI by default\n        model_class = ModelClass.OPENAI\n    kwargs: Dict[str, Any] = dict(api_key=api_key, base_url=base_url)\n    if mode:\n        kwargs |= dict(mode=mode)\n    return MODEL_CLASS_ROUTE[model_class](**kwargs)\n</code></pre>"},{"location":"reference/client/#client.get_ollama_aclient","title":"<code>get_ollama_aclient(mode=instructor.Mode.JSON, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the async Ollama client.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_ollama_aclient(\n    mode: instructor.Mode = instructor.Mode.JSON, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the async Ollama client.\"\"\"\n    if not base_url:\n        host = \"http://localhost:11434\"\n        base_url = f\"{host}/v1\"\n\n    return instructor.from_openai(\n        AsyncOpenAI(base_url=base_url, api_key=\"ollama\" if not api_key else api_key), mode=mode\n    )\n</code></pre>"},{"location":"reference/client/#client.get_ollama_client","title":"<code>get_ollama_client(mode=instructor.Mode.JSON, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the Ollama client.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_ollama_client(\n    mode: instructor.Mode = instructor.Mode.JSON, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.Instructor:\n    \"\"\"Get the Ollama client.\"\"\"\n    if not base_url:\n        host = \"http://localhost:11434\"\n        base_url = f\"{host}/v1\"\n\n    return instructor.from_openai(OpenAI(base_url=base_url, api_key=\"ollama\" if not api_key else api_key), mode=mode)\n</code></pre>"},{"location":"reference/client/#client.get_openai_aclient","title":"<code>get_openai_aclient(mode=instructor.Mode.TOOLS, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the async OpenAI client.</p> <p>Unless passes as an argument, Ensure that <code>OPENAI_API_KEY</code> is set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_openai_aclient(\n    mode: instructor.Mode = instructor.Mode.TOOLS, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the async OpenAI client.\n\n    Unless passes as an argument,\n    Ensure that ``OPENAI_API_KEY`` is set.\n    \"\"\"\n    if not api_key:\n        assert_env_is_set(\"OPENAI_API_KEY\")\n        return instructor.from_openai(AsyncOpenAI(base_url=base_url), mode=mode)\n    return instructor.from_openai(AsyncOpenAI(base_url=base_url, api_key=api_key), mode=mode)\n</code></pre>"},{"location":"reference/client/#client.get_openai_client","title":"<code>get_openai_client(mode=instructor.Mode.TOOLS, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the OpenAI client.</p> <p>Unless passes as an argument, Ensure that <code>OPENAI_API_KEY</code> is set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_openai_client(\n    mode: instructor.Mode = instructor.Mode.TOOLS, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.Instructor:\n    \"\"\"Get the OpenAI client.\n\n    Unless passes as an argument,\n    Ensure that ``OPENAI_API_KEY`` is set.\n    \"\"\"\n    if not api_key:\n        assert_env_is_set(\"OPENAI_API_KEY\")\n        return instructor.from_openai(OpenAI(base_url=base_url), mode=mode)\n    return instructor.from_openai(OpenAI(base_url=base_url, api_key=api_key), mode=mode)\n</code></pre>"},{"location":"reference/client/#client.infer_model_class","title":"<code>infer_model_class(model_name)</code>","text":"<p>Attempt to infer the model class from the model name.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>def infer_model_class(model_name: str) -&gt; ModelClass:\n    \"\"\"Attempt to infer the model class from the model name.\"\"\"\n    if \"databricks\" in model_name:\n        return ModelClass.DATABRICKS\n    elif \"gpt\" in model_name:\n        return ModelClass.OPENAI\n    elif \"claude\" in model_name:\n        return ModelClass.ANTHROPIC\n    elif \"llama\" in model_name:\n        return ModelClass.OLLAMA\n    raise ValueError(f\"Model name `{model_name}` does not match any of the available model classes.\")\n</code></pre>"},{"location":"reference/client/base/","title":"base","text":"<p>Module for handling client routing.</p> <p>Serves as a workaround for Spark serialization issues.</p>"},{"location":"reference/client/base/#client.base.ModelClass","title":"<code>ModelClass</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of available model classes.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>class ModelClass(str, Enum):\n    \"\"\"Enumeration of available model classes.\"\"\"\n\n    OPENAI = \"openai\"\n    ANTHROPIC = \"anthropic\"\n    DATABRICKS = \"databricks\"\n    OLLAMA = \"ollama\"\n</code></pre>"},{"location":"reference/client/base/#client.base.get_anthropic_aclient","title":"<code>get_anthropic_aclient(mode=instructor.Mode.ANTHROPIC_JSON, base_url=None, api_key=None, enable_prompt_caching=False)</code>  <code>cached</code>","text":"<p>Get the async Anthropic client.</p> <p>Ensure that <code>ANTHROPIC_API_KEY</code> is set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_anthropic_aclient(\n    mode: instructor.Mode = instructor.Mode.ANTHROPIC_JSON,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    enable_prompt_caching: bool = False,\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the async Anthropic client.\n\n    Ensure that ``ANTHROPIC_API_KEY`` is set.\n    \"\"\"\n    try:\n        from anthropic import AsyncAnthropic\n    except ImportError:\n        raise ImportError(\n            \"Please install ``anthropic`` by running ``pip install anthropic`` \"\n            \"or ``pip install spark-instructor[anthropic]``\"\n        )\n\n    if not api_key:\n        assert_env_is_set(\"ANTHROPIC_API_KEY\")\n        return instructor.from_anthropic(AsyncAnthropic(), mode=mode, enable_prompt_caching=enable_prompt_caching)\n    return instructor.from_anthropic(\n        AsyncAnthropic(api_key=api_key, base_url=base_url), mode=mode, enable_prompt_caching=enable_prompt_caching\n    )\n</code></pre>"},{"location":"reference/client/base/#client.base.get_anthropic_client","title":"<code>get_anthropic_client(mode=instructor.Mode.ANTHROPIC_JSON, base_url=None, api_key=None, enable_prompt_caching=False)</code>  <code>cached</code>","text":"<p>Get the Anthropic client.</p> <p>Ensure that <code>ANTHROPIC_API_KEY</code> is set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_anthropic_client(\n    mode: instructor.Mode = instructor.Mode.ANTHROPIC_JSON,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    enable_prompt_caching: bool = False,\n) -&gt; instructor.Instructor:\n    \"\"\"Get the Anthropic client.\n\n    Ensure that ``ANTHROPIC_API_KEY`` is set.\n    \"\"\"\n    try:\n        from anthropic import Anthropic\n    except ImportError:\n        raise ImportError(\n            \"Please install ``anthropic`` by running ``pip install anthropic`` \"\n            \"or ``pip install spark-instructor[anthropic]``\"\n        )\n    if not api_key:\n        assert_env_is_set(\"ANTHROPIC_API_KEY\")\n        return instructor.from_anthropic(Anthropic(), mode=mode, enable_prompt_caching=enable_prompt_caching)\n    return instructor.from_anthropic(\n        Anthropic(api_key=api_key, base_url=base_url), mode=mode, enable_prompt_caching=enable_prompt_caching\n    )\n</code></pre>"},{"location":"reference/client/base/#client.base.get_async_instructor","title":"<code>get_async_instructor(model_class=None, mode=None, api_key=None, base_url=None)</code>","text":"<p>Get the instructor client based on the model class and mode.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>def get_async_instructor(\n    model_class: Optional[ModelClass] = None,\n    mode: Optional[instructor.Mode] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the instructor client based on the model class and mode.\"\"\"\n    if model_class is None:\n        # Use OpenAI by default\n        model_class = ModelClass.OPENAI\n    kwargs: Dict[str, Any] = dict(api_key=api_key, base_url=base_url)\n    if mode:\n        kwargs |= dict(mode=mode)\n    return MODEL_CLASS_ROUTE_ASYNC[model_class](**kwargs)\n</code></pre>"},{"location":"reference/client/base/#client.base.get_databricks_aclient","title":"<code>get_databricks_aclient(mode=instructor.Mode.MD_JSON, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the async databricks client.</p> <p>Unless passed as arguments, ensure that the <code>DATABRICKS_HOST</code> and <code>DATABRICKS_TOKEN</code> environment variables are set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_databricks_aclient(\n    mode: instructor.Mode = instructor.Mode.MD_JSON, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the async databricks client.\n\n    Unless passed as arguments,\n    ensure that the ``DATABRICKS_HOST`` and ``DATABRICKS_TOKEN`` environment variables are set.\n    \"\"\"\n    if base_url is None:\n        base_url = f\"{get_env_variable('DATABRICKS_HOST')}/serving-endpoints\"\n    if api_key is None:\n        api_key = get_env_variable(\"DATABRICKS_TOKEN\")\n    return instructor.from_openai(\n        AsyncOpenAI(api_key=api_key, base_url=base_url),\n        mode=mode,\n    )\n</code></pre>"},{"location":"reference/client/base/#client.base.get_databricks_client","title":"<code>get_databricks_client(mode=instructor.Mode.MD_JSON, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the databricks client.</p> <p>Unless passed as arguments, ensure that the <code>DATABRICKS_HOST</code> and <code>DATABRICKS_TOKEN</code> environment variables are set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_databricks_client(\n    mode: instructor.Mode = instructor.Mode.MD_JSON, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.Instructor:\n    \"\"\"Get the databricks client.\n\n    Unless passed as arguments,\n    ensure that the ``DATABRICKS_HOST`` and ``DATABRICKS_TOKEN`` environment variables are set.\n    \"\"\"\n    if base_url is None:\n        base_url = f\"{get_env_variable('DATABRICKS_HOST')}/serving-endpoints\"\n    if api_key is None:\n        api_key = get_env_variable(\"DATABRICKS_TOKEN\")\n    return instructor.from_openai(\n        OpenAI(api_key=api_key, base_url=base_url),\n        mode=mode,\n    )\n</code></pre>"},{"location":"reference/client/base/#client.base.get_instructor","title":"<code>get_instructor(model_class=None, mode=None, api_key=None, base_url=None)</code>","text":"<p>Get the instructor client based on the model class and mode.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>def get_instructor(\n    model_class: Optional[ModelClass] = None,\n    mode: Optional[instructor.Mode] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n) -&gt; instructor.Instructor:\n    \"\"\"Get the instructor client based on the model class and mode.\"\"\"\n    if model_class is None:\n        # Use OpenAI by default\n        model_class = ModelClass.OPENAI\n    kwargs: Dict[str, Any] = dict(api_key=api_key, base_url=base_url)\n    if mode:\n        kwargs |= dict(mode=mode)\n    return MODEL_CLASS_ROUTE[model_class](**kwargs)\n</code></pre>"},{"location":"reference/client/base/#client.base.get_ollama_aclient","title":"<code>get_ollama_aclient(mode=instructor.Mode.JSON, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the async Ollama client.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_ollama_aclient(\n    mode: instructor.Mode = instructor.Mode.JSON, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the async Ollama client.\"\"\"\n    if not base_url:\n        host = \"http://localhost:11434\"\n        base_url = f\"{host}/v1\"\n\n    return instructor.from_openai(\n        AsyncOpenAI(base_url=base_url, api_key=\"ollama\" if not api_key else api_key), mode=mode\n    )\n</code></pre>"},{"location":"reference/client/base/#client.base.get_ollama_client","title":"<code>get_ollama_client(mode=instructor.Mode.JSON, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the Ollama client.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_ollama_client(\n    mode: instructor.Mode = instructor.Mode.JSON, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.Instructor:\n    \"\"\"Get the Ollama client.\"\"\"\n    if not base_url:\n        host = \"http://localhost:11434\"\n        base_url = f\"{host}/v1\"\n\n    return instructor.from_openai(OpenAI(base_url=base_url, api_key=\"ollama\" if not api_key else api_key), mode=mode)\n</code></pre>"},{"location":"reference/client/base/#client.base.get_openai_aclient","title":"<code>get_openai_aclient(mode=instructor.Mode.TOOLS, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the async OpenAI client.</p> <p>Unless passes as an argument, Ensure that <code>OPENAI_API_KEY</code> is set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_openai_aclient(\n    mode: instructor.Mode = instructor.Mode.TOOLS, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.AsyncInstructor:\n    \"\"\"Get the async OpenAI client.\n\n    Unless passes as an argument,\n    Ensure that ``OPENAI_API_KEY`` is set.\n    \"\"\"\n    if not api_key:\n        assert_env_is_set(\"OPENAI_API_KEY\")\n        return instructor.from_openai(AsyncOpenAI(base_url=base_url), mode=mode)\n    return instructor.from_openai(AsyncOpenAI(base_url=base_url, api_key=api_key), mode=mode)\n</code></pre>"},{"location":"reference/client/base/#client.base.get_openai_client","title":"<code>get_openai_client(mode=instructor.Mode.TOOLS, base_url=None, api_key=None)</code>  <code>cached</code>","text":"<p>Get the OpenAI client.</p> <p>Unless passes as an argument, Ensure that <code>OPENAI_API_KEY</code> is set.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>@lru_cache\ndef get_openai_client(\n    mode: instructor.Mode = instructor.Mode.TOOLS, base_url: Optional[str] = None, api_key: Optional[str] = None\n) -&gt; instructor.Instructor:\n    \"\"\"Get the OpenAI client.\n\n    Unless passes as an argument,\n    Ensure that ``OPENAI_API_KEY`` is set.\n    \"\"\"\n    if not api_key:\n        assert_env_is_set(\"OPENAI_API_KEY\")\n        return instructor.from_openai(OpenAI(base_url=base_url), mode=mode)\n    return instructor.from_openai(OpenAI(base_url=base_url, api_key=api_key), mode=mode)\n</code></pre>"},{"location":"reference/client/base/#client.base.infer_model_class","title":"<code>infer_model_class(model_name)</code>","text":"<p>Attempt to infer the model class from the model name.</p> Source code in <code>spark_instructor/client/base.py</code> <pre><code>def infer_model_class(model_name: str) -&gt; ModelClass:\n    \"\"\"Attempt to infer the model class from the model name.\"\"\"\n    if \"databricks\" in model_name:\n        return ModelClass.DATABRICKS\n    elif \"gpt\" in model_name:\n        return ModelClass.OPENAI\n    elif \"claude\" in model_name:\n        return ModelClass.ANTHROPIC\n    elif \"llama\" in model_name:\n        return ModelClass.OLLAMA\n    raise ValueError(f\"Model name `{model_name}` does not match any of the available model classes.\")\n</code></pre>"},{"location":"reference/completions/","title":"completions","text":"<p>A package for defining completion object models.</p> <p>This helps Spark understand the schema of our completions, regardless of the model provider.</p>"},{"location":"reference/completions/#completions.BaseCompletion","title":"<code>BaseCompletion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Completion model for defining what model was used and number of tokens.</p> Source code in <code>spark_instructor/completions/base.py</code> <pre><code>class BaseCompletion(BaseModel):\n    \"\"\"Completion model for defining what model was used and number of tokens.\"\"\"\n\n    id: str\n    model: str\n</code></pre>"},{"location":"reference/completions/#completions.is_anthropic_available","title":"<code>is_anthropic_available()</code>","text":"<p>Check if Anthropic-related modules are available.</p> Source code in <code>spark_instructor/completions/__init__.py</code> <pre><code>def is_anthropic_available():\n    \"\"\"Check if Anthropic-related modules are available.\"\"\"\n    return ANTHROPIC_AVAILABLE\n</code></pre>"},{"location":"reference/completions/anthropic_completions/","title":"anthropic_completions","text":"<p>This module defines Pydantic models for Anthropic's chat completion API responses.</p> <p>It extends the base models from spark_instructor.completions.base to provide specific implementations for Anthropic's API structure. These models can be used to parse and validate Anthropic API responses, ensuring type safety and providing easy access to response data.</p> <p>Anthropic's completion models are not serializable out of the box, primarily due to Tools schema. We may add support for Anthropic Tools later on.</p>"},{"location":"reference/completions/anthropic_completions/#completions.anthropic_completions.AnthropicCompletion","title":"<code>AnthropicCompletion</code>","text":"<p>               Bases: <code>BaseCompletion</code></p> <p>Represents a complete Anthropic chat completion response.</p> <p>This class extends BaseCompletion to include all fields specific to Anthropic's API response structure.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>List[AnthropicContent]</code> <p>A list of content items in the completion.</p> <code>role</code> <code>str</code> <p>The role of the message (e.g., \"assistant\").</p> <code>stop_reason</code> <code>str</code> <p>The reason why the completion stopped.</p> <code>stop_sequence</code> <code>Optional[str]</code> <p>The stop sequence used, if any. Defaults to None.</p> <code>type</code> <code>str</code> <p>The type of the completion, typically \"message\" for Anthropic.</p> <code>usage</code> <code>AnthropicUsage</code> <p>An object containing token usage information.</p> Source code in <code>spark_instructor/completions/anthropic_completions.py</code> <pre><code>class AnthropicCompletion(BaseCompletion):\n    \"\"\"Represents a complete Anthropic chat completion response.\n\n    This class extends BaseCompletion to include all fields specific to\n    Anthropic's API response structure.\n\n    Attributes:\n        content (List[AnthropicContent]): A list of content items in the completion.\n        role (str): The role of the message (e.g., \"assistant\").\n        stop_reason (str): The reason why the completion stopped.\n        stop_sequence (Optional[str]): The stop sequence used, if any. Defaults to None.\n        type (str): The type of the completion, typically \"message\" for Anthropic.\n        usage (AnthropicUsage): An object containing token usage information.\n    \"\"\"\n\n    content: List[AnthropicContent]\n    role: str\n    stop_reason: str\n    stop_sequence: Optional[str] = None\n    type: str\n    usage: AnthropicUsage\n</code></pre>"},{"location":"reference/completions/anthropic_completions/#completions.anthropic_completions.AnthropicContent","title":"<code>AnthropicContent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a content item in an Anthropic chat completion response.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The text content of the message.</p> <code>type</code> <code>str</code> <p>The type of the content, typically \"text\" for Anthropic responses.</p> Source code in <code>spark_instructor/completions/anthropic_completions.py</code> <pre><code>class AnthropicContent(BaseModel):\n    \"\"\"Represents a content item in an Anthropic chat completion response.\n\n    Attributes:\n        text (str): The text content of the message.\n        type (str): The type of the content, typically \"text\" for Anthropic responses.\n    \"\"\"\n\n    text: str\n    type: str\n</code></pre>"},{"location":"reference/completions/anthropic_completions/#completions.anthropic_completions.AnthropicUsage","title":"<code>AnthropicUsage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the token usage information for an Anthropic chat completion.</p> <p>Attributes:</p> Name Type Description <code>input_tokens</code> <code>int</code> <p>The number of tokens in the input prompt.</p> <code>output_tokens</code> <code>int</code> <p>The number of tokens in the generated completion.</p> Source code in <code>spark_instructor/completions/anthropic_completions.py</code> <pre><code>class AnthropicUsage(BaseModel):\n    \"\"\"Represents the token usage information for an Anthropic chat completion.\n\n    Attributes:\n        input_tokens (int): The number of tokens in the input prompt.\n        output_tokens (int): The number of tokens in the generated completion.\n    \"\"\"\n\n    input_tokens: int\n    output_tokens: int\n</code></pre>"},{"location":"reference/completions/anthropic_completions/#completions.anthropic_completions.anthropic_tool_call_to_openai_tool_call","title":"<code>anthropic_tool_call_to_openai_tool_call(block)</code>","text":"<p>Convert an Anthropic tool call to an OpenAI tool call.</p> Source code in <code>spark_instructor/completions/anthropic_completions.py</code> <pre><code>def anthropic_tool_call_to_openai_tool_call(block: ToolUseBlock) -&gt; ChatCompletionMessageToolCall:\n    \"\"\"Convert an Anthropic tool call to an OpenAI tool call.\"\"\"\n    return ChatCompletionMessageToolCall(\n        id=block.id,\n        function=Function(name=block.name, arguments=json.dumps(block.input)),\n        type=\"function\",\n    )\n</code></pre>"},{"location":"reference/completions/anthropic_completions/#completions.anthropic_completions.transform_message_to_chat_completion","title":"<code>transform_message_to_chat_completion(message, enable_created_at=False)</code>","text":"<p>Transform a Message object to a ChatCompletion object.</p> <p>This function converts the structure of a Message (from the original schema) to a ChatCompletion (matching the target schema). It combines text blocks, converts tool use blocks to tool calls, maps stop reasons to finish reasons, and restructures usage information.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message | PromptCachingBetaMessage</code> <p>The original Message object to be transformed.</p> required <code>enable_created_at</code> <code>bool</code> <p>Whether to include a unix timestamp.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ChatCompletion</code> <code>ChatCompletion</code> <p>A new ChatCompletion object structured according to the target schema.</p> Note <ul> <li>Text blocks are combined into a single content string.</li> <li>Tool use blocks are converted to ChatCompletionMessageToolCall objects.</li> <li>The stop_reason is mapped to a corresponding finish_reason.</li> <li>Usage information is restructured to fit the CompletionUsage model.</li> <li>The timestamp is 0 by default but is updated when <code>enable_created_at</code> is True</li> </ul> Source code in <code>spark_instructor/completions/anthropic_completions.py</code> <pre><code>def transform_message_to_chat_completion(\n    message: Message | PromptCachingBetaMessage, enable_created_at: bool = False\n) -&gt; ChatCompletion:\n    \"\"\"Transform a Message object to a ChatCompletion object.\n\n    This function converts the structure of a Message (from the original schema)\n    to a ChatCompletion (matching the target schema). It combines text blocks,\n    converts tool use blocks to tool calls, maps stop reasons to finish reasons,\n    and restructures usage information.\n\n    Args:\n        message (Message | PromptCachingBetaMessage): The original Message object to be transformed.\n        enable_created_at (bool): Whether to include a unix timestamp.\n\n    Returns:\n        ChatCompletion: A new ChatCompletion object structured according to the target schema.\n\n    Note:\n        - Text blocks are combined into a single content string.\n        - Tool use blocks are converted to ChatCompletionMessageToolCall objects.\n        - The stop_reason is mapped to a corresponding finish_reason.\n        - Usage information is restructured to fit the CompletionUsage model.\n        - The timestamp is 0 by default but is updated when ``enable_created_at`` is True\n    \"\"\"\n    # Convert content to a single string\n    content = \" \".join([block.text for block in message.content if isinstance(block, TextBlock)]) or None\n\n    # Create tool calls from ToolUseBlock instances\n    tool_calls = [\n        anthropic_tool_call_to_openai_tool_call(block) for block in message.content if isinstance(block, ToolUseBlock)\n    ]\n\n    # Create the ChatCompletionMessage\n    chat_message = ChatCompletionMessage(\n        content=content, tool_calls=tool_calls if tool_calls else None, role=\"assistant\"\n    )\n\n    # Map stop_reason to finish_reason\n    finish_reason_map = {\"end_turn\": \"stop\", \"max_tokens\": \"length\", \"stop_sequence\": \"stop\", \"tool_use\": \"tool_calls\"}\n    finish_reason: FinishReason = cast(\n        FinishReason,\n        finish_reason_map.get(message.stop_reason, \"stop\") if message.stop_reason else \"stop\",\n    )\n\n    # Create the Choice\n    choice = Choice(finish_reason=finish_reason, index=0, message=chat_message)  # Assuming single choice\n\n    # Create CompletionUsage\n    prompt_tokens_details = (\n        PromptTokensDetails(cached_tokens=message.usage.cache_read_input_tokens)\n        if isinstance(message, PromptCachingBetaMessage)\n        else None\n    )\n    usage = CompletionUsage(\n        completion_tokens=message.usage.output_tokens,\n        prompt_tokens=message.usage.input_tokens,\n        total_tokens=message.usage.input_tokens + message.usage.output_tokens,\n        prompt_tokens_details=prompt_tokens_details,\n    )\n\n    # Create the ChatCompletion\n    return ChatCompletion(\n        id=message.id,\n        choices=[choice],\n        created=int(time.time()) if enable_created_at else 0,\n        model=message.model,\n        object=\"chat.completion\",\n        usage=usage,\n    )\n</code></pre>"},{"location":"reference/completions/base/","title":"base","text":"<p>A module for defining base classes for each completion type.</p>"},{"location":"reference/completions/base/#completions.base.BaseCompletion","title":"<code>BaseCompletion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Completion model for defining what model was used and number of tokens.</p> Source code in <code>spark_instructor/completions/base.py</code> <pre><code>class BaseCompletion(BaseModel):\n    \"\"\"Completion model for defining what model was used and number of tokens.\"\"\"\n\n    id: str\n    model: str\n</code></pre>"},{"location":"reference/completions/databricks_completions/","title":"databricks_completions","text":"<p>This module defines Pydantic models for Databricks' chat completion API responses.</p> <p>Databricks uses OpenAI schema.</p>"},{"location":"reference/completions/ollama_completions/","title":"ollama_completions","text":"<p>This module defines Pydantic models for Ollama chat completion API responses.</p> <p>Ollama uses OpenAI schema.</p>"},{"location":"reference/completions/openai_completions/","title":"openai_completions","text":"<p>This module defines Pydantic models for OpenAI's chat completion API responses.</p> <p>OpenAI already provides a well-defined schema which is spark-serializable.</p>"},{"location":"reference/factory/","title":"factory","text":"<p>Package for routing factories.</p>"},{"location":"reference/factory/#factory.ClientFactory","title":"<code>ClientFactory</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for defining client factories to route API traffic to different providers.</p> <p>This class serves as a template for creating specific client factories for various AI model providers (e.g., OpenAI, Anthropic, etc.). It provides a standardized interface for creating clients, formatting messages, and handling completions across different API providers.</p> <p>Attributes:</p> Name Type Description <code>async_client</code> <code>AsyncInstructor</code> <p>An asynchronous instructor client for making API calls.</p> <p>The ClientFactory class defines several abstract methods that must be implemented by subclasses: - from_config: For creating a factory instance from configuration parameters. - format_messages: For converting Spark-specific message formats to provider-specific formats. - format_completion: For standardizing completion responses from different providers.</p> <p>It also provides concrete implementations for creating completions with or without Pydantic models.</p> Usage <p>Subclass ClientFactory for each API provider, implementing the abstract methods according to the provider's specific requirements. Then use these subclasses to create provider-specific clients and handle API interactions in a standardized way across your application.</p> Example <pre><code>class OpenAIFactory(ClientFactory):\n    @classmethod\n    def from_config(cls, mode=None, base_url=None, api_key=None, **kwargs):\n        # Implementation for creating an OpenAI client\n        ...\n\n    def format_messages(self, messages):\n        # Implementation for formatting messages for OpenAI\n        ...\n\n    def format_completion(self, completion):\n        # Implementation for formatting OpenAI completion\n        ...\n\n# Using the factory\nopenai_factory = OpenAIFactory.from_config(api_key=\"your-api-key\")\ncompletion = await openai_factory.create(...)\n</code></pre> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>@dataclass\nclass ClientFactory(ABC):\n    \"\"\"An abstract base class for defining client factories to route API traffic to different providers.\n\n    This class serves as a template for creating specific client factories for various AI model providers\n    (e.g., OpenAI, Anthropic, etc.). It provides a standardized interface for creating clients,\n    formatting messages, and handling completions across different API providers.\n\n    Attributes:\n        async_client (instructor.AsyncInstructor): An asynchronous instructor client for making API calls.\n\n    The ClientFactory class defines several abstract methods that must be implemented by subclasses:\n    - from_config: For creating a factory instance from configuration parameters.\n    - format_messages: For converting Spark-specific message formats to provider-specific formats.\n    - format_completion: For standardizing completion responses from different providers.\n\n    It also provides concrete implementations for creating completions with or without Pydantic models.\n\n    Usage:\n        Subclass ClientFactory for each API provider, implementing the abstract methods according to\n        the provider's specific requirements. Then use these subclasses to create provider-specific\n        clients and handle API interactions in a standardized way across your application.\n\n    Example:\n        ```python\n        class OpenAIFactory(ClientFactory):\n            @classmethod\n            def from_config(cls, mode=None, base_url=None, api_key=None, **kwargs):\n                # Implementation for creating an OpenAI client\n                ...\n\n            def format_messages(self, messages):\n                # Implementation for formatting messages for OpenAI\n                ...\n\n            def format_completion(self, completion):\n                # Implementation for formatting OpenAI completion\n                ...\n\n        # Using the factory\n        openai_factory = OpenAIFactory.from_config(api_key=\"your-api-key\")\n        completion = await openai_factory.create(...)\n        ```\n    \"\"\"\n\n    async_client: instructor.AsyncInstructor\n\n    @classmethod\n    @abstractmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs,\n    ) -&gt; \"ClientFactory\":\n        \"\"\"Create a client factory instance from configuration parameters.\n\n        This method should be implemented to initialize the factory with provider-specific settings.\n\n        Args:\n            mode (Optional[instructor.Mode]): The mode of operation for the instructor client.\n            base_url (Optional[str]): The base URL for the API endpoint.\n            api_key (Optional[str]): The API key for authentication.\n            **kwargs: Additional keyword arguments for provider-specific configuration.\n\n        Returns:\n            ClientFactory: An instance of the concrete ClientFactory subclass.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format Spark completion messages to provider-specific chat completion messages.\n\n        This method should be implemented to convert the standardized Spark message format\n        to the format expected by the specific API provider.\n\n        Args:\n            messages (SparkChatCompletionMessages): The messages in Spark format.\n\n        Returns:\n            List[ChatCompletionMessageParam]: The formatted messages ready for the API provider.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def format_completion(self, completion: Any) -&gt; OpenAICompletion:\n        \"\"\"Format a provider-specific completion to a standardized OpenAI-style completion.\n\n        This method should be implemented to convert the completion response from the\n        provider's format to a standardized OpenAICompletion format.\n\n        Args:\n            completion (Any): The completion response from the API provider.\n\n        Returns:\n            OpenAICompletion: The formatted completion in OpenAI-compatible format.\n        \"\"\"\n        pass\n\n    async def create_with_completion(\n        self,\n        response_model: Type[T],\n        messages: SparkChatCompletionMessages,\n        model: str,\n        max_tokens: int,\n        temperature: float,\n        max_retries: int,\n        **kwargs,\n    ) -&gt; Tuple[T, ChatCompletion]:\n        \"\"\"Create a Pydantic model instance along with the full completion response.\n\n        This method sends a request to the API, formats the response into a Pydantic model,\n        and returns both the model instance and the full completion details.\n\n        Args:\n            response_model (Type[T]): The Pydantic model class for structuring the response.\n            messages (SparkChatCompletionMessages): The input messages for the completion.\n            model (str): The name or identifier of the AI model to use.\n            max_tokens (int): The maximum number of tokens in the completion response.\n            temperature (float): The sampling temperature for the model's output.\n            max_retries (int): The maximum number of retry attempts for failed requests.\n            **kwargs: Additional keyword arguments for the API request.\n\n        Returns:\n            Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.\n        \"\"\"\n        pydantic_object, completion = await self.async_client.chat.completions.create_with_completion(\n            response_model=response_model,\n            messages=self.format_messages(messages),\n            model=model,\n            max_retries=max_retries,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            **kwargs,\n        )\n        return pydantic_object, self.format_completion(completion)\n\n    async def create(\n        self,\n        response_model: Type[T] | None,\n        messages: SparkChatCompletionMessages,\n        model: str,\n        max_tokens: int,\n        temperature: float,\n        max_retries: int,\n        **kwargs,\n    ) -&gt; ChatCompletion:\n        \"\"\"Create a completion response.\n\n        This method sends a request to the API and returns the completion response.\n        If a response_model is provided, it structures the response accordingly.\n\n        Args:\n            response_model (Type[T] | None): Optional Pydantic model class for structuring the response.\n            messages (SparkChatCompletionMessages): The input messages for the completion.\n            model (str): The name or identifier of the AI model to use.\n            max_tokens (int): The maximum number of tokens in the completion response.\n            temperature (float): The sampling temperature for the model's output.\n            max_retries (int): The maximum number of retry attempts for failed requests.\n            **kwargs: Additional keyword arguments for the API request.\n\n        Returns:\n            ChatCompletion: The completion response, formatted according to the OpenAI standard.\n        \"\"\"\n        completion = await self.async_client.chat.completions.create(\n            response_model=response_model,  # type: ignore\n            messages=self.format_messages(messages),\n            model=model,\n            max_retries=max_retries,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            **kwargs,\n        )\n        return self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/#factory.ClientFactory.create","title":"<code>create(response_model, messages, model, max_tokens, temperature, max_retries, **kwargs)</code>  <code>async</code>","text":"<p>Create a completion response.</p> <p>This method sends a request to the API and returns the completion response. If a response_model is provided, it structures the response accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Type[T] | None</code> <p>Optional Pydantic model class for structuring the response.</p> required <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The input messages for the completion.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in the completion response.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature for the model's output.</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts for failed requests.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the API request.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ChatCompletion</code> <code>ChatCompletion</code> <p>The completion response, formatted according to the OpenAI standard.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>async def create(\n    self,\n    response_model: Type[T] | None,\n    messages: SparkChatCompletionMessages,\n    model: str,\n    max_tokens: int,\n    temperature: float,\n    max_retries: int,\n    **kwargs,\n) -&gt; ChatCompletion:\n    \"\"\"Create a completion response.\n\n    This method sends a request to the API and returns the completion response.\n    If a response_model is provided, it structures the response accordingly.\n\n    Args:\n        response_model (Type[T] | None): Optional Pydantic model class for structuring the response.\n        messages (SparkChatCompletionMessages): The input messages for the completion.\n        model (str): The name or identifier of the AI model to use.\n        max_tokens (int): The maximum number of tokens in the completion response.\n        temperature (float): The sampling temperature for the model's output.\n        max_retries (int): The maximum number of retry attempts for failed requests.\n        **kwargs: Additional keyword arguments for the API request.\n\n    Returns:\n        ChatCompletion: The completion response, formatted according to the OpenAI standard.\n    \"\"\"\n    completion = await self.async_client.chat.completions.create(\n        response_model=response_model,  # type: ignore\n        messages=self.format_messages(messages),\n        model=model,\n        max_retries=max_retries,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        **kwargs,\n    )\n    return self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/#factory.ClientFactory.create_with_completion","title":"<code>create_with_completion(response_model, messages, model, max_tokens, temperature, max_retries, **kwargs)</code>  <code>async</code>","text":"<p>Create a Pydantic model instance along with the full completion response.</p> <p>This method sends a request to the API, formats the response into a Pydantic model, and returns both the model instance and the full completion details.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Type[T]</code> <p>The Pydantic model class for structuring the response.</p> required <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The input messages for the completion.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in the completion response.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature for the model's output.</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts for failed requests.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the API request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[T, ChatCompletion]</code> <p>Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>async def create_with_completion(\n    self,\n    response_model: Type[T],\n    messages: SparkChatCompletionMessages,\n    model: str,\n    max_tokens: int,\n    temperature: float,\n    max_retries: int,\n    **kwargs,\n) -&gt; Tuple[T, ChatCompletion]:\n    \"\"\"Create a Pydantic model instance along with the full completion response.\n\n    This method sends a request to the API, formats the response into a Pydantic model,\n    and returns both the model instance and the full completion details.\n\n    Args:\n        response_model (Type[T]): The Pydantic model class for structuring the response.\n        messages (SparkChatCompletionMessages): The input messages for the completion.\n        model (str): The name or identifier of the AI model to use.\n        max_tokens (int): The maximum number of tokens in the completion response.\n        temperature (float): The sampling temperature for the model's output.\n        max_retries (int): The maximum number of retry attempts for failed requests.\n        **kwargs: Additional keyword arguments for the API request.\n\n    Returns:\n        Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.\n    \"\"\"\n    pydantic_object, completion = await self.async_client.chat.completions.create_with_completion(\n        response_model=response_model,\n        messages=self.format_messages(messages),\n        model=model,\n        max_retries=max_retries,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        **kwargs,\n    )\n    return pydantic_object, self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/#factory.ClientFactory.format_completion","title":"<code>format_completion(completion)</code>  <code>abstractmethod</code>","text":"<p>Format a provider-specific completion to a standardized OpenAI-style completion.</p> <p>This method should be implemented to convert the completion response from the provider's format to a standardized OpenAICompletion format.</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>Any</code> <p>The completion response from the API provider.</p> required <p>Returns:</p> Name Type Description <code>OpenAICompletion</code> <code>OpenAICompletion</code> <p>The formatted completion in OpenAI-compatible format.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>@abstractmethod\ndef format_completion(self, completion: Any) -&gt; OpenAICompletion:\n    \"\"\"Format a provider-specific completion to a standardized OpenAI-style completion.\n\n    This method should be implemented to convert the completion response from the\n    provider's format to a standardized OpenAICompletion format.\n\n    Args:\n        completion (Any): The completion response from the API provider.\n\n    Returns:\n        OpenAICompletion: The formatted completion in OpenAI-compatible format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/factory/#factory.ClientFactory.format_messages","title":"<code>format_messages(messages)</code>  <code>abstractmethod</code>","text":"<p>Format Spark completion messages to provider-specific chat completion messages.</p> <p>This method should be implemented to convert the standardized Spark message format to the format expected by the specific API provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The messages in Spark format.</p> required <p>Returns:</p> Type Description <code>List[ChatCompletionMessageParam]</code> <p>List[ChatCompletionMessageParam]: The formatted messages ready for the API provider.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>@abstractmethod\ndef format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format Spark completion messages to provider-specific chat completion messages.\n\n    This method should be implemented to convert the standardized Spark message format\n    to the format expected by the specific API provider.\n\n    Args:\n        messages (SparkChatCompletionMessages): The messages in Spark format.\n\n    Returns:\n        List[ChatCompletionMessageParam]: The formatted messages ready for the API provider.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/factory/#factory.ClientFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create a client factory instance from configuration parameters.</p> <p>This method should be implemented to initialize the factory with provider-specific settings.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[Mode]</code> <p>The mode of operation for the instructor client.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL for the API endpoint.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key for authentication.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for provider-specific configuration.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ClientFactory</code> <code>ClientFactory</code> <p>An instance of the concrete ClientFactory subclass.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"ClientFactory\":\n    \"\"\"Create a client factory instance from configuration parameters.\n\n    This method should be implemented to initialize the factory with provider-specific settings.\n\n    Args:\n        mode (Optional[instructor.Mode]): The mode of operation for the instructor client.\n        base_url (Optional[str]): The base URL for the API endpoint.\n        api_key (Optional[str]): The API key for authentication.\n        **kwargs: Additional keyword arguments for provider-specific configuration.\n\n    Returns:\n        ClientFactory: An instance of the concrete ClientFactory subclass.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/factory/#factory.DatabricksFactory","title":"<code>DatabricksFactory</code>","text":"<p>               Bases: <code>OpenAIFactory</code></p> <p>A databricks factory.</p> Source code in <code>spark_instructor/factory/databricks_factory.py</code> <pre><code>class DatabricksFactory(OpenAIFactory):\n    \"\"\"A databricks factory.\"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs\n    ) -&gt; \"DatabricksFactory\":\n        \"\"\"Build a databricks factory from custom entries.\"\"\"\n        return cls(get_databricks_aclient(mode or instructor.Mode.MD_JSON, base_url, api_key))\n\n    def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format messages by using default callable.\"\"\"\n        return [message(string_only=True) for message in messages.root]\n</code></pre>"},{"location":"reference/factory/#factory.DatabricksFactory.format_messages","title":"<code>format_messages(messages)</code>","text":"<p>Format messages by using default callable.</p> Source code in <code>spark_instructor/factory/databricks_factory.py</code> <pre><code>def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format messages by using default callable.\"\"\"\n    return [message(string_only=True) for message in messages.root]\n</code></pre>"},{"location":"reference/factory/#factory.DatabricksFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build a databricks factory from custom entries.</p> Source code in <code>spark_instructor/factory/databricks_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs\n) -&gt; \"DatabricksFactory\":\n    \"\"\"Build a databricks factory from custom entries.\"\"\"\n    return cls(get_databricks_aclient(mode or instructor.Mode.MD_JSON, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/#factory.O1Factory","title":"<code>O1Factory</code>","text":"<p>               Bases: <code>OpenAIFactory</code></p> <p>An OpenAI o1 factory.</p> <p>Used as the factory for o1 models which have unique functionality.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>class O1Factory(OpenAIFactory):\n    \"\"\"An OpenAI o1 factory.\n\n    Used as the factory for o1 models which have unique functionality.\n    \"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs,\n    ) -&gt; \"O1Factory\":\n        \"\"\"Create an OpenAI factory from custom inputs.\"\"\"\n        return cls(get_openai_aclient(mode or instructor.Mode.JSON_O1, base_url, api_key))\n\n    def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format messages by using default callable.\"\"\"\n        # Ignore system messages and images as they are not yet supported\n        return [message(string_only=True) for message in messages.root if message.role != \"system\"]\n\n    async def create(\n        self,\n        response_model: Type[T] | None,\n        messages: SparkChatCompletionMessages,\n        model: str,\n        max_tokens: int,\n        temperature: float,\n        max_retries: int,\n        **kwargs,\n    ) -&gt; ChatCompletion:\n        \"\"\"Create a completion response.\n\n        This method sends a request to the API and returns the completion response.\n        If a response_model is provided, it structures the response accordingly.\n        We handle o1 ``max_completion_tokens`` independently.\n\n        Args:\n            response_model (Type[T] | None): Optional Pydantic model class for structuring the response.\n            messages (SparkChatCompletionMessages): The input messages for the completion.\n            model (str): The name or identifier of the AI model to use.\n            max_tokens (int): The maximum number of tokens in the completion response.\n            temperature (float): The sampling temperature for the model's output.\n                Always set to 1 regardless of input (not supported for o1 yet).\n            max_retries (int): The maximum number of retry attempts for failed requests.\n            **kwargs: Additional keyword arguments for the API request.\n\n        Returns:\n            ChatCompletion: The completion response, formatted according to the OpenAI standard.\n        \"\"\"\n        completion = await self.async_client.chat.completions.create(\n            response_model=response_model,  # type: ignore\n            messages=self.format_messages(messages),\n            model=model,\n            max_retries=max_retries,\n            max_completion_tokens=max_tokens,\n            temperature=1,\n            **kwargs,\n        )\n        return self.format_completion(cast(ChatCompletion, completion))\n\n    async def create_with_completion(\n        self,\n        response_model: Type[T],\n        messages: SparkChatCompletionMessages,\n        model: str,\n        max_tokens: int,\n        temperature: float,\n        max_retries: int,\n        **kwargs,\n    ) -&gt; Tuple[T, ChatCompletion]:\n        \"\"\"Create a Pydantic model instance along with the full completion response.\n\n        This method sends a request to the API, formats the response into a Pydantic model,\n        and returns both the model instance and the full completion details.\n        We handle o1 ``max_completion_tokens`` independently.\n\n        Args:\n            response_model (Type[T]): The Pydantic model class for structuring the response.\n            messages (SparkChatCompletionMessages): The input messages for the completion.\n            model (str): The name or identifier of the AI model to use.\n            max_tokens (int): The maximum number of tokens in the completion response.\n            temperature (float): The sampling temperature for the model's output.\n                Always set to 1 regardless of input (not supported by o1 yet).\n            max_retries (int): The maximum number of retry attempts for failed requests.\n            **kwargs: Additional keyword arguments for the API request.\n\n        Returns:\n            Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.\n        \"\"\"\n        pydantic_object, completion = await self.async_client.chat.completions.create_with_completion(\n            response_model=response_model,\n            messages=self.format_messages(messages),\n            model=model,\n            max_retries=max_retries,\n            max_completion_tokens=max_tokens,\n            temperature=1,\n            **kwargs,\n        )\n        return pydantic_object, self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/#factory.O1Factory.create","title":"<code>create(response_model, messages, model, max_tokens, temperature, max_retries, **kwargs)</code>  <code>async</code>","text":"<p>Create a completion response.</p> <p>This method sends a request to the API and returns the completion response. If a response_model is provided, it structures the response accordingly. We handle o1 <code>max_completion_tokens</code> independently.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Type[T] | None</code> <p>Optional Pydantic model class for structuring the response.</p> required <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The input messages for the completion.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in the completion response.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature for the model's output. Always set to 1 regardless of input (not supported for o1 yet).</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts for failed requests.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the API request.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ChatCompletion</code> <code>ChatCompletion</code> <p>The completion response, formatted according to the OpenAI standard.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>async def create(\n    self,\n    response_model: Type[T] | None,\n    messages: SparkChatCompletionMessages,\n    model: str,\n    max_tokens: int,\n    temperature: float,\n    max_retries: int,\n    **kwargs,\n) -&gt; ChatCompletion:\n    \"\"\"Create a completion response.\n\n    This method sends a request to the API and returns the completion response.\n    If a response_model is provided, it structures the response accordingly.\n    We handle o1 ``max_completion_tokens`` independently.\n\n    Args:\n        response_model (Type[T] | None): Optional Pydantic model class for structuring the response.\n        messages (SparkChatCompletionMessages): The input messages for the completion.\n        model (str): The name or identifier of the AI model to use.\n        max_tokens (int): The maximum number of tokens in the completion response.\n        temperature (float): The sampling temperature for the model's output.\n            Always set to 1 regardless of input (not supported for o1 yet).\n        max_retries (int): The maximum number of retry attempts for failed requests.\n        **kwargs: Additional keyword arguments for the API request.\n\n    Returns:\n        ChatCompletion: The completion response, formatted according to the OpenAI standard.\n    \"\"\"\n    completion = await self.async_client.chat.completions.create(\n        response_model=response_model,  # type: ignore\n        messages=self.format_messages(messages),\n        model=model,\n        max_retries=max_retries,\n        max_completion_tokens=max_tokens,\n        temperature=1,\n        **kwargs,\n    )\n    return self.format_completion(cast(ChatCompletion, completion))\n</code></pre>"},{"location":"reference/factory/#factory.O1Factory.create_with_completion","title":"<code>create_with_completion(response_model, messages, model, max_tokens, temperature, max_retries, **kwargs)</code>  <code>async</code>","text":"<p>Create a Pydantic model instance along with the full completion response.</p> <p>This method sends a request to the API, formats the response into a Pydantic model, and returns both the model instance and the full completion details. We handle o1 <code>max_completion_tokens</code> independently.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Type[T]</code> <p>The Pydantic model class for structuring the response.</p> required <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The input messages for the completion.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in the completion response.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature for the model's output. Always set to 1 regardless of input (not supported by o1 yet).</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts for failed requests.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the API request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[T, ChatCompletion]</code> <p>Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>async def create_with_completion(\n    self,\n    response_model: Type[T],\n    messages: SparkChatCompletionMessages,\n    model: str,\n    max_tokens: int,\n    temperature: float,\n    max_retries: int,\n    **kwargs,\n) -&gt; Tuple[T, ChatCompletion]:\n    \"\"\"Create a Pydantic model instance along with the full completion response.\n\n    This method sends a request to the API, formats the response into a Pydantic model,\n    and returns both the model instance and the full completion details.\n    We handle o1 ``max_completion_tokens`` independently.\n\n    Args:\n        response_model (Type[T]): The Pydantic model class for structuring the response.\n        messages (SparkChatCompletionMessages): The input messages for the completion.\n        model (str): The name or identifier of the AI model to use.\n        max_tokens (int): The maximum number of tokens in the completion response.\n        temperature (float): The sampling temperature for the model's output.\n            Always set to 1 regardless of input (not supported by o1 yet).\n        max_retries (int): The maximum number of retry attempts for failed requests.\n        **kwargs: Additional keyword arguments for the API request.\n\n    Returns:\n        Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.\n    \"\"\"\n    pydantic_object, completion = await self.async_client.chat.completions.create_with_completion(\n        response_model=response_model,\n        messages=self.format_messages(messages),\n        model=model,\n        max_retries=max_retries,\n        max_completion_tokens=max_tokens,\n        temperature=1,\n        **kwargs,\n    )\n    return pydantic_object, self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/#factory.O1Factory.format_messages","title":"<code>format_messages(messages)</code>","text":"<p>Format messages by using default callable.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format messages by using default callable.\"\"\"\n    # Ignore system messages and images as they are not yet supported\n    return [message(string_only=True) for message in messages.root if message.role != \"system\"]\n</code></pre>"},{"location":"reference/factory/#factory.O1Factory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an OpenAI factory from custom inputs.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"O1Factory\":\n    \"\"\"Create an OpenAI factory from custom inputs.\"\"\"\n    return cls(get_openai_aclient(mode or instructor.Mode.JSON_O1, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/#factory.OllamaFactory","title":"<code>OllamaFactory</code>","text":"<p>               Bases: <code>OpenAIFactory</code></p> <p>An Ollama factory.</p> Source code in <code>spark_instructor/factory/ollama_factory.py</code> <pre><code>class OllamaFactory(OpenAIFactory):\n    \"\"\"An Ollama factory.\"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs\n    ) -&gt; \"OllamaFactory\":\n        \"\"\"Create an Ollama factory from custom entries.\"\"\"\n        return cls(get_ollama_aclient(mode or instructor.Mode.JSON, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/#factory.OllamaFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an Ollama factory from custom entries.</p> Source code in <code>spark_instructor/factory/ollama_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs\n) -&gt; \"OllamaFactory\":\n    \"\"\"Create an Ollama factory from custom entries.\"\"\"\n    return cls(get_ollama_aclient(mode or instructor.Mode.JSON, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/#factory.OpenAIFactory","title":"<code>OpenAIFactory</code>","text":"<p>               Bases: <code>ClientFactory</code></p> <p>An OpenAI factory.</p> <p>Used as default factory for most providers.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>class OpenAIFactory(ClientFactory):\n    \"\"\"An OpenAI factory.\n\n    Used as default factory for most providers.\n    \"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs,\n    ) -&gt; \"OpenAIFactory\":\n        \"\"\"Create an OpenAI factory from custom inputs.\"\"\"\n        return cls(get_openai_aclient(mode or instructor.Mode.TOOLS, base_url, api_key))\n\n    def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format messages by using default callable.\"\"\"\n        return [message() for message in messages.root]\n\n    def format_completion(self, completion: ChatCompletion) -&gt; ChatCompletion:\n        \"\"\"Return standard OpenAI completion message.\"\"\"\n        return completion\n</code></pre>"},{"location":"reference/factory/#factory.OpenAIFactory.format_completion","title":"<code>format_completion(completion)</code>","text":"<p>Return standard OpenAI completion message.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>def format_completion(self, completion: ChatCompletion) -&gt; ChatCompletion:\n    \"\"\"Return standard OpenAI completion message.\"\"\"\n    return completion\n</code></pre>"},{"location":"reference/factory/#factory.OpenAIFactory.format_messages","title":"<code>format_messages(messages)</code>","text":"<p>Format messages by using default callable.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format messages by using default callable.\"\"\"\n    return [message() for message in messages.root]\n</code></pre>"},{"location":"reference/factory/#factory.OpenAIFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an OpenAI factory from custom inputs.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"OpenAIFactory\":\n    \"\"\"Create an OpenAI factory from custom inputs.\"\"\"\n    return cls(get_openai_aclient(mode or instructor.Mode.TOOLS, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/anthropic_factory/","title":"anthropic_factory","text":"<p>Module for creating an Anthropic factory.</p>"},{"location":"reference/factory/anthropic_factory/#factory.anthropic_factory.AnthropicFactory","title":"<code>AnthropicFactory</code>","text":"<p>               Bases: <code>ClientFactory</code></p> <p>An Anthropic factory.</p> Source code in <code>spark_instructor/factory/anthropic_factory.py</code> <pre><code>class AnthropicFactory(ClientFactory):\n    \"\"\"An Anthropic factory.\"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs,\n    ) -&gt; \"AnthropicFactory\":\n        \"\"\"Get an anthropic client.\"\"\"\n        return cls(\n            get_anthropic_aclient(\n                mode or instructor.Mode.ANTHROPIC_TOOLS, base_url, api_key, kwargs.get(\"enable_caching\", False)\n            )\n        )\n\n    def format_completion(self, completion: Message) -&gt; ChatCompletion:\n        \"\"\"Format an Anthropic ``Message`` as an OpenAI ``ChatCompletion``.\"\"\"\n        return transform_message_to_chat_completion(completion, enable_created_at=True)\n\n    def format_messages(\n        self, messages: SparkChatCompletionMessages, ignore_system: bool = False\n    ) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format messages so that images serialize properly.\"\"\"\n        messages_unpacked = (\n            [message() for message in messages.root if message.role != \"system\"]\n            if ignore_system\n            else [message() for message in messages.root]\n        )\n        for j, message in enumerate(messages_unpacked):\n            if message[\"role\"] == \"user\":\n                casted_message = cast(ChatCompletionUserMessageParam, message)\n                if isinstance(casted_message[\"content\"], list):\n                    for i, content in enumerate(casted_message[\"content\"]):\n                        casted_content = cast(ChatCompletionContentPartParam, content)\n                        if casted_content[\"type\"] == \"image_url\":\n                            casted_content_image = cast(ChatCompletionContentPartImageParam, casted_content)\n                            casted_message[\"content\"][i] = convert_image_url_to_image_block_param(\n                                casted_content_image[\"image_url\"]\n                            )\n                    if messages.root[j].cache_control:\n                        casted_message[\"content\"][-1][\"cache_control\"] = {\"type\": \"ephemeral\"}\n            elif message[\"role\"] == \"system\" and messages.root[j].cache_control and isinstance(message[\"content\"], str):\n                message[\"content\"] = [\n                    {\"type\": \"text\", \"text\": message[\"content\"], \"cache_control\": {\"type\": \"ephemeral\"}}  # type: ignore\n                ]  # type: ignore\n        return messages_unpacked\n</code></pre>"},{"location":"reference/factory/anthropic_factory/#factory.anthropic_factory.AnthropicFactory.format_completion","title":"<code>format_completion(completion)</code>","text":"<p>Format an Anthropic <code>Message</code> as an OpenAI <code>ChatCompletion</code>.</p> Source code in <code>spark_instructor/factory/anthropic_factory.py</code> <pre><code>def format_completion(self, completion: Message) -&gt; ChatCompletion:\n    \"\"\"Format an Anthropic ``Message`` as an OpenAI ``ChatCompletion``.\"\"\"\n    return transform_message_to_chat_completion(completion, enable_created_at=True)\n</code></pre>"},{"location":"reference/factory/anthropic_factory/#factory.anthropic_factory.AnthropicFactory.format_messages","title":"<code>format_messages(messages, ignore_system=False)</code>","text":"<p>Format messages so that images serialize properly.</p> Source code in <code>spark_instructor/factory/anthropic_factory.py</code> <pre><code>def format_messages(\n    self, messages: SparkChatCompletionMessages, ignore_system: bool = False\n) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format messages so that images serialize properly.\"\"\"\n    messages_unpacked = (\n        [message() for message in messages.root if message.role != \"system\"]\n        if ignore_system\n        else [message() for message in messages.root]\n    )\n    for j, message in enumerate(messages_unpacked):\n        if message[\"role\"] == \"user\":\n            casted_message = cast(ChatCompletionUserMessageParam, message)\n            if isinstance(casted_message[\"content\"], list):\n                for i, content in enumerate(casted_message[\"content\"]):\n                    casted_content = cast(ChatCompletionContentPartParam, content)\n                    if casted_content[\"type\"] == \"image_url\":\n                        casted_content_image = cast(ChatCompletionContentPartImageParam, casted_content)\n                        casted_message[\"content\"][i] = convert_image_url_to_image_block_param(\n                            casted_content_image[\"image_url\"]\n                        )\n                if messages.root[j].cache_control:\n                    casted_message[\"content\"][-1][\"cache_control\"] = {\"type\": \"ephemeral\"}\n        elif message[\"role\"] == \"system\" and messages.root[j].cache_control and isinstance(message[\"content\"], str):\n            message[\"content\"] = [\n                {\"type\": \"text\", \"text\": message[\"content\"], \"cache_control\": {\"type\": \"ephemeral\"}}  # type: ignore\n            ]  # type: ignore\n    return messages_unpacked\n</code></pre>"},{"location":"reference/factory/anthropic_factory/#factory.anthropic_factory.AnthropicFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Get an anthropic client.</p> Source code in <code>spark_instructor/factory/anthropic_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"AnthropicFactory\":\n    \"\"\"Get an anthropic client.\"\"\"\n    return cls(\n        get_anthropic_aclient(\n            mode or instructor.Mode.ANTHROPIC_TOOLS, base_url, api_key, kwargs.get(\"enable_caching\", False)\n        )\n    )\n</code></pre>"},{"location":"reference/factory/base/","title":"base","text":"<p>Module for defining abstract classes for instructor client retrieval.</p>"},{"location":"reference/factory/base/#factory.base.ClientFactory","title":"<code>ClientFactory</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for defining client factories to route API traffic to different providers.</p> <p>This class serves as a template for creating specific client factories for various AI model providers (e.g., OpenAI, Anthropic, etc.). It provides a standardized interface for creating clients, formatting messages, and handling completions across different API providers.</p> <p>Attributes:</p> Name Type Description <code>async_client</code> <code>AsyncInstructor</code> <p>An asynchronous instructor client for making API calls.</p> <p>The ClientFactory class defines several abstract methods that must be implemented by subclasses: - from_config: For creating a factory instance from configuration parameters. - format_messages: For converting Spark-specific message formats to provider-specific formats. - format_completion: For standardizing completion responses from different providers.</p> <p>It also provides concrete implementations for creating completions with or without Pydantic models.</p> Usage <p>Subclass ClientFactory for each API provider, implementing the abstract methods according to the provider's specific requirements. Then use these subclasses to create provider-specific clients and handle API interactions in a standardized way across your application.</p> Example <pre><code>class OpenAIFactory(ClientFactory):\n    @classmethod\n    def from_config(cls, mode=None, base_url=None, api_key=None, **kwargs):\n        # Implementation for creating an OpenAI client\n        ...\n\n    def format_messages(self, messages):\n        # Implementation for formatting messages for OpenAI\n        ...\n\n    def format_completion(self, completion):\n        # Implementation for formatting OpenAI completion\n        ...\n\n# Using the factory\nopenai_factory = OpenAIFactory.from_config(api_key=\"your-api-key\")\ncompletion = await openai_factory.create(...)\n</code></pre> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>@dataclass\nclass ClientFactory(ABC):\n    \"\"\"An abstract base class for defining client factories to route API traffic to different providers.\n\n    This class serves as a template for creating specific client factories for various AI model providers\n    (e.g., OpenAI, Anthropic, etc.). It provides a standardized interface for creating clients,\n    formatting messages, and handling completions across different API providers.\n\n    Attributes:\n        async_client (instructor.AsyncInstructor): An asynchronous instructor client for making API calls.\n\n    The ClientFactory class defines several abstract methods that must be implemented by subclasses:\n    - from_config: For creating a factory instance from configuration parameters.\n    - format_messages: For converting Spark-specific message formats to provider-specific formats.\n    - format_completion: For standardizing completion responses from different providers.\n\n    It also provides concrete implementations for creating completions with or without Pydantic models.\n\n    Usage:\n        Subclass ClientFactory for each API provider, implementing the abstract methods according to\n        the provider's specific requirements. Then use these subclasses to create provider-specific\n        clients and handle API interactions in a standardized way across your application.\n\n    Example:\n        ```python\n        class OpenAIFactory(ClientFactory):\n            @classmethod\n            def from_config(cls, mode=None, base_url=None, api_key=None, **kwargs):\n                # Implementation for creating an OpenAI client\n                ...\n\n            def format_messages(self, messages):\n                # Implementation for formatting messages for OpenAI\n                ...\n\n            def format_completion(self, completion):\n                # Implementation for formatting OpenAI completion\n                ...\n\n        # Using the factory\n        openai_factory = OpenAIFactory.from_config(api_key=\"your-api-key\")\n        completion = await openai_factory.create(...)\n        ```\n    \"\"\"\n\n    async_client: instructor.AsyncInstructor\n\n    @classmethod\n    @abstractmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs,\n    ) -&gt; \"ClientFactory\":\n        \"\"\"Create a client factory instance from configuration parameters.\n\n        This method should be implemented to initialize the factory with provider-specific settings.\n\n        Args:\n            mode (Optional[instructor.Mode]): The mode of operation for the instructor client.\n            base_url (Optional[str]): The base URL for the API endpoint.\n            api_key (Optional[str]): The API key for authentication.\n            **kwargs: Additional keyword arguments for provider-specific configuration.\n\n        Returns:\n            ClientFactory: An instance of the concrete ClientFactory subclass.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format Spark completion messages to provider-specific chat completion messages.\n\n        This method should be implemented to convert the standardized Spark message format\n        to the format expected by the specific API provider.\n\n        Args:\n            messages (SparkChatCompletionMessages): The messages in Spark format.\n\n        Returns:\n            List[ChatCompletionMessageParam]: The formatted messages ready for the API provider.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def format_completion(self, completion: Any) -&gt; OpenAICompletion:\n        \"\"\"Format a provider-specific completion to a standardized OpenAI-style completion.\n\n        This method should be implemented to convert the completion response from the\n        provider's format to a standardized OpenAICompletion format.\n\n        Args:\n            completion (Any): The completion response from the API provider.\n\n        Returns:\n            OpenAICompletion: The formatted completion in OpenAI-compatible format.\n        \"\"\"\n        pass\n\n    async def create_with_completion(\n        self,\n        response_model: Type[T],\n        messages: SparkChatCompletionMessages,\n        model: str,\n        max_tokens: int,\n        temperature: float,\n        max_retries: int,\n        **kwargs,\n    ) -&gt; Tuple[T, ChatCompletion]:\n        \"\"\"Create a Pydantic model instance along with the full completion response.\n\n        This method sends a request to the API, formats the response into a Pydantic model,\n        and returns both the model instance and the full completion details.\n\n        Args:\n            response_model (Type[T]): The Pydantic model class for structuring the response.\n            messages (SparkChatCompletionMessages): The input messages for the completion.\n            model (str): The name or identifier of the AI model to use.\n            max_tokens (int): The maximum number of tokens in the completion response.\n            temperature (float): The sampling temperature for the model's output.\n            max_retries (int): The maximum number of retry attempts for failed requests.\n            **kwargs: Additional keyword arguments for the API request.\n\n        Returns:\n            Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.\n        \"\"\"\n        pydantic_object, completion = await self.async_client.chat.completions.create_with_completion(\n            response_model=response_model,\n            messages=self.format_messages(messages),\n            model=model,\n            max_retries=max_retries,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            **kwargs,\n        )\n        return pydantic_object, self.format_completion(completion)\n\n    async def create(\n        self,\n        response_model: Type[T] | None,\n        messages: SparkChatCompletionMessages,\n        model: str,\n        max_tokens: int,\n        temperature: float,\n        max_retries: int,\n        **kwargs,\n    ) -&gt; ChatCompletion:\n        \"\"\"Create a completion response.\n\n        This method sends a request to the API and returns the completion response.\n        If a response_model is provided, it structures the response accordingly.\n\n        Args:\n            response_model (Type[T] | None): Optional Pydantic model class for structuring the response.\n            messages (SparkChatCompletionMessages): The input messages for the completion.\n            model (str): The name or identifier of the AI model to use.\n            max_tokens (int): The maximum number of tokens in the completion response.\n            temperature (float): The sampling temperature for the model's output.\n            max_retries (int): The maximum number of retry attempts for failed requests.\n            **kwargs: Additional keyword arguments for the API request.\n\n        Returns:\n            ChatCompletion: The completion response, formatted according to the OpenAI standard.\n        \"\"\"\n        completion = await self.async_client.chat.completions.create(\n            response_model=response_model,  # type: ignore\n            messages=self.format_messages(messages),\n            model=model,\n            max_retries=max_retries,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            **kwargs,\n        )\n        return self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/base/#factory.base.ClientFactory.create","title":"<code>create(response_model, messages, model, max_tokens, temperature, max_retries, **kwargs)</code>  <code>async</code>","text":"<p>Create a completion response.</p> <p>This method sends a request to the API and returns the completion response. If a response_model is provided, it structures the response accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Type[T] | None</code> <p>Optional Pydantic model class for structuring the response.</p> required <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The input messages for the completion.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in the completion response.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature for the model's output.</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts for failed requests.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the API request.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ChatCompletion</code> <code>ChatCompletion</code> <p>The completion response, formatted according to the OpenAI standard.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>async def create(\n    self,\n    response_model: Type[T] | None,\n    messages: SparkChatCompletionMessages,\n    model: str,\n    max_tokens: int,\n    temperature: float,\n    max_retries: int,\n    **kwargs,\n) -&gt; ChatCompletion:\n    \"\"\"Create a completion response.\n\n    This method sends a request to the API and returns the completion response.\n    If a response_model is provided, it structures the response accordingly.\n\n    Args:\n        response_model (Type[T] | None): Optional Pydantic model class for structuring the response.\n        messages (SparkChatCompletionMessages): The input messages for the completion.\n        model (str): The name or identifier of the AI model to use.\n        max_tokens (int): The maximum number of tokens in the completion response.\n        temperature (float): The sampling temperature for the model's output.\n        max_retries (int): The maximum number of retry attempts for failed requests.\n        **kwargs: Additional keyword arguments for the API request.\n\n    Returns:\n        ChatCompletion: The completion response, formatted according to the OpenAI standard.\n    \"\"\"\n    completion = await self.async_client.chat.completions.create(\n        response_model=response_model,  # type: ignore\n        messages=self.format_messages(messages),\n        model=model,\n        max_retries=max_retries,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        **kwargs,\n    )\n    return self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/base/#factory.base.ClientFactory.create_with_completion","title":"<code>create_with_completion(response_model, messages, model, max_tokens, temperature, max_retries, **kwargs)</code>  <code>async</code>","text":"<p>Create a Pydantic model instance along with the full completion response.</p> <p>This method sends a request to the API, formats the response into a Pydantic model, and returns both the model instance and the full completion details.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Type[T]</code> <p>The Pydantic model class for structuring the response.</p> required <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The input messages for the completion.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in the completion response.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature for the model's output.</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts for failed requests.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the API request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[T, ChatCompletion]</code> <p>Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>async def create_with_completion(\n    self,\n    response_model: Type[T],\n    messages: SparkChatCompletionMessages,\n    model: str,\n    max_tokens: int,\n    temperature: float,\n    max_retries: int,\n    **kwargs,\n) -&gt; Tuple[T, ChatCompletion]:\n    \"\"\"Create a Pydantic model instance along with the full completion response.\n\n    This method sends a request to the API, formats the response into a Pydantic model,\n    and returns both the model instance and the full completion details.\n\n    Args:\n        response_model (Type[T]): The Pydantic model class for structuring the response.\n        messages (SparkChatCompletionMessages): The input messages for the completion.\n        model (str): The name or identifier of the AI model to use.\n        max_tokens (int): The maximum number of tokens in the completion response.\n        temperature (float): The sampling temperature for the model's output.\n        max_retries (int): The maximum number of retry attempts for failed requests.\n        **kwargs: Additional keyword arguments for the API request.\n\n    Returns:\n        Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.\n    \"\"\"\n    pydantic_object, completion = await self.async_client.chat.completions.create_with_completion(\n        response_model=response_model,\n        messages=self.format_messages(messages),\n        model=model,\n        max_retries=max_retries,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        **kwargs,\n    )\n    return pydantic_object, self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/base/#factory.base.ClientFactory.format_completion","title":"<code>format_completion(completion)</code>  <code>abstractmethod</code>","text":"<p>Format a provider-specific completion to a standardized OpenAI-style completion.</p> <p>This method should be implemented to convert the completion response from the provider's format to a standardized OpenAICompletion format.</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>Any</code> <p>The completion response from the API provider.</p> required <p>Returns:</p> Name Type Description <code>OpenAICompletion</code> <code>OpenAICompletion</code> <p>The formatted completion in OpenAI-compatible format.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>@abstractmethod\ndef format_completion(self, completion: Any) -&gt; OpenAICompletion:\n    \"\"\"Format a provider-specific completion to a standardized OpenAI-style completion.\n\n    This method should be implemented to convert the completion response from the\n    provider's format to a standardized OpenAICompletion format.\n\n    Args:\n        completion (Any): The completion response from the API provider.\n\n    Returns:\n        OpenAICompletion: The formatted completion in OpenAI-compatible format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/factory/base/#factory.base.ClientFactory.format_messages","title":"<code>format_messages(messages)</code>  <code>abstractmethod</code>","text":"<p>Format Spark completion messages to provider-specific chat completion messages.</p> <p>This method should be implemented to convert the standardized Spark message format to the format expected by the specific API provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The messages in Spark format.</p> required <p>Returns:</p> Type Description <code>List[ChatCompletionMessageParam]</code> <p>List[ChatCompletionMessageParam]: The formatted messages ready for the API provider.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>@abstractmethod\ndef format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format Spark completion messages to provider-specific chat completion messages.\n\n    This method should be implemented to convert the standardized Spark message format\n    to the format expected by the specific API provider.\n\n    Args:\n        messages (SparkChatCompletionMessages): The messages in Spark format.\n\n    Returns:\n        List[ChatCompletionMessageParam]: The formatted messages ready for the API provider.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/factory/base/#factory.base.ClientFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create a client factory instance from configuration parameters.</p> <p>This method should be implemented to initialize the factory with provider-specific settings.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[Mode]</code> <p>The mode of operation for the instructor client.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL for the API endpoint.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key for authentication.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for provider-specific configuration.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ClientFactory</code> <code>ClientFactory</code> <p>An instance of the concrete ClientFactory subclass.</p> Source code in <code>spark_instructor/factory/base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"ClientFactory\":\n    \"\"\"Create a client factory instance from configuration parameters.\n\n    This method should be implemented to initialize the factory with provider-specific settings.\n\n    Args:\n        mode (Optional[instructor.Mode]): The mode of operation for the instructor client.\n        base_url (Optional[str]): The base URL for the API endpoint.\n        api_key (Optional[str]): The API key for authentication.\n        **kwargs: Additional keyword arguments for provider-specific configuration.\n\n    Returns:\n        ClientFactory: An instance of the concrete ClientFactory subclass.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/factory/databricks_factory/","title":"databricks_factory","text":"<p>Module for creating a Databricks factory.</p>"},{"location":"reference/factory/databricks_factory/#factory.databricks_factory.DatabricksFactory","title":"<code>DatabricksFactory</code>","text":"<p>               Bases: <code>OpenAIFactory</code></p> <p>A databricks factory.</p> Source code in <code>spark_instructor/factory/databricks_factory.py</code> <pre><code>class DatabricksFactory(OpenAIFactory):\n    \"\"\"A databricks factory.\"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs\n    ) -&gt; \"DatabricksFactory\":\n        \"\"\"Build a databricks factory from custom entries.\"\"\"\n        return cls(get_databricks_aclient(mode or instructor.Mode.MD_JSON, base_url, api_key))\n\n    def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format messages by using default callable.\"\"\"\n        return [message(string_only=True) for message in messages.root]\n</code></pre>"},{"location":"reference/factory/databricks_factory/#factory.databricks_factory.DatabricksFactory.format_messages","title":"<code>format_messages(messages)</code>","text":"<p>Format messages by using default callable.</p> Source code in <code>spark_instructor/factory/databricks_factory.py</code> <pre><code>def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format messages by using default callable.\"\"\"\n    return [message(string_only=True) for message in messages.root]\n</code></pre>"},{"location":"reference/factory/databricks_factory/#factory.databricks_factory.DatabricksFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build a databricks factory from custom entries.</p> Source code in <code>spark_instructor/factory/databricks_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs\n) -&gt; \"DatabricksFactory\":\n    \"\"\"Build a databricks factory from custom entries.\"\"\"\n    return cls(get_databricks_aclient(mode or instructor.Mode.MD_JSON, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/ollama_factory/","title":"ollama_factory","text":"<p>Module for creating an Ollama factory.</p>"},{"location":"reference/factory/ollama_factory/#factory.ollama_factory.OllamaFactory","title":"<code>OllamaFactory</code>","text":"<p>               Bases: <code>OpenAIFactory</code></p> <p>An Ollama factory.</p> Source code in <code>spark_instructor/factory/ollama_factory.py</code> <pre><code>class OllamaFactory(OpenAIFactory):\n    \"\"\"An Ollama factory.\"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs\n    ) -&gt; \"OllamaFactory\":\n        \"\"\"Create an Ollama factory from custom entries.\"\"\"\n        return cls(get_ollama_aclient(mode or instructor.Mode.JSON, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/ollama_factory/#factory.ollama_factory.OllamaFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an Ollama factory from custom entries.</p> Source code in <code>spark_instructor/factory/ollama_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs\n) -&gt; \"OllamaFactory\":\n    \"\"\"Create an Ollama factory from custom entries.\"\"\"\n    return cls(get_ollama_aclient(mode or instructor.Mode.JSON, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/openai_factory/","title":"openai_factory","text":"<p>Module for creating an OpenAI factory.</p>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.O1Factory","title":"<code>O1Factory</code>","text":"<p>               Bases: <code>OpenAIFactory</code></p> <p>An OpenAI o1 factory.</p> <p>Used as the factory for o1 models which have unique functionality.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>class O1Factory(OpenAIFactory):\n    \"\"\"An OpenAI o1 factory.\n\n    Used as the factory for o1 models which have unique functionality.\n    \"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs,\n    ) -&gt; \"O1Factory\":\n        \"\"\"Create an OpenAI factory from custom inputs.\"\"\"\n        return cls(get_openai_aclient(mode or instructor.Mode.JSON_O1, base_url, api_key))\n\n    def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format messages by using default callable.\"\"\"\n        # Ignore system messages and images as they are not yet supported\n        return [message(string_only=True) for message in messages.root if message.role != \"system\"]\n\n    async def create(\n        self,\n        response_model: Type[T] | None,\n        messages: SparkChatCompletionMessages,\n        model: str,\n        max_tokens: int,\n        temperature: float,\n        max_retries: int,\n        **kwargs,\n    ) -&gt; ChatCompletion:\n        \"\"\"Create a completion response.\n\n        This method sends a request to the API and returns the completion response.\n        If a response_model is provided, it structures the response accordingly.\n        We handle o1 ``max_completion_tokens`` independently.\n\n        Args:\n            response_model (Type[T] | None): Optional Pydantic model class for structuring the response.\n            messages (SparkChatCompletionMessages): The input messages for the completion.\n            model (str): The name or identifier of the AI model to use.\n            max_tokens (int): The maximum number of tokens in the completion response.\n            temperature (float): The sampling temperature for the model's output.\n                Always set to 1 regardless of input (not supported for o1 yet).\n            max_retries (int): The maximum number of retry attempts for failed requests.\n            **kwargs: Additional keyword arguments for the API request.\n\n        Returns:\n            ChatCompletion: The completion response, formatted according to the OpenAI standard.\n        \"\"\"\n        completion = await self.async_client.chat.completions.create(\n            response_model=response_model,  # type: ignore\n            messages=self.format_messages(messages),\n            model=model,\n            max_retries=max_retries,\n            max_completion_tokens=max_tokens,\n            temperature=1,\n            **kwargs,\n        )\n        return self.format_completion(cast(ChatCompletion, completion))\n\n    async def create_with_completion(\n        self,\n        response_model: Type[T],\n        messages: SparkChatCompletionMessages,\n        model: str,\n        max_tokens: int,\n        temperature: float,\n        max_retries: int,\n        **kwargs,\n    ) -&gt; Tuple[T, ChatCompletion]:\n        \"\"\"Create a Pydantic model instance along with the full completion response.\n\n        This method sends a request to the API, formats the response into a Pydantic model,\n        and returns both the model instance and the full completion details.\n        We handle o1 ``max_completion_tokens`` independently.\n\n        Args:\n            response_model (Type[T]): The Pydantic model class for structuring the response.\n            messages (SparkChatCompletionMessages): The input messages for the completion.\n            model (str): The name or identifier of the AI model to use.\n            max_tokens (int): The maximum number of tokens in the completion response.\n            temperature (float): The sampling temperature for the model's output.\n                Always set to 1 regardless of input (not supported by o1 yet).\n            max_retries (int): The maximum number of retry attempts for failed requests.\n            **kwargs: Additional keyword arguments for the API request.\n\n        Returns:\n            Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.\n        \"\"\"\n        pydantic_object, completion = await self.async_client.chat.completions.create_with_completion(\n            response_model=response_model,\n            messages=self.format_messages(messages),\n            model=model,\n            max_retries=max_retries,\n            max_completion_tokens=max_tokens,\n            temperature=1,\n            **kwargs,\n        )\n        return pydantic_object, self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.O1Factory.create","title":"<code>create(response_model, messages, model, max_tokens, temperature, max_retries, **kwargs)</code>  <code>async</code>","text":"<p>Create a completion response.</p> <p>This method sends a request to the API and returns the completion response. If a response_model is provided, it structures the response accordingly. We handle o1 <code>max_completion_tokens</code> independently.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Type[T] | None</code> <p>Optional Pydantic model class for structuring the response.</p> required <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The input messages for the completion.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in the completion response.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature for the model's output. Always set to 1 regardless of input (not supported for o1 yet).</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts for failed requests.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the API request.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ChatCompletion</code> <code>ChatCompletion</code> <p>The completion response, formatted according to the OpenAI standard.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>async def create(\n    self,\n    response_model: Type[T] | None,\n    messages: SparkChatCompletionMessages,\n    model: str,\n    max_tokens: int,\n    temperature: float,\n    max_retries: int,\n    **kwargs,\n) -&gt; ChatCompletion:\n    \"\"\"Create a completion response.\n\n    This method sends a request to the API and returns the completion response.\n    If a response_model is provided, it structures the response accordingly.\n    We handle o1 ``max_completion_tokens`` independently.\n\n    Args:\n        response_model (Type[T] | None): Optional Pydantic model class for structuring the response.\n        messages (SparkChatCompletionMessages): The input messages for the completion.\n        model (str): The name or identifier of the AI model to use.\n        max_tokens (int): The maximum number of tokens in the completion response.\n        temperature (float): The sampling temperature for the model's output.\n            Always set to 1 regardless of input (not supported for o1 yet).\n        max_retries (int): The maximum number of retry attempts for failed requests.\n        **kwargs: Additional keyword arguments for the API request.\n\n    Returns:\n        ChatCompletion: The completion response, formatted according to the OpenAI standard.\n    \"\"\"\n    completion = await self.async_client.chat.completions.create(\n        response_model=response_model,  # type: ignore\n        messages=self.format_messages(messages),\n        model=model,\n        max_retries=max_retries,\n        max_completion_tokens=max_tokens,\n        temperature=1,\n        **kwargs,\n    )\n    return self.format_completion(cast(ChatCompletion, completion))\n</code></pre>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.O1Factory.create_with_completion","title":"<code>create_with_completion(response_model, messages, model, max_tokens, temperature, max_retries, **kwargs)</code>  <code>async</code>","text":"<p>Create a Pydantic model instance along with the full completion response.</p> <p>This method sends a request to the API, formats the response into a Pydantic model, and returns both the model instance and the full completion details. We handle o1 <code>max_completion_tokens</code> independently.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Type[T]</code> <p>The Pydantic model class for structuring the response.</p> required <code>messages</code> <code>SparkChatCompletionMessages</code> <p>The input messages for the completion.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in the completion response.</p> required <code>temperature</code> <code>float</code> <p>The sampling temperature for the model's output. Always set to 1 regardless of input (not supported by o1 yet).</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of retry attempts for failed requests.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the API request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[T, ChatCompletion]</code> <p>Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>async def create_with_completion(\n    self,\n    response_model: Type[T],\n    messages: SparkChatCompletionMessages,\n    model: str,\n    max_tokens: int,\n    temperature: float,\n    max_retries: int,\n    **kwargs,\n) -&gt; Tuple[T, ChatCompletion]:\n    \"\"\"Create a Pydantic model instance along with the full completion response.\n\n    This method sends a request to the API, formats the response into a Pydantic model,\n    and returns both the model instance and the full completion details.\n    We handle o1 ``max_completion_tokens`` independently.\n\n    Args:\n        response_model (Type[T]): The Pydantic model class for structuring the response.\n        messages (SparkChatCompletionMessages): The input messages for the completion.\n        model (str): The name or identifier of the AI model to use.\n        max_tokens (int): The maximum number of tokens in the completion response.\n        temperature (float): The sampling temperature for the model's output.\n            Always set to 1 regardless of input (not supported by o1 yet).\n        max_retries (int): The maximum number of retry attempts for failed requests.\n        **kwargs: Additional keyword arguments for the API request.\n\n    Returns:\n        Tuple[T, ChatCompletion]: A tuple containing the Pydantic model instance and the full completion.\n    \"\"\"\n    pydantic_object, completion = await self.async_client.chat.completions.create_with_completion(\n        response_model=response_model,\n        messages=self.format_messages(messages),\n        model=model,\n        max_retries=max_retries,\n        max_completion_tokens=max_tokens,\n        temperature=1,\n        **kwargs,\n    )\n    return pydantic_object, self.format_completion(completion)\n</code></pre>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.O1Factory.format_messages","title":"<code>format_messages(messages)</code>","text":"<p>Format messages by using default callable.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format messages by using default callable.\"\"\"\n    # Ignore system messages and images as they are not yet supported\n    return [message(string_only=True) for message in messages.root if message.role != \"system\"]\n</code></pre>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.O1Factory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an OpenAI factory from custom inputs.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"O1Factory\":\n    \"\"\"Create an OpenAI factory from custom inputs.\"\"\"\n    return cls(get_openai_aclient(mode or instructor.Mode.JSON_O1, base_url, api_key))\n</code></pre>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.OpenAIFactory","title":"<code>OpenAIFactory</code>","text":"<p>               Bases: <code>ClientFactory</code></p> <p>An OpenAI factory.</p> <p>Used as default factory for most providers.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>class OpenAIFactory(ClientFactory):\n    \"\"\"An OpenAI factory.\n\n    Used as default factory for most providers.\n    \"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        mode: Optional[instructor.Mode] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs,\n    ) -&gt; \"OpenAIFactory\":\n        \"\"\"Create an OpenAI factory from custom inputs.\"\"\"\n        return cls(get_openai_aclient(mode or instructor.Mode.TOOLS, base_url, api_key))\n\n    def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n        \"\"\"Format messages by using default callable.\"\"\"\n        return [message() for message in messages.root]\n\n    def format_completion(self, completion: ChatCompletion) -&gt; ChatCompletion:\n        \"\"\"Return standard OpenAI completion message.\"\"\"\n        return completion\n</code></pre>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.OpenAIFactory.format_completion","title":"<code>format_completion(completion)</code>","text":"<p>Return standard OpenAI completion message.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>def format_completion(self, completion: ChatCompletion) -&gt; ChatCompletion:\n    \"\"\"Return standard OpenAI completion message.\"\"\"\n    return completion\n</code></pre>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.OpenAIFactory.format_messages","title":"<code>format_messages(messages)</code>","text":"<p>Format messages by using default callable.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>def format_messages(self, messages: SparkChatCompletionMessages) -&gt; List[ChatCompletionMessageParam]:\n    \"\"\"Format messages by using default callable.\"\"\"\n    return [message() for message in messages.root]\n</code></pre>"},{"location":"reference/factory/openai_factory/#factory.openai_factory.OpenAIFactory.from_config","title":"<code>from_config(mode=None, base_url=None, api_key=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an OpenAI factory from custom inputs.</p> Source code in <code>spark_instructor/factory/openai_factory.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    mode: Optional[instructor.Mode] = None,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"OpenAIFactory\":\n    \"\"\"Create an OpenAI factory from custom inputs.\"\"\"\n    return cls(get_openai_aclient(mode or instructor.Mode.TOOLS, base_url, api_key))\n</code></pre>"},{"location":"reference/types/","title":"types","text":"<p>Package for types.</p>"},{"location":"reference/types/#types.SparkChatCompletionMessages","title":"<code>SparkChatCompletionMessages = RootModel[List[SparkChatCompletionMessage]]</code>  <code>module-attribute</code>","text":"<p>A root model representing a list of SparkChatCompletionMessage objects.</p> <p>This model is used to represent an entire conversation or a series of messages in a chat completion context. It provides a convenient way to handle multiple messages as a single entity while maintaining Pydantic's validation and serialization capabilities.</p> Usage <pre><code>messages = SparkChatCompletionMessages(root=[\n    SparkChatCompletionMessage(role=\"user\", content=\"Hello\"),\n    SparkChatCompletionMessage(role=\"assistant\", content=\"Hi there!\")\n])\n</code></pre>"},{"location":"reference/types/#types.SparkChatCompletionMessage","title":"<code>SparkChatCompletionMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Spark-serializable chat completion message that can be used for any role in a conversation.</p> <p>This class provides a flexible structure for representing messages in a chat completion context, supporting various roles (user, assistant, system, tool) and additional attributes like image URLs and tool calls.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['user', 'assistant', 'system', 'tool']</code> <p>The role of the message sender.</p> <code>content</code> <code>Optional[str]</code> <p>The text content of the message.</p> <code>image_urls</code> <code>Optional[List[ImageURLPD]]</code> <p>List of image URLs associated with the message.</p> <code>name</code> <code>Optional[str]</code> <p>The name of the entity associated with the message.</p> <code>tool_calls</code> <code>Optional[List[ChatCompletionMessageToolCallParamPD]]</code> <p>Tool calls made in the message (for 'assistant' role).</p> <code>tool_call_id</code> <code>Optional[str]</code> <p>The ID of the tool call (for 'tool' role).</p> <code>cache_control</code> <code>Optional[bool]</code> <p>Whether to use Anthropic's prompt caching feature (beta).</p> <p>The class provides methods to format the message for different roles and serialize it to OpenAI-compatible types.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>class SparkChatCompletionMessage(BaseModel):\n    \"\"\"A Spark-serializable chat completion message that can be used for any role in a conversation.\n\n    This class provides a flexible structure for representing messages in a chat completion context,\n    supporting various roles (user, assistant, system, tool) and additional attributes like image URLs\n    and tool calls.\n\n    Attributes:\n        role (Literal[\"user\", \"assistant\", \"system\", \"tool\"]): The role of the message sender.\n        content (Optional[str]): The text content of the message.\n        image_urls (Optional[List[ImageURLPD]]): List of image URLs associated with the message.\n        name (Optional[str]): The name of the entity associated with the message.\n        tool_calls (Optional[List[ChatCompletionMessageToolCallParamPD]]): Tool calls made in the message (for 'assistant' role).\n        tool_call_id (Optional[str]): The ID of the tool call (for 'tool' role).\n        cache_control (Optional[bool]): Whether to use Anthropic's prompt caching feature (beta).\n\n    The class provides methods to format the message for different roles and serialize it to OpenAI-compatible types.\n    \"\"\"  # noqa: E501\n\n    role: Literal[\"user\", \"assistant\", \"system\", \"tool\"] = Field(\"user\", description=\"The role of the message.\")\n    content: Optional[str] = Field(None, description=\"The text content of the message.\")\n    image_urls: Optional[List[ImageURLPD]] = Field(None, description=\"The image urls of the message.\")  # type: ignore\n    name: Optional[str] = Field(None, description=\"The name of the relevant chat entity\")\n    tool_calls: Optional[List[ChatCompletionMessageToolCallParamPD]] = Field(  # type: ignore\n        None, description=\"The tool calls of the message (`assistant` role only)\"\n    )\n    tool_call_id: Optional[str] = Field(None, description=\"The tool call id of the message (`tool` role only).\")\n    cache_control: Optional[bool] = Field(None, description=\"Whether to use Anthropic cache control.\")\n\n    def content_formatted(self) -&gt; List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]:\n        \"\"\"Format the message content, including any images, into a list of content parts.\n\n        This method prepares the message content for use in API calls, handling both text and image content.\n\n        Returns:\n            List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]:\n                A list of formatted content parts, including text and images.\n        \"\"\"\n        results: List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]] = (\n            []\n            if not self.image_urls\n            else [\n                ChatCompletionContentPartImageParam(image_url=format_image_url_pd(image_url), type=\"image_url\")\n                for image_url in self.image_urls\n            ]\n        )\n        if self.content:\n            results.append(ChatCompletionContentPartTextParam(text=self.content, type=\"text\"))\n        return results\n\n    def as_user(self, string_only: bool = False) -&gt; ChatCompletionUserMessageParam:\n        \"\"\"Format the message as a user message.\n\n        This method prepares the message for use as a user input in a chat completion.\n\n        Args:\n            string_only (bool): If True, return only the text content for messages with images.\n\n        Returns:\n            ChatCompletionUserMessageParam: The formatted user message.\n        \"\"\"\n        if self.name:\n            if string_only and self.content:\n                return ChatCompletionUserMessageParam(content=self.content, role=\"user\", name=self.name)\n            return ChatCompletionUserMessageParam(content=self.content_formatted(), role=\"user\", name=self.name)\n        if string_only and self.content:\n            return ChatCompletionUserMessageParam(content=self.content, role=\"user\")\n        return ChatCompletionUserMessageParam(content=self.content_formatted(), role=\"user\")\n\n    def as_assistant(self) -&gt; ChatCompletionAssistantMessageParam:\n        \"\"\"Format the message as an assistant message.\n\n        This method prepares the message for use as an assistant response in a chat completion,\n        including any tool calls if present.\n\n        Returns:\n            ChatCompletionAssistantMessageParam: The formatted assistant message.\n        \"\"\"\n        if self.name:\n            if self.tool_calls:\n                return ChatCompletionAssistantMessageParam(\n                    content=self.content,\n                    role=\"assistant\",\n                    name=self.name,\n                    tool_calls=[call.model_dump() for call in self.tool_calls],  # type: ignore\n                )\n            return ChatCompletionAssistantMessageParam(content=self.content, role=\"assistant\", name=self.name)\n        if self.tool_calls:\n            return ChatCompletionAssistantMessageParam(\n                content=self.content,\n                role=\"assistant\",\n                tool_calls=[call.model_dump() for call in self.tool_calls],  # type: ignore\n            )\n        return ChatCompletionAssistantMessageParam(content=self.content, role=\"assistant\")\n\n    def as_system(self) -&gt; ChatCompletionSystemMessageParam:\n        \"\"\"Format the message as a system message.\n\n        This method prepares the message for use as a system instruction in a chat completion.\n\n        Returns:\n            ChatCompletionSystemMessageParam: The formatted system message.\n\n        Raises:\n            AssertionError: If the content is empty.\n        \"\"\"\n        assert self.content, \"`content` must not be empty\"\n        if self.name:\n            return ChatCompletionSystemMessageParam(content=self.content, role=\"system\", name=self.name)\n        return ChatCompletionSystemMessageParam(content=self.content, role=\"system\")\n\n    def as_tool(self) -&gt; ChatCompletionToolMessageParam:\n        \"\"\"Format the message as a tool message.\n\n        This method prepares the message for use as a tool response in a chat completion.\n\n        Returns:\n            ChatCompletionToolMessageParam: The formatted tool message.\n\n        Raises:\n            AssertionError: If either content or tool_call_id is empty.\n        \"\"\"\n        assert self.content and self.tool_call_id, \"`content` and `tool_call_id` must not be empty\"\n        return ChatCompletionToolMessageParam(content=self.content, role=\"tool\", tool_call_id=self.tool_call_id)\n\n    def __call__(self, *args, string_only: bool = False, **kwargs) -&gt; ChatCompletionMessageParam:\n        \"\"\"Serialize the message to an OpenAI-compatible type.\n\n        This method allows the object to be called directly, returning the appropriate\n        message type based on the role.\n\n        Args:\n            string_only (bool): If True, return only the text content.\n                The resulting user message will only have string content.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            ChatCompletionMessageParam: The serialized message in OpenAI-compatible format.\n        \"\"\"\n        if self.role == \"system\":\n            return self.as_system()\n        if self.role == \"user\":\n            return self.as_user(string_only=string_only)\n        if self.role == \"assistant\":\n            return self.as_assistant()\n        return self.as_tool()\n</code></pre>"},{"location":"reference/types/#types.SparkChatCompletionMessage.__call__","title":"<code>__call__(*args, string_only=False, **kwargs)</code>","text":"<p>Serialize the message to an OpenAI-compatible type.</p> <p>This method allows the object to be called directly, returning the appropriate message type based on the role.</p> <p>Parameters:</p> Name Type Description Default <code>string_only</code> <code>bool</code> <p>If True, return only the text content. The resulting user message will only have string content.</p> <code>False</code> <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ChatCompletionMessageParam</code> <code>ChatCompletionMessageParam</code> <p>The serialized message in OpenAI-compatible format.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def __call__(self, *args, string_only: bool = False, **kwargs) -&gt; ChatCompletionMessageParam:\n    \"\"\"Serialize the message to an OpenAI-compatible type.\n\n    This method allows the object to be called directly, returning the appropriate\n    message type based on the role.\n\n    Args:\n        string_only (bool): If True, return only the text content.\n            The resulting user message will only have string content.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        ChatCompletionMessageParam: The serialized message in OpenAI-compatible format.\n    \"\"\"\n    if self.role == \"system\":\n        return self.as_system()\n    if self.role == \"user\":\n        return self.as_user(string_only=string_only)\n    if self.role == \"assistant\":\n        return self.as_assistant()\n    return self.as_tool()\n</code></pre>"},{"location":"reference/types/#types.SparkChatCompletionMessage.as_assistant","title":"<code>as_assistant()</code>","text":"<p>Format the message as an assistant message.</p> <p>This method prepares the message for use as an assistant response in a chat completion, including any tool calls if present.</p> <p>Returns:</p> Name Type Description <code>ChatCompletionAssistantMessageParam</code> <code>ChatCompletionAssistantMessageParam</code> <p>The formatted assistant message.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def as_assistant(self) -&gt; ChatCompletionAssistantMessageParam:\n    \"\"\"Format the message as an assistant message.\n\n    This method prepares the message for use as an assistant response in a chat completion,\n    including any tool calls if present.\n\n    Returns:\n        ChatCompletionAssistantMessageParam: The formatted assistant message.\n    \"\"\"\n    if self.name:\n        if self.tool_calls:\n            return ChatCompletionAssistantMessageParam(\n                content=self.content,\n                role=\"assistant\",\n                name=self.name,\n                tool_calls=[call.model_dump() for call in self.tool_calls],  # type: ignore\n            )\n        return ChatCompletionAssistantMessageParam(content=self.content, role=\"assistant\", name=self.name)\n    if self.tool_calls:\n        return ChatCompletionAssistantMessageParam(\n            content=self.content,\n            role=\"assistant\",\n            tool_calls=[call.model_dump() for call in self.tool_calls],  # type: ignore\n        )\n    return ChatCompletionAssistantMessageParam(content=self.content, role=\"assistant\")\n</code></pre>"},{"location":"reference/types/#types.SparkChatCompletionMessage.as_system","title":"<code>as_system()</code>","text":"<p>Format the message as a system message.</p> <p>This method prepares the message for use as a system instruction in a chat completion.</p> <p>Returns:</p> Name Type Description <code>ChatCompletionSystemMessageParam</code> <code>ChatCompletionSystemMessageParam</code> <p>The formatted system message.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the content is empty.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def as_system(self) -&gt; ChatCompletionSystemMessageParam:\n    \"\"\"Format the message as a system message.\n\n    This method prepares the message for use as a system instruction in a chat completion.\n\n    Returns:\n        ChatCompletionSystemMessageParam: The formatted system message.\n\n    Raises:\n        AssertionError: If the content is empty.\n    \"\"\"\n    assert self.content, \"`content` must not be empty\"\n    if self.name:\n        return ChatCompletionSystemMessageParam(content=self.content, role=\"system\", name=self.name)\n    return ChatCompletionSystemMessageParam(content=self.content, role=\"system\")\n</code></pre>"},{"location":"reference/types/#types.SparkChatCompletionMessage.as_tool","title":"<code>as_tool()</code>","text":"<p>Format the message as a tool message.</p> <p>This method prepares the message for use as a tool response in a chat completion.</p> <p>Returns:</p> Name Type Description <code>ChatCompletionToolMessageParam</code> <code>ChatCompletionToolMessageParam</code> <p>The formatted tool message.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If either content or tool_call_id is empty.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def as_tool(self) -&gt; ChatCompletionToolMessageParam:\n    \"\"\"Format the message as a tool message.\n\n    This method prepares the message for use as a tool response in a chat completion.\n\n    Returns:\n        ChatCompletionToolMessageParam: The formatted tool message.\n\n    Raises:\n        AssertionError: If either content or tool_call_id is empty.\n    \"\"\"\n    assert self.content and self.tool_call_id, \"`content` and `tool_call_id` must not be empty\"\n    return ChatCompletionToolMessageParam(content=self.content, role=\"tool\", tool_call_id=self.tool_call_id)\n</code></pre>"},{"location":"reference/types/#types.SparkChatCompletionMessage.as_user","title":"<code>as_user(string_only=False)</code>","text":"<p>Format the message as a user message.</p> <p>This method prepares the message for use as a user input in a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>string_only</code> <code>bool</code> <p>If True, return only the text content for messages with images.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ChatCompletionUserMessageParam</code> <code>ChatCompletionUserMessageParam</code> <p>The formatted user message.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def as_user(self, string_only: bool = False) -&gt; ChatCompletionUserMessageParam:\n    \"\"\"Format the message as a user message.\n\n    This method prepares the message for use as a user input in a chat completion.\n\n    Args:\n        string_only (bool): If True, return only the text content for messages with images.\n\n    Returns:\n        ChatCompletionUserMessageParam: The formatted user message.\n    \"\"\"\n    if self.name:\n        if string_only and self.content:\n            return ChatCompletionUserMessageParam(content=self.content, role=\"user\", name=self.name)\n        return ChatCompletionUserMessageParam(content=self.content_formatted(), role=\"user\", name=self.name)\n    if string_only and self.content:\n        return ChatCompletionUserMessageParam(content=self.content, role=\"user\")\n    return ChatCompletionUserMessageParam(content=self.content_formatted(), role=\"user\")\n</code></pre>"},{"location":"reference/types/#types.SparkChatCompletionMessage.content_formatted","title":"<code>content_formatted()</code>","text":"<p>Format the message content, including any images, into a list of content parts.</p> <p>This method prepares the message content for use in API calls, handling both text and image content.</p> <p>Returns:</p> Type Description <code>List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]</code> <p>List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]: A list of formatted content parts, including text and images.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def content_formatted(self) -&gt; List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]:\n    \"\"\"Format the message content, including any images, into a list of content parts.\n\n    This method prepares the message content for use in API calls, handling both text and image content.\n\n    Returns:\n        List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]:\n            A list of formatted content parts, including text and images.\n    \"\"\"\n    results: List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]] = (\n        []\n        if not self.image_urls\n        else [\n            ChatCompletionContentPartImageParam(image_url=format_image_url_pd(image_url), type=\"image_url\")\n            for image_url in self.image_urls\n        ]\n    )\n    if self.content:\n        results.append(ChatCompletionContentPartTextParam(text=self.content, type=\"text\"))\n    return results\n</code></pre>"},{"location":"reference/types/base/","title":"base","text":"<p>A module for defining base classes for each completion type.</p>"},{"location":"reference/types/base/#types.base.SparkChatCompletionMessages","title":"<code>SparkChatCompletionMessages = RootModel[List[SparkChatCompletionMessage]]</code>  <code>module-attribute</code>","text":"<p>A root model representing a list of SparkChatCompletionMessage objects.</p> <p>This model is used to represent an entire conversation or a series of messages in a chat completion context. It provides a convenient way to handle multiple messages as a single entity while maintaining Pydantic's validation and serialization capabilities.</p> Usage <pre><code>messages = SparkChatCompletionMessages(root=[\n    SparkChatCompletionMessage(role=\"user\", content=\"Hello\"),\n    SparkChatCompletionMessage(role=\"assistant\", content=\"Hi there!\")\n])\n</code></pre>"},{"location":"reference/types/base/#types.base.SparkChatCompletionMessage","title":"<code>SparkChatCompletionMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Spark-serializable chat completion message that can be used for any role in a conversation.</p> <p>This class provides a flexible structure for representing messages in a chat completion context, supporting various roles (user, assistant, system, tool) and additional attributes like image URLs and tool calls.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['user', 'assistant', 'system', 'tool']</code> <p>The role of the message sender.</p> <code>content</code> <code>Optional[str]</code> <p>The text content of the message.</p> <code>image_urls</code> <code>Optional[List[ImageURLPD]]</code> <p>List of image URLs associated with the message.</p> <code>name</code> <code>Optional[str]</code> <p>The name of the entity associated with the message.</p> <code>tool_calls</code> <code>Optional[List[ChatCompletionMessageToolCallParamPD]]</code> <p>Tool calls made in the message (for 'assistant' role).</p> <code>tool_call_id</code> <code>Optional[str]</code> <p>The ID of the tool call (for 'tool' role).</p> <code>cache_control</code> <code>Optional[bool]</code> <p>Whether to use Anthropic's prompt caching feature (beta).</p> <p>The class provides methods to format the message for different roles and serialize it to OpenAI-compatible types.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>class SparkChatCompletionMessage(BaseModel):\n    \"\"\"A Spark-serializable chat completion message that can be used for any role in a conversation.\n\n    This class provides a flexible structure for representing messages in a chat completion context,\n    supporting various roles (user, assistant, system, tool) and additional attributes like image URLs\n    and tool calls.\n\n    Attributes:\n        role (Literal[\"user\", \"assistant\", \"system\", \"tool\"]): The role of the message sender.\n        content (Optional[str]): The text content of the message.\n        image_urls (Optional[List[ImageURLPD]]): List of image URLs associated with the message.\n        name (Optional[str]): The name of the entity associated with the message.\n        tool_calls (Optional[List[ChatCompletionMessageToolCallParamPD]]): Tool calls made in the message (for 'assistant' role).\n        tool_call_id (Optional[str]): The ID of the tool call (for 'tool' role).\n        cache_control (Optional[bool]): Whether to use Anthropic's prompt caching feature (beta).\n\n    The class provides methods to format the message for different roles and serialize it to OpenAI-compatible types.\n    \"\"\"  # noqa: E501\n\n    role: Literal[\"user\", \"assistant\", \"system\", \"tool\"] = Field(\"user\", description=\"The role of the message.\")\n    content: Optional[str] = Field(None, description=\"The text content of the message.\")\n    image_urls: Optional[List[ImageURLPD]] = Field(None, description=\"The image urls of the message.\")  # type: ignore\n    name: Optional[str] = Field(None, description=\"The name of the relevant chat entity\")\n    tool_calls: Optional[List[ChatCompletionMessageToolCallParamPD]] = Field(  # type: ignore\n        None, description=\"The tool calls of the message (`assistant` role only)\"\n    )\n    tool_call_id: Optional[str] = Field(None, description=\"The tool call id of the message (`tool` role only).\")\n    cache_control: Optional[bool] = Field(None, description=\"Whether to use Anthropic cache control.\")\n\n    def content_formatted(self) -&gt; List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]:\n        \"\"\"Format the message content, including any images, into a list of content parts.\n\n        This method prepares the message content for use in API calls, handling both text and image content.\n\n        Returns:\n            List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]:\n                A list of formatted content parts, including text and images.\n        \"\"\"\n        results: List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]] = (\n            []\n            if not self.image_urls\n            else [\n                ChatCompletionContentPartImageParam(image_url=format_image_url_pd(image_url), type=\"image_url\")\n                for image_url in self.image_urls\n            ]\n        )\n        if self.content:\n            results.append(ChatCompletionContentPartTextParam(text=self.content, type=\"text\"))\n        return results\n\n    def as_user(self, string_only: bool = False) -&gt; ChatCompletionUserMessageParam:\n        \"\"\"Format the message as a user message.\n\n        This method prepares the message for use as a user input in a chat completion.\n\n        Args:\n            string_only (bool): If True, return only the text content for messages with images.\n\n        Returns:\n            ChatCompletionUserMessageParam: The formatted user message.\n        \"\"\"\n        if self.name:\n            if string_only and self.content:\n                return ChatCompletionUserMessageParam(content=self.content, role=\"user\", name=self.name)\n            return ChatCompletionUserMessageParam(content=self.content_formatted(), role=\"user\", name=self.name)\n        if string_only and self.content:\n            return ChatCompletionUserMessageParam(content=self.content, role=\"user\")\n        return ChatCompletionUserMessageParam(content=self.content_formatted(), role=\"user\")\n\n    def as_assistant(self) -&gt; ChatCompletionAssistantMessageParam:\n        \"\"\"Format the message as an assistant message.\n\n        This method prepares the message for use as an assistant response in a chat completion,\n        including any tool calls if present.\n\n        Returns:\n            ChatCompletionAssistantMessageParam: The formatted assistant message.\n        \"\"\"\n        if self.name:\n            if self.tool_calls:\n                return ChatCompletionAssistantMessageParam(\n                    content=self.content,\n                    role=\"assistant\",\n                    name=self.name,\n                    tool_calls=[call.model_dump() for call in self.tool_calls],  # type: ignore\n                )\n            return ChatCompletionAssistantMessageParam(content=self.content, role=\"assistant\", name=self.name)\n        if self.tool_calls:\n            return ChatCompletionAssistantMessageParam(\n                content=self.content,\n                role=\"assistant\",\n                tool_calls=[call.model_dump() for call in self.tool_calls],  # type: ignore\n            )\n        return ChatCompletionAssistantMessageParam(content=self.content, role=\"assistant\")\n\n    def as_system(self) -&gt; ChatCompletionSystemMessageParam:\n        \"\"\"Format the message as a system message.\n\n        This method prepares the message for use as a system instruction in a chat completion.\n\n        Returns:\n            ChatCompletionSystemMessageParam: The formatted system message.\n\n        Raises:\n            AssertionError: If the content is empty.\n        \"\"\"\n        assert self.content, \"`content` must not be empty\"\n        if self.name:\n            return ChatCompletionSystemMessageParam(content=self.content, role=\"system\", name=self.name)\n        return ChatCompletionSystemMessageParam(content=self.content, role=\"system\")\n\n    def as_tool(self) -&gt; ChatCompletionToolMessageParam:\n        \"\"\"Format the message as a tool message.\n\n        This method prepares the message for use as a tool response in a chat completion.\n\n        Returns:\n            ChatCompletionToolMessageParam: The formatted tool message.\n\n        Raises:\n            AssertionError: If either content or tool_call_id is empty.\n        \"\"\"\n        assert self.content and self.tool_call_id, \"`content` and `tool_call_id` must not be empty\"\n        return ChatCompletionToolMessageParam(content=self.content, role=\"tool\", tool_call_id=self.tool_call_id)\n\n    def __call__(self, *args, string_only: bool = False, **kwargs) -&gt; ChatCompletionMessageParam:\n        \"\"\"Serialize the message to an OpenAI-compatible type.\n\n        This method allows the object to be called directly, returning the appropriate\n        message type based on the role.\n\n        Args:\n            string_only (bool): If True, return only the text content.\n                The resulting user message will only have string content.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            ChatCompletionMessageParam: The serialized message in OpenAI-compatible format.\n        \"\"\"\n        if self.role == \"system\":\n            return self.as_system()\n        if self.role == \"user\":\n            return self.as_user(string_only=string_only)\n        if self.role == \"assistant\":\n            return self.as_assistant()\n        return self.as_tool()\n</code></pre>"},{"location":"reference/types/base/#types.base.SparkChatCompletionMessage.__call__","title":"<code>__call__(*args, string_only=False, **kwargs)</code>","text":"<p>Serialize the message to an OpenAI-compatible type.</p> <p>This method allows the object to be called directly, returning the appropriate message type based on the role.</p> <p>Parameters:</p> Name Type Description Default <code>string_only</code> <code>bool</code> <p>If True, return only the text content. The resulting user message will only have string content.</p> <code>False</code> <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ChatCompletionMessageParam</code> <code>ChatCompletionMessageParam</code> <p>The serialized message in OpenAI-compatible format.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def __call__(self, *args, string_only: bool = False, **kwargs) -&gt; ChatCompletionMessageParam:\n    \"\"\"Serialize the message to an OpenAI-compatible type.\n\n    This method allows the object to be called directly, returning the appropriate\n    message type based on the role.\n\n    Args:\n        string_only (bool): If True, return only the text content.\n            The resulting user message will only have string content.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        ChatCompletionMessageParam: The serialized message in OpenAI-compatible format.\n    \"\"\"\n    if self.role == \"system\":\n        return self.as_system()\n    if self.role == \"user\":\n        return self.as_user(string_only=string_only)\n    if self.role == \"assistant\":\n        return self.as_assistant()\n    return self.as_tool()\n</code></pre>"},{"location":"reference/types/base/#types.base.SparkChatCompletionMessage.as_assistant","title":"<code>as_assistant()</code>","text":"<p>Format the message as an assistant message.</p> <p>This method prepares the message for use as an assistant response in a chat completion, including any tool calls if present.</p> <p>Returns:</p> Name Type Description <code>ChatCompletionAssistantMessageParam</code> <code>ChatCompletionAssistantMessageParam</code> <p>The formatted assistant message.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def as_assistant(self) -&gt; ChatCompletionAssistantMessageParam:\n    \"\"\"Format the message as an assistant message.\n\n    This method prepares the message for use as an assistant response in a chat completion,\n    including any tool calls if present.\n\n    Returns:\n        ChatCompletionAssistantMessageParam: The formatted assistant message.\n    \"\"\"\n    if self.name:\n        if self.tool_calls:\n            return ChatCompletionAssistantMessageParam(\n                content=self.content,\n                role=\"assistant\",\n                name=self.name,\n                tool_calls=[call.model_dump() for call in self.tool_calls],  # type: ignore\n            )\n        return ChatCompletionAssistantMessageParam(content=self.content, role=\"assistant\", name=self.name)\n    if self.tool_calls:\n        return ChatCompletionAssistantMessageParam(\n            content=self.content,\n            role=\"assistant\",\n            tool_calls=[call.model_dump() for call in self.tool_calls],  # type: ignore\n        )\n    return ChatCompletionAssistantMessageParam(content=self.content, role=\"assistant\")\n</code></pre>"},{"location":"reference/types/base/#types.base.SparkChatCompletionMessage.as_system","title":"<code>as_system()</code>","text":"<p>Format the message as a system message.</p> <p>This method prepares the message for use as a system instruction in a chat completion.</p> <p>Returns:</p> Name Type Description <code>ChatCompletionSystemMessageParam</code> <code>ChatCompletionSystemMessageParam</code> <p>The formatted system message.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the content is empty.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def as_system(self) -&gt; ChatCompletionSystemMessageParam:\n    \"\"\"Format the message as a system message.\n\n    This method prepares the message for use as a system instruction in a chat completion.\n\n    Returns:\n        ChatCompletionSystemMessageParam: The formatted system message.\n\n    Raises:\n        AssertionError: If the content is empty.\n    \"\"\"\n    assert self.content, \"`content` must not be empty\"\n    if self.name:\n        return ChatCompletionSystemMessageParam(content=self.content, role=\"system\", name=self.name)\n    return ChatCompletionSystemMessageParam(content=self.content, role=\"system\")\n</code></pre>"},{"location":"reference/types/base/#types.base.SparkChatCompletionMessage.as_tool","title":"<code>as_tool()</code>","text":"<p>Format the message as a tool message.</p> <p>This method prepares the message for use as a tool response in a chat completion.</p> <p>Returns:</p> Name Type Description <code>ChatCompletionToolMessageParam</code> <code>ChatCompletionToolMessageParam</code> <p>The formatted tool message.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If either content or tool_call_id is empty.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def as_tool(self) -&gt; ChatCompletionToolMessageParam:\n    \"\"\"Format the message as a tool message.\n\n    This method prepares the message for use as a tool response in a chat completion.\n\n    Returns:\n        ChatCompletionToolMessageParam: The formatted tool message.\n\n    Raises:\n        AssertionError: If either content or tool_call_id is empty.\n    \"\"\"\n    assert self.content and self.tool_call_id, \"`content` and `tool_call_id` must not be empty\"\n    return ChatCompletionToolMessageParam(content=self.content, role=\"tool\", tool_call_id=self.tool_call_id)\n</code></pre>"},{"location":"reference/types/base/#types.base.SparkChatCompletionMessage.as_user","title":"<code>as_user(string_only=False)</code>","text":"<p>Format the message as a user message.</p> <p>This method prepares the message for use as a user input in a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>string_only</code> <code>bool</code> <p>If True, return only the text content for messages with images.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ChatCompletionUserMessageParam</code> <code>ChatCompletionUserMessageParam</code> <p>The formatted user message.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def as_user(self, string_only: bool = False) -&gt; ChatCompletionUserMessageParam:\n    \"\"\"Format the message as a user message.\n\n    This method prepares the message for use as a user input in a chat completion.\n\n    Args:\n        string_only (bool): If True, return only the text content for messages with images.\n\n    Returns:\n        ChatCompletionUserMessageParam: The formatted user message.\n    \"\"\"\n    if self.name:\n        if string_only and self.content:\n            return ChatCompletionUserMessageParam(content=self.content, role=\"user\", name=self.name)\n        return ChatCompletionUserMessageParam(content=self.content_formatted(), role=\"user\", name=self.name)\n    if string_only and self.content:\n        return ChatCompletionUserMessageParam(content=self.content, role=\"user\")\n    return ChatCompletionUserMessageParam(content=self.content_formatted(), role=\"user\")\n</code></pre>"},{"location":"reference/types/base/#types.base.SparkChatCompletionMessage.content_formatted","title":"<code>content_formatted()</code>","text":"<p>Format the message content, including any images, into a list of content parts.</p> <p>This method prepares the message content for use in API calls, handling both text and image content.</p> <p>Returns:</p> Type Description <code>List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]</code> <p>List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]: A list of formatted content parts, including text and images.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def content_formatted(self) -&gt; List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]:\n    \"\"\"Format the message content, including any images, into a list of content parts.\n\n    This method prepares the message content for use in API calls, handling both text and image content.\n\n    Returns:\n        List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]]:\n            A list of formatted content parts, including text and images.\n    \"\"\"\n    results: List[Union[ChatCompletionContentPartTextParam, ChatCompletionContentPartImageParam]] = (\n        []\n        if not self.image_urls\n        else [\n            ChatCompletionContentPartImageParam(image_url=format_image_url_pd(image_url), type=\"image_url\")\n            for image_url in self.image_urls\n        ]\n    )\n    if self.content:\n        results.append(ChatCompletionContentPartTextParam(text=self.content, type=\"text\"))\n    return results\n</code></pre>"},{"location":"reference/types/base/#types.base.format_image_url_pd","title":"<code>format_image_url_pd(image_url_pd)</code>","text":"<p>Format the ImageURLPD object into an ImageURL object.</p> Source code in <code>spark_instructor/types/base.py</code> <pre><code>def format_image_url_pd(image_url_pd: BaseModel) -&gt; ImageURL:\n    \"\"\"Format the ImageURLPD object into an ImageURL object.\"\"\"\n    model_dump = image_url_pd.model_dump()\n    if not model_dump[\"detail\"]:\n        return ImageURL(url=model_dump[\"url\"])\n    return ImageURL(url=model_dump[\"url\"], detail=model_dump[\"detail\"])\n</code></pre>"},{"location":"reference/types/openai_types/","title":"openai_types","text":"<p>A module for defining OpenAI classes for each completion type.</p>"},{"location":"reference/udf/","title":"udf","text":"<p>Package for defining <code>spark-instructor</code> user-defined functions in Spark.</p>"},{"location":"reference/udf/instruct/","title":"instruct","text":"<p>Module for defining spark instruct functions.</p>"},{"location":"reference/udf/instruct/#udf.instruct.AsyncRetryingConfig","title":"<code>AsyncRetryingConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Config for creating an <code>tenacity.AsyncRetrying</code> class.</p> Source code in <code>spark_instructor/udf/instruct.py</code> <pre><code>class AsyncRetryingConfig(TypedDict, total=False):\n    \"\"\"Config for creating an ``tenacity.AsyncRetrying`` class.\"\"\"\n\n    max_attempts: int\n    wait_multiplier: float\n    wait_min: float\n    wait_max: float\n    retry_exception: Type[Exception]\n    reraise: bool\n</code></pre>"},{"location":"reference/udf/instruct/#udf.instruct.create_async_retrying","title":"<code>create_async_retrying(config)</code>","text":"<p>Create a <code>tenacity.AsyncRetrying</code> object from a configuration dictionary.</p> <p>pyspark has trouble serializing <code>tenacity.AsyncRetrying</code> objects, so we can create one within the UDF instead.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AsyncRetryingConfig</code> <p>A dictionary containing retry configuration parameters</p> required <p>Returns     An AsyncRetrying object</p> Source code in <code>spark_instructor/udf/instruct.py</code> <pre><code>def create_async_retrying(config: AsyncRetryingConfig) -&gt; AsyncRetrying:\n    \"\"\"Create a ``tenacity.AsyncRetrying`` object from a configuration dictionary.\n\n    pyspark has trouble serializing ``tenacity.AsyncRetrying`` objects, so we can create one within the UDF instead.\n\n    Args:\n        config (AsyncRetryingConfig): A dictionary containing retry configuration parameters\n    Returns\n        An AsyncRetrying object\n    \"\"\"\n    stop_strategy = stop_after_attempt(config.get(\"max_attempts\", 5))\n\n    wait_strategy = wait_exponential(\n        multiplier=config.get(\"wait_multiplier\", 1), min=config.get(\"wait_min\", 1), max=config.get(\"wait_max\", 60)\n    )\n\n    retry_exception = config.get(\"retry_exception\", Exception)\n    if not isinstance(retry_exception, type):\n        raise ValueError(\"retry_exception must be a type\")\n\n    retry_strategy = retry_if_exception_type(retry_exception)\n\n    return AsyncRetrying(\n        stop=stop_strategy, wait=wait_strategy, retry=retry_strategy, reraise=config.get(\"reraise\", True)\n    )\n</code></pre>"},{"location":"reference/udf/instruct/#udf.instruct.get_or_create_event_loop","title":"<code>get_or_create_event_loop()</code>","text":"<p>Get the current event loop or create a new one.</p> Source code in <code>spark_instructor/udf/instruct.py</code> <pre><code>def get_or_create_event_loop() -&gt; asyncio.AbstractEventLoop:\n    \"\"\"Get the current event loop or create a new one.\"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n        if loop.is_closed():\n            raise RuntimeError\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n    return loop\n</code></pre>"},{"location":"reference/udf/instruct/#udf.instruct.instruct","title":"<code>instruct(response_model=None, default_model=None, default_model_class=None, default_mode=None, default_max_tokens=None, default_temperature=None, default_max_retries=1, registry=ClientRegistry(), concurrency_limit=16, task_timeout=600, logger=default_logger, safe_mode=False, enable_caching=False, **kwargs)</code>","text":"<p>Create a pandas UDF for serving model responses in a Spark environment.</p> <p>This function generates a UDF that can process conversations and return model responses, optionally structured according to a Pydantic model. It supports various configuration options and can work with different model types and completion modes.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>Optional[Type[T]]</code> <p>The Pydantic model type for the response. If None, the UDF will return a standard chat completion message as a string.</p> <code>None</code> <code>default_model</code> <code>Optional[str]</code> <p>The default model to use if not specified in the UDF call.</p> <code>None</code> <code>default_model_class</code> <code>Optional[str]</code> <p>The default model class to use if not specified.</p> <code>None</code> <code>default_mode</code> <code>Optional[Mode]</code> <p>The default instructor mode to use.</p> <code>None</code> <code>default_max_tokens</code> <code>Optional[int]</code> <p>The default maximum number of tokens for the response.</p> <code>None</code> <code>default_temperature</code> <code>Optional[float]</code> <p>The default temperature for the model's output.</p> <code>None</code> <code>default_max_retries</code> <code>Optional[int | AsyncRetryingConfig</code> <p>The default maximum number of retries for failed requests. Can be a dictionary for creating an exponential backoff using <code>create_async_retrying</code>.</p> <code>1</code> <code>registry</code> <code>ClientRegistry</code> <p>The client registry for routing requests to appropriate model factories.</p> <code>ClientRegistry()</code> <code>concurrency_limit</code> <code>int</code> <p>The concurrency limit for the Sephamore.</p> <code>16</code> <code>task_timeout</code> <code>float</code> <p>The timeout to use before raising an exception. This represents timeout on the task level.</p> <code>600</code> <code>logger</code> <code>Logger</code> <p>Logger to use.</p> <code>default_logger</code> <code>safe_mode</code> <code>bool</code> <p>If True, return a null row instead of raising an exception when an error occurs. Recommended for large batches.</p> <code>False</code> <code>enable_caching</code> <code>bool</code> <p>If True, enable Anthropic prompt caching.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model creation function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A pandas UDF that can be used in Spark DataFrame operations.</p> The returned UDF accepts the following parameters <ul> <li>conversation (Column): Column containing conversation data as SparkChatCompletionMessages.</li> <li>model (Optional[Column]): Column containing model names.</li> <li>model_class (Optional[Column]): Column containing model classes.</li> <li>mode (Optional[Column]): Column containing instructor modes.</li> <li>max_tokens (Optional[Column]): Column containing maximum token values.</li> <li>temperature (Optional[Column]): Column containing temperature values.</li> <li>max_retries (Optional[Column]): Column containing maximum retry values.</li> </ul> <p>If any of the optional parameters are not provided in the UDF call, they will use the default values specified in the <code>instruct</code> function arguments.</p> <p>The UDF processes each row asynchronously, allowing for efficient parallel processing of multiple conversations.</p> Example <pre><code>&gt;&gt;&gt; from databricks.connect import DatabricksSession\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt; from pyspark.sql.functions import lit\n&gt;&gt;&gt; from spark_instructor.utils.prompt import create_chat_completion_messages\n&gt;&gt;&gt; from spark_instructor.response_models import TextResponse\n&gt;&gt;&gt; import json\n&gt;&gt;&gt;\n&gt;&gt;&gt; spark = DatabricksSession.builder.serverless().getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame([(\"What is the capital of France?\",)], [\"content\"])\n&gt;&gt;&gt; df = df.withColumn(\"conversation\", create_chat_completion_messages([{\"role\": lit(\"user\"), \"content\": \"content\"}]))\n&gt;&gt;&gt; instruct_udf = instruct(TextResponse, default_model=\"gpt-4o-mini\")\n&gt;&gt;&gt; result_df = df.withColumn(\"response\", instruct_udf(\"conversation\"))\n&gt;&gt;&gt; result_df.schema.jsonValue()\n{'type': 'struct', 'fields': [{'name': 'content', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'conversation', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'role', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'content', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'image_urls', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'url', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'detail', 'type': 'string', 'nullable': True, 'metadata': {}}]}, 'containsNull': True}, 'nullable': True, 'metadata': {}}, {'name': 'name', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'tool_calls', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'id', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'function', 'type': {'type': 'struct', 'fields': [{'name': 'arguments', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'name', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': False, 'metadata': {}}, {'name': 'type', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'containsNull': True}, 'nullable': True, 'metadata': {}}, {'name': 'tool_call_id', 'type': 'string', 'nullable': True, 'metadata': {}}]}, 'containsNull': False}, 'nullable': False, 'metadata': {}}, {'name': 'response', 'type': {'type': 'struct', 'fields': [{'name': 'text_response', 'type': {'type': 'struct', 'fields': [{'name': 'text', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}, {'name': 'chat_completion', 'type': {'type': 'struct', 'fields': [{'name': 'id', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'choices', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'finish_reason', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'index', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'logprobs', 'type': {'type': 'struct', 'fields': [{'name': 'content', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'token', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'bytes', 'type': {'type': 'array', 'elementType': 'integer', 'containsNull': False}, 'nullable': True, 'metadata': {}}, {'name': 'logprob', 'type': 'double', 'nullable': False, 'metadata': {}}, {'name': 'top_logprobs', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'token', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'bytes', 'type': {'type': 'array', 'elementType': 'integer', 'containsNull': False}, 'nullable': True, 'metadata': {}}, {'name': 'logprob', 'type': 'double', 'nullable': False, 'metadata': {}}]}, 'containsNull': False}, 'nullable': False, 'metadata': {}}]}, 'containsNull': True}, 'nullable': True, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}, {'name': 'message', 'type': {'type': 'struct', 'fields': [{'name': 'content', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'role', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'function_call', 'type': {'type': 'struct', 'fields': [{'name': 'arguments', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'name', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}, {'name': 'tool_calls', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'id', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'function', 'type': {'type': 'struct', 'fields': [{'name': 'arguments', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'name', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': False, 'metadata': {}}, {'name': 'type', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'containsNull': True}, 'nullable': True, 'metadata': {}}]}, 'nullable': False, 'metadata': {}}]}, 'containsNull': False}, 'nullable': False, 'metadata': {}}, {'name': 'created', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'model', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'object', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'service_tier', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'system_fingerprint', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'usage', 'type': {'type': 'struct', 'fields': [{'name': 'completion_tokens', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'prompt_tokens', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'total_tokens', 'type': 'integer', 'nullable': False, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}]}\n</code></pre> <p>Note:     - The UDF uses the provided ClientRegistry to determine which model factory to use.     - If <code>response_model</code> is None, the UDF will return the raw completion message as a string.     - The function supports both structured (Pydantic model) and unstructured (string) responses.</p> Source code in <code>spark_instructor/udf/instruct.py</code> <pre><code>def instruct(\n    response_model: Optional[Type[T]] = None,\n    default_model: Optional[str] = None,\n    default_model_class: Optional[str] = None,\n    default_mode: Optional[instructor.Mode] = None,\n    default_max_tokens: Optional[int] = None,\n    default_temperature: Optional[float] = None,\n    default_max_retries: Optional[int | AsyncRetryingConfig] = 1,\n    registry: ClientRegistry = ClientRegistry(),\n    concurrency_limit: int = 16,\n    task_timeout: float = 600,\n    logger: logging.Logger = default_logger,\n    safe_mode: bool = False,\n    enable_caching: bool = False,\n    **kwargs,\n) -&gt; Callable:\n    \"\"\"Create a pandas UDF for serving model responses in a Spark environment.\n\n    This function generates a UDF that can process conversations and return model responses,\n    optionally structured according to a Pydantic model. It supports various configuration\n    options and can work with different model types and completion modes.\n\n    Args:\n        response_model (Optional[Type[T]]): The Pydantic model type for the response.\n            If None, the UDF will return a standard chat completion message as a string.\n        default_model (Optional[str]): The default model to use if not specified in the UDF call.\n        default_model_class (Optional[str]): The default model class to use if not specified.\n        default_mode (Optional[instructor.Mode]): The default instructor mode to use.\n        default_max_tokens (Optional[int]): The default maximum number of tokens for the response.\n        default_temperature (Optional[float]): The default temperature for the model's output.\n        default_max_retries (Optional[int | AsyncRetryingConfig): The default maximum number of retries for failed requests.\n            Can be a dictionary for creating an exponential backoff using ``create_async_retrying``.\n        registry (ClientRegistry): The client registry for routing requests to appropriate model factories.\n        concurrency_limit (int): The concurrency limit for the Sephamore.\n        task_timeout (float): The timeout to use before raising an exception.\n            This represents timeout on the task level.\n        logger (Logger): Logger to use.\n        safe_mode (bool): If True, return a null row instead of raising an exception when an error occurs.\n            Recommended for large batches.\n        enable_caching (bool): If True, enable Anthropic prompt caching.\n        **kwargs: Additional keyword arguments to pass to the model creation function.\n\n    Returns:\n        Callable: A pandas UDF that can be used in Spark DataFrame operations.\n\n    The returned UDF accepts the following parameters:\n        - conversation (Column): Column containing conversation data as SparkChatCompletionMessages.\n        - model (Optional[Column]): Column containing model names.\n        - model_class (Optional[Column]): Column containing model classes.\n        - mode (Optional[Column]): Column containing instructor modes.\n        - max_tokens (Optional[Column]): Column containing maximum token values.\n        - temperature (Optional[Column]): Column containing temperature values.\n        - max_retries (Optional[Column]): Column containing maximum retry values.\n\n    If any of the optional parameters are not provided in the UDF call, they will use the default\n    values specified in the `instruct` function arguments.\n\n    The UDF processes each row asynchronously, allowing for efficient parallel processing of\n    multiple conversations.\n\n    Example:\n        ```python\n\n        &gt;&gt;&gt; from databricks.connect import DatabricksSession\n        &gt;&gt;&gt; from pydantic import BaseModel\n        &gt;&gt;&gt; from pyspark.sql.functions import lit\n        &gt;&gt;&gt; from spark_instructor.utils.prompt import create_chat_completion_messages\n        &gt;&gt;&gt; from spark_instructor.response_models import TextResponse\n        &gt;&gt;&gt; import json\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; spark = DatabricksSession.builder.serverless().getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"What is the capital of France?\",)], [\"content\"])\n        &gt;&gt;&gt; df = df.withColumn(\"conversation\", create_chat_completion_messages([{\"role\": lit(\"user\"), \"content\": \"content\"}]))\n        &gt;&gt;&gt; instruct_udf = instruct(TextResponse, default_model=\"gpt-4o-mini\")\n        &gt;&gt;&gt; result_df = df.withColumn(\"response\", instruct_udf(\"conversation\"))\n        &gt;&gt;&gt; result_df.schema.jsonValue()\n        {'type': 'struct', 'fields': [{'name': 'content', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'conversation', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'role', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'content', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'image_urls', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'url', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'detail', 'type': 'string', 'nullable': True, 'metadata': {}}]}, 'containsNull': True}, 'nullable': True, 'metadata': {}}, {'name': 'name', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'tool_calls', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'id', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'function', 'type': {'type': 'struct', 'fields': [{'name': 'arguments', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'name', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': False, 'metadata': {}}, {'name': 'type', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'containsNull': True}, 'nullable': True, 'metadata': {}}, {'name': 'tool_call_id', 'type': 'string', 'nullable': True, 'metadata': {}}]}, 'containsNull': False}, 'nullable': False, 'metadata': {}}, {'name': 'response', 'type': {'type': 'struct', 'fields': [{'name': 'text_response', 'type': {'type': 'struct', 'fields': [{'name': 'text', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}, {'name': 'chat_completion', 'type': {'type': 'struct', 'fields': [{'name': 'id', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'choices', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'finish_reason', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'index', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'logprobs', 'type': {'type': 'struct', 'fields': [{'name': 'content', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'token', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'bytes', 'type': {'type': 'array', 'elementType': 'integer', 'containsNull': False}, 'nullable': True, 'metadata': {}}, {'name': 'logprob', 'type': 'double', 'nullable': False, 'metadata': {}}, {'name': 'top_logprobs', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'token', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'bytes', 'type': {'type': 'array', 'elementType': 'integer', 'containsNull': False}, 'nullable': True, 'metadata': {}}, {'name': 'logprob', 'type': 'double', 'nullable': False, 'metadata': {}}]}, 'containsNull': False}, 'nullable': False, 'metadata': {}}]}, 'containsNull': True}, 'nullable': True, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}, {'name': 'message', 'type': {'type': 'struct', 'fields': [{'name': 'content', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'role', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'function_call', 'type': {'type': 'struct', 'fields': [{'name': 'arguments', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'name', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}, {'name': 'tool_calls', 'type': {'type': 'array', 'elementType': {'type': 'struct', 'fields': [{'name': 'id', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'function', 'type': {'type': 'struct', 'fields': [{'name': 'arguments', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'name', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': False, 'metadata': {}}, {'name': 'type', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'containsNull': True}, 'nullable': True, 'metadata': {}}]}, 'nullable': False, 'metadata': {}}]}, 'containsNull': False}, 'nullable': False, 'metadata': {}}, {'name': 'created', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'model', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'object', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'service_tier', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'system_fingerprint', 'type': 'string', 'nullable': True, 'metadata': {}}, {'name': 'usage', 'type': {'type': 'struct', 'fields': [{'name': 'completion_tokens', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'prompt_tokens', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'total_tokens', 'type': 'integer', 'nullable': False, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}]}, 'nullable': True, 'metadata': {}}]}\n\n        ```\n    Note:\n        - The UDF uses the provided ClientRegistry to determine which model factory to use.\n        - If `response_model` is None, the UDF will return the raw completion message as a string.\n        - The function supports both structured (Pydantic model) and unstructured (string) responses.\n    \"\"\"  # noqa: E501\n    logger.info(\n        f\"Initializing instruct function with concurrency limit: \"\n        f\"{concurrency_limit}, timeout: {task_timeout}, safe_mode: {safe_mode}\"\n    )\n    model_serializer = ModelSerializer(response_model, OpenAICompletion)\n    response_model_name = model_serializer.response_model_name\n    completion_model_name = model_serializer.completion_model_name\n\n    @pandas_udf(returnType=model_serializer.spark_schema)  # type: ignore\n    def _pandas_udf(\n        conversation: pd.Series,\n        model: pd.Series,\n        model_class: pd.Series,\n        mode: pd.Series,\n        max_tokens: pd.Series,\n        temperature: pd.Series,\n        max_retries: pd.Series,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Pandas UDF for processing conversations and generating model responses.\n\n        Args:\n            conversation (pd.Series): DataFrame containing conversation data.\n            model (pd.Series): Series containing model names.\n            model_class (pd.Series): Series containing model classes.\n            mode (pd.Series): Series containing modes.\n            max_tokens (pd.Series): Series containing maximum token values.\n            temperature (pd.Series): Series containing temperature values.\n            max_retries (pd.Series): Series containing maximum retry values.\n\n        Returns:\n            pd.DataFrame: DataFrame containing the processed responses.\n        \"\"\"\n        # Convert dataframe rows to list of Conversation objects\n        logger.info(f\"Processing batch of {len(conversation)} conversations\")\n        semaphore = asyncio.Semaphore(concurrency_limit)\n\n        async def process_row(\n            conversation_: str,\n            model_: str,\n            model_class_: str,\n            mode_: str,\n            max_tokens_: int,\n            temperature_: float,\n            max_retries_: int | AsyncRetryingConfig,\n        ) -&gt; Optional[Any]:\n            logger.debug(f\"Processing row with model: {model_}, model_class: {model_class_}\")\n            factory_type = (\n                registry.get_factory(model_class_)\n                if model_class_ is not None\n                else registry.get_factory_from_model(model_)\n            )\n            factory = factory_type.from_config(\n                instructor.Mode(mode_) if mode_ is not None else mode_, enable_caching=enable_caching\n            )\n            create_fn = factory.create_with_completion if response_model else factory.create\n            try:\n                async with timeout(task_timeout):\n                    result = await create_fn(\n                        messages=SparkChatCompletionMessages.model_validate_json(conversation_),\n                        response_model=response_model,  # type: ignore\n                        model=model_,\n                        max_tokens=max_tokens_,\n                        temperature=temperature_,\n                        max_retries=(\n                            create_async_retrying(max_retries_)\n                            if isinstance(max_retries_, dict)\n                            else max_retries_  # type: ignore\n                        ),\n                        **kwargs,\n                    )\n                logger.debug(\"Row processed successfully\")\n                return result\n            except asyncio.TimeoutError:\n                logger.error(f\"Timeout occurred while processing row with model: {model_}\")\n                if safe_mode:\n                    return None\n                raise\n            except Exception as e:\n                logger.error(f\"Error processing row with model {model_}: {str(e)}\")\n                if safe_mode:\n                    return None\n                raise\n\n        async def process_row_with_semaphore(\n            conversation_,\n            model_,\n            model_class_,\n            mode_,\n            max_tokens_,\n            temperature_,\n            max_retries_,\n        ) -&gt; dict[str, Any]:\n            async with semaphore:\n                result = await process_row(\n                    conversation_,\n                    model_,\n                    model_class_,\n                    mode_,\n                    max_tokens_,\n                    temperature_,\n                    max_retries_,\n                )\n                if result is None:\n                    # Ensure failed result is serializable\n                    return (\n                        {completion_model_name: None}\n                        if not response_model_name\n                        else {response_model_name: None, completion_model_name: None}\n                    )\n                if not response_model_name:\n                    # Basic text response\n                    return {completion_model_name: result.model_dump()}\n                # Structured response\n                return {\n                    response_model_name: result[0].model_dump(),\n                    completion_model_name: result[1].model_dump(),\n                }\n\n        async def process_all_rows():\n            tasks = [\n                process_row_with_semaphore(\n                    conv,\n                    mdl or default_model,\n                    mdl_cls,\n                    md,\n                    max_tkns or default_max_tokens,\n                    temp or default_temperature,\n                    max_rtr or default_max_retries,\n                )\n                for conv, mdl, mdl_cls, md, max_tkns, temp, max_rtr in zip(\n                    conversation,\n                    model,\n                    model_class,\n                    mode,\n                    max_tokens,\n                    temperature,\n                    max_retries,\n                )\n            ]\n            return await asyncio.gather(*tasks)\n\n        loop = get_or_create_event_loop()\n        try:\n            results = loop.run_until_complete(process_all_rows())\n            logger.info(f\"Successfully processed {len(results)} rows\")\n        except Exception as e:\n            logger.error(f\"Error processing batch: {str(e)}\")\n            raise\n\n        # Convert results to DataFrame\n        return pd.DataFrame(results)\n\n    def pandas_udf_wrapped(\n        conversation: Column,\n        model: Optional[Column] = None,\n        model_class: Optional[Column] = None,\n        mode: Optional[Column] = None,\n        max_tokens: Optional[Column] = None,\n        temperature: Optional[Column] = None,\n        max_retries: Optional[Column] = None,\n    ) -&gt; Column:\n        \"\"\"Create a pandas UDF that wraps the model inference function.\n\n        Column arguments which are not passed will be set to their defaults provided by the constructor.\n\n        Args:\n            conversation (Column): Column containing conversation data.\n            model (Optional[Column]): Column containing model names.\n            model_class (Optional[Column]): Column containing model classes.\n            mode (Optional[Column]): Column containing modes.\n            max_tokens (Optional[Column]): Column containing maximum token values.\n            temperature (Optional[Column]): Column containing temperature values.\n            max_retries (Optional[Column]): Column containing maximum retry values.\n\n        Returns:\n            Column: Column containing the processed responses.\n        \"\"\"\n        if model is None:\n            model = lit(default_model)\n        if max_tokens is None:\n            max_tokens = lit(default_max_tokens)\n        if temperature is None:\n            temperature = lit(default_temperature)\n        if max_retries is None:\n            max_retries = lit(default_max_retries) if isinstance(default_max_retries, int) else lit(None)\n        if model_class is None:\n            model_class = lit(default_model_class) if default_model_class else lit(None)\n        if mode is None:\n            mode = lit(default_mode.value) if default_mode else lit(None)\n\n        return _pandas_udf(to_json(conversation), model, model_class, mode, max_tokens, temperature, max_retries)\n\n    return pandas_udf_wrapped\n</code></pre>"},{"location":"reference/udf/message_router/","title":"message_router","text":"<p>Module for <code>MessageRouter</code>.</p>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter","title":"<code>MessageRouter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[ResponseModel]</code></p> <p>A wrapper for serializing <code>instructor</code> calls and managing model interactions.</p> <p>This class provides methods to create Pydantic objects and completions from chat messages, and to generate Spark UDFs for these operations.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The name of the model to use.</p> <code>response_model_type</code> <code>Type[ResponseModel]</code> <p>The Pydantic model type for the response.</p> <code>model_class</code> <code>Optional[ModelClass]</code> <p>The class of the model (e.g., <code>ModelClass.OPENAI</code>). If not provided, it will be inferred based on the <code>model</code>.</p> <code>mode</code> <code>Optional[Mode]</code> <p>The mode for the instructor client.</p> <code>base_url</code> <code>Optional[str]</code> <p>The base URL for API calls.</p> <code>api_key</code> <code>Optional[str]</code> <p>The API key for authentication.</p> Notes <p>WARNING: <code>MessageRouter</code> is now deprecated. Use <code>instruct</code> instead.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>@dataclass\nclass MessageRouter(Generic[ResponseModel]):\n    \"\"\"A wrapper for serializing ``instructor`` calls and managing model interactions.\n\n    This class provides methods to create Pydantic objects and completions from chat messages,\n    and to generate Spark UDFs for these operations.\n\n    Attributes:\n        model (str): The name of the model to use.\n        response_model_type (Type[ResponseModel]): The Pydantic model type for the response.\n        model_class (Optional[ModelClass]): The class of the model (e.g., ``ModelClass.OPENAI``).\n            If not provided, it will be inferred based on the ``model``.\n        mode (Optional[Mode]): The mode for the instructor client.\n        base_url (Optional[str]): The base URL for API calls.\n        api_key (Optional[str]): The API key for authentication.\n\n    Notes:\n        **WARNING:** ``MessageRouter`` is now deprecated. Use ``instruct`` instead.\n    \"\"\"\n\n    model: str\n    response_model_type: Type[ResponseModel]\n    model_class: Optional[ModelClass] = None\n    mode: Optional[Mode] = None\n    base_url: Optional[str] = None\n    api_key: Optional[str] = None\n\n    def __post_init__(self):\n        \"\"\"Initialize the ``model_class`` if not provided.\n\n        The ``model_class`` will be inferred based on the ``model`` attribute.\n        \"\"\"\n        warnings.warn(\n            \"`MessageRouter` is deprecated and may be removed in future versions. \" \"Please use `instruct` instead.\",\n            DeprecationWarning,\n        )\n        if self.model_class is None:\n            self.model_class = infer_model_class(self.model)\n\n    @property\n    def completion_type(self) -&gt; Union[Type[\"AnthropicCompletion\"], Type[DatabricksCompletion], Type[OpenAICompletion]]:\n        \"\"\"Get the appropriate completion type based on the ``model_class`` attribute.\n\n        Returns:\n            Union[Type[AnthropicCompletion], Type[DatabricksCompletion], Type[OpenAICompletion]]:\n                The completion type corresponding to the model class.\n        \"\"\"\n        if self.model_class == ModelClass.ANTHROPIC:\n            if is_anthropic_available():\n                from spark_instructor.completions.anthropic_completions import (\n                    AnthropicCompletion,\n                )\n\n                return AnthropicCompletion\n            else:\n                raise ImportError(\n                    \"Please install ``anthropic`` by running ``pip install anthropic`` \"\n                    \"or ``pip install spark-instructor[anthropic]``\"\n                )\n        if self.model_class == ModelClass.DATABRICKS:\n            return DatabricksCompletion\n        return OpenAICompletion\n\n    @property\n    def model_serializer(self) -&gt; ModelSerializer:\n        \"\"\"Get the model serializer for the response model type and completion type.\n\n        Returns:\n            ModelSerializer: An instance of ModelSerializer.\n        \"\"\"\n        return ModelSerializer(self.response_model_type, self.completion_type)\n\n    @property\n    def spark_schema(self) -&gt; StructType:\n        \"\"\"Get the Spark schema for the model.\n\n        Returns:\n            StructType: The Spark schema corresponding to the model.\n        \"\"\"\n        return self.model_serializer.spark_schema\n\n    def get_instructor(self) -&gt; instructor.Instructor:\n        \"\"\"Get an instance of the instructor client.\n\n        Returns:\n            instructor.Instructor: An initialized instructor client.\n        \"\"\"\n        return get_instructor(\n            model_class=self.model_class, mode=self.mode, api_key=self.api_key, base_url=self.base_url\n        )\n\n    def create_object_from_messages(self, messages: list[ChatCompletionMessageParam], **kwargs: Any) -&gt; ResponseModel:\n        \"\"\"Create a Pydantic object response from chat messages.\n\n        Args:\n            messages (list[ChatCompletionMessageParam]): The list of chat messages.\n            **kwargs (Any): Additional keyword arguments for the chat completion.\n\n        Returns:\n            ResponseModel: A Pydantic object representing the response.\n        \"\"\"\n        client = self.get_instructor()\n        return client.chat.completions.create(\n            model=self.model, response_model=self.response_model_type, messages=messages, **kwargs\n        )\n\n    def create_object_from_messages_udf(self, **kwargs: Any) -&gt; Callable:\n        \"\"\"Create a Spark UDF that returns a ``StructType`` response based on the ``response_model_type`` attribute.\n\n        Args:\n            **kwargs (Any): Additional keyword arguments for the chat completion.\n\n        Returns:\n            Callable: A Spark UDF that takes messages and returns a serialized object.\n        \"\"\"\n\n        def _func(messages: list[ChatCompletionMessageParam]) -&gt; ResponseModel:\n            return self.create_object_from_messages(messages, **kwargs)\n\n        schema = self.model_serializer.response_model_spark_schema\n        assert schema, \"Null response models are not supported by `MessageRouter`\"\n\n        @udf(returnType=schema)\n        def func(messages: list[ChatCompletionMessageParam]) -&gt; Dict[str, Any]:\n            return _func(messages).model_dump()\n\n        return func\n\n    def create_object_and_completion_from_messages(\n        self, messages: list[ChatCompletionMessageParam], **kwargs: Any\n    ) -&gt; Tuple[ResponseModel, Union[\"AnthropicCompletion\", DatabricksCompletion, OpenAICompletion]]:\n        \"\"\"Create a Pydantic object response and completion using the ``instructor`` client.\n\n        The completion will be of the type corresponding to the ``model_class`` attribute.\n\n        Args:\n            messages (list[ChatCompletionMessageParam]): The list of chat messages.\n            **kwargs (Any): Additional keyword arguments for the chat completion.\n\n        Returns:\n            Tuple[ResponseModel, Union[AnthropicCompletion, DatabricksCompletion, OpenAICompletion]]:\n                A tuple containing the Pydantic object and the completion.\n        \"\"\"\n        client = self.get_instructor()\n        pydantic_object, completion = client.chat.completions.create_with_completion(\n            model=self.model, response_model=self.response_model_type, messages=messages, **kwargs\n        )\n        return pydantic_object, completion\n\n    def create_object_and_completion_from_messages_udf(self, **kwargs: Any) -&gt; Callable:\n        \"\"\"Create a Spark UDF that returns a ``StructType``.\n\n        The response will be based on the ``response_model_type`` and ``model_class`` attributes.\n\n        Args:\n            **kwargs (Any): Additional keyword arguments for the chat completion.\n\n        Returns:\n            Callable: A Spark UDF that takes messages and returns a dictionary with\n                      serialized object and completion.\n        \"\"\"\n\n        def _func(\n            messages: list[ChatCompletionMessageParam],\n        ) -&gt; Tuple[ResponseModel, Union[\"AnthropicCompletion\", DatabricksCompletion, OpenAICompletion]]:\n            return self.create_object_and_completion_from_messages(messages, **kwargs)\n\n        schema = self.model_serializer.spark_schema\n        response_model_name = self.model_serializer.response_model_name\n        assert response_model_name, \"Null response models are not supported by `MessageRouter`\"\n        completion_model_name = self.model_serializer.completion_model_name\n\n        @udf(returnType=schema)\n        def func(messages: list[ChatCompletionMessageParam]) -&gt; Dict[str, Any]:\n            response_model, completion = _func(messages)\n            return {response_model_name: response_model.model_dump(), completion_model_name: completion.model_dump()}\n\n        return func\n</code></pre>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.completion_type","title":"<code>completion_type: Union[Type[AnthropicCompletion], Type[DatabricksCompletion], Type[OpenAICompletion]]</code>  <code>property</code>","text":"<p>Get the appropriate completion type based on the <code>model_class</code> attribute.</p> <p>Returns:</p> Type Description <code>Union[Type[AnthropicCompletion], Type[DatabricksCompletion], Type[OpenAICompletion]]</code> <p>Union[Type[AnthropicCompletion], Type[DatabricksCompletion], Type[OpenAICompletion]]: The completion type corresponding to the model class.</p>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.model_serializer","title":"<code>model_serializer: ModelSerializer</code>  <code>property</code>","text":"<p>Get the model serializer for the response model type and completion type.</p> <p>Returns:</p> Name Type Description <code>ModelSerializer</code> <code>ModelSerializer</code> <p>An instance of ModelSerializer.</p>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.spark_schema","title":"<code>spark_schema: StructType</code>  <code>property</code>","text":"<p>Get the Spark schema for the model.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>The Spark schema corresponding to the model.</p>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialize the <code>model_class</code> if not provided.</p> <p>The <code>model_class</code> will be inferred based on the <code>model</code> attribute.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the ``model_class`` if not provided.\n\n    The ``model_class`` will be inferred based on the ``model`` attribute.\n    \"\"\"\n    warnings.warn(\n        \"`MessageRouter` is deprecated and may be removed in future versions. \" \"Please use `instruct` instead.\",\n        DeprecationWarning,\n    )\n    if self.model_class is None:\n        self.model_class = infer_model_class(self.model)\n</code></pre>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.create_object_and_completion_from_messages","title":"<code>create_object_and_completion_from_messages(messages, **kwargs)</code>","text":"<p>Create a Pydantic object response and completion using the <code>instructor</code> client.</p> <p>The completion will be of the type corresponding to the <code>model_class</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ChatCompletionMessageParam]</code> <p>The list of chat messages.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the chat completion.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[ResponseModel, Union[AnthropicCompletion, DatabricksCompletion, OpenAICompletion]]</code> <p>Tuple[ResponseModel, Union[AnthropicCompletion, DatabricksCompletion, OpenAICompletion]]: A tuple containing the Pydantic object and the completion.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>def create_object_and_completion_from_messages(\n    self, messages: list[ChatCompletionMessageParam], **kwargs: Any\n) -&gt; Tuple[ResponseModel, Union[\"AnthropicCompletion\", DatabricksCompletion, OpenAICompletion]]:\n    \"\"\"Create a Pydantic object response and completion using the ``instructor`` client.\n\n    The completion will be of the type corresponding to the ``model_class`` attribute.\n\n    Args:\n        messages (list[ChatCompletionMessageParam]): The list of chat messages.\n        **kwargs (Any): Additional keyword arguments for the chat completion.\n\n    Returns:\n        Tuple[ResponseModel, Union[AnthropicCompletion, DatabricksCompletion, OpenAICompletion]]:\n            A tuple containing the Pydantic object and the completion.\n    \"\"\"\n    client = self.get_instructor()\n    pydantic_object, completion = client.chat.completions.create_with_completion(\n        model=self.model, response_model=self.response_model_type, messages=messages, **kwargs\n    )\n    return pydantic_object, completion\n</code></pre>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.create_object_and_completion_from_messages_udf","title":"<code>create_object_and_completion_from_messages_udf(**kwargs)</code>","text":"<p>Create a Spark UDF that returns a <code>StructType</code>.</p> <p>The response will be based on the <code>response_model_type</code> and <code>model_class</code> attributes.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the chat completion.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A Spark UDF that takes messages and returns a dictionary with       serialized object and completion.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>def create_object_and_completion_from_messages_udf(self, **kwargs: Any) -&gt; Callable:\n    \"\"\"Create a Spark UDF that returns a ``StructType``.\n\n    The response will be based on the ``response_model_type`` and ``model_class`` attributes.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments for the chat completion.\n\n    Returns:\n        Callable: A Spark UDF that takes messages and returns a dictionary with\n                  serialized object and completion.\n    \"\"\"\n\n    def _func(\n        messages: list[ChatCompletionMessageParam],\n    ) -&gt; Tuple[ResponseModel, Union[\"AnthropicCompletion\", DatabricksCompletion, OpenAICompletion]]:\n        return self.create_object_and_completion_from_messages(messages, **kwargs)\n\n    schema = self.model_serializer.spark_schema\n    response_model_name = self.model_serializer.response_model_name\n    assert response_model_name, \"Null response models are not supported by `MessageRouter`\"\n    completion_model_name = self.model_serializer.completion_model_name\n\n    @udf(returnType=schema)\n    def func(messages: list[ChatCompletionMessageParam]) -&gt; Dict[str, Any]:\n        response_model, completion = _func(messages)\n        return {response_model_name: response_model.model_dump(), completion_model_name: completion.model_dump()}\n\n    return func\n</code></pre>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.create_object_from_messages","title":"<code>create_object_from_messages(messages, **kwargs)</code>","text":"<p>Create a Pydantic object response from chat messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ChatCompletionMessageParam]</code> <p>The list of chat messages.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the chat completion.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ResponseModel</code> <code>ResponseModel</code> <p>A Pydantic object representing the response.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>def create_object_from_messages(self, messages: list[ChatCompletionMessageParam], **kwargs: Any) -&gt; ResponseModel:\n    \"\"\"Create a Pydantic object response from chat messages.\n\n    Args:\n        messages (list[ChatCompletionMessageParam]): The list of chat messages.\n        **kwargs (Any): Additional keyword arguments for the chat completion.\n\n    Returns:\n        ResponseModel: A Pydantic object representing the response.\n    \"\"\"\n    client = self.get_instructor()\n    return client.chat.completions.create(\n        model=self.model, response_model=self.response_model_type, messages=messages, **kwargs\n    )\n</code></pre>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.create_object_from_messages_udf","title":"<code>create_object_from_messages_udf(**kwargs)</code>","text":"<p>Create a Spark UDF that returns a <code>StructType</code> response based on the <code>response_model_type</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the chat completion.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A Spark UDF that takes messages and returns a serialized object.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>def create_object_from_messages_udf(self, **kwargs: Any) -&gt; Callable:\n    \"\"\"Create a Spark UDF that returns a ``StructType`` response based on the ``response_model_type`` attribute.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments for the chat completion.\n\n    Returns:\n        Callable: A Spark UDF that takes messages and returns a serialized object.\n    \"\"\"\n\n    def _func(messages: list[ChatCompletionMessageParam]) -&gt; ResponseModel:\n        return self.create_object_from_messages(messages, **kwargs)\n\n    schema = self.model_serializer.response_model_spark_schema\n    assert schema, \"Null response models are not supported by `MessageRouter`\"\n\n    @udf(returnType=schema)\n    def func(messages: list[ChatCompletionMessageParam]) -&gt; Dict[str, Any]:\n        return _func(messages).model_dump()\n\n    return func\n</code></pre>"},{"location":"reference/udf/message_router/#udf.message_router.MessageRouter.get_instructor","title":"<code>get_instructor()</code>","text":"<p>Get an instance of the instructor client.</p> <p>Returns:</p> Type Description <code>Instructor</code> <p>instructor.Instructor: An initialized instructor client.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>def get_instructor(self) -&gt; instructor.Instructor:\n    \"\"\"Get an instance of the instructor client.\n\n    Returns:\n        instructor.Instructor: An initialized instructor client.\n    \"\"\"\n    return get_instructor(\n        model_class=self.model_class, mode=self.mode, api_key=self.api_key, base_url=self.base_url\n    )\n</code></pre>"},{"location":"reference/udf/message_router/#udf.message_router.ModelSerializer","title":"<code>ModelSerializer</code>  <code>dataclass</code>","text":"<p>A class for serializing Pydantic models to Spark schemas.</p> <p>This class provides functionality to convert Pydantic models to Spark StructType schemas, with fields named according to the snake case version of the model class names.</p> <p>Attributes:</p> Name Type Description <code>response_model_type</code> <code>Type[BaseModel]</code> <p>The Pydantic model type for the main data.</p> <code>completion_model_type</code> <code>Type[BaseModel]</code> <p>The Pydantic model type for the completion data.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>@dataclass\nclass ModelSerializer:\n    \"\"\"A class for serializing Pydantic models to Spark schemas.\n\n    This class provides functionality to convert Pydantic models to Spark StructType schemas,\n    with fields named according to the snake case version of the model class names.\n\n    Attributes:\n        response_model_type (Type[BaseModel]): The Pydantic model type for the main data.\n        completion_model_type (Type[BaseModel]): The Pydantic model type for the completion data.\n    \"\"\"\n\n    response_model_type: Type[BaseModel] | None\n    completion_model_type: Type[BaseModel]\n\n    @staticmethod\n    def to_snake_case(name: str) -&gt; str:\n        \"\"\"Convert a string from camel case to snake case.\n\n        This method takes a camel case string and converts it to snake case.\n        For example, 'CamelCase' becomes 'camel_case'.\n\n        Args:\n            name (str): The camel case string to convert.\n\n        Returns:\n            str: The snake case version of the input string.\n        \"\"\"\n        name = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n        return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name).lower()\n\n    @property\n    def response_model_name(self) -&gt; str | None:\n        \"\"\"Pydantic model field name in snake-case.\"\"\"\n        if self.response_model_type is not None:\n            return self.to_snake_case(self.response_model_type.__name__)\n        return None\n\n    @property\n    def response_model_spark_schema(self) -&gt; StructType | None:\n        \"\"\"Response model spark schema.\"\"\"\n        if self.response_model_type is not None:\n            return create_spark_schema(self.response_model_type)\n        return None\n\n    @property\n    def completion_model_name(self) -&gt; str:\n        \"\"\"Pydantic model field name in snake-case.\"\"\"\n        return self.to_snake_case(self.completion_model_type.__name__)\n\n    @property\n    def completion_model_spark_schema(self) -&gt; StructType:\n        \"\"\"Response model spark schema.\"\"\"\n        return create_spark_schema(self.completion_model_type)\n\n    @property\n    def spark_schema(self) -&gt; StructType:\n        \"\"\"Generate a Spark StructType schema for the Pydantic models.\n\n        This property creates a Spark schema that includes two fields:\n        one for the main Pydantic model and one for the completion model.\n        The field names are derived from the snake case versions of the model class names.\n\n        Returns:\n            StructType: A Spark StructType containing two StructFields, one for each model.\n                        Each field is named after the snake case version of its model class name\n                        and contains the corresponding Spark schema.\n        \"\"\"\n        return (\n            StructType(\n                [\n                    StructField(self.response_model_name, self.response_model_spark_schema, nullable=True),\n                    StructField(self.completion_model_name, self.completion_model_spark_schema, nullable=True),\n                ]\n            )\n            if self.response_model_spark_schema and self.response_model_name\n            else StructType(\n                [\n                    StructField(self.completion_model_name, self.completion_model_spark_schema, nullable=True),\n                ]\n            )\n        )\n</code></pre>"},{"location":"reference/udf/message_router/#udf.message_router.ModelSerializer.completion_model_name","title":"<code>completion_model_name: str</code>  <code>property</code>","text":"<p>Pydantic model field name in snake-case.</p>"},{"location":"reference/udf/message_router/#udf.message_router.ModelSerializer.completion_model_spark_schema","title":"<code>completion_model_spark_schema: StructType</code>  <code>property</code>","text":"<p>Response model spark schema.</p>"},{"location":"reference/udf/message_router/#udf.message_router.ModelSerializer.response_model_name","title":"<code>response_model_name: str | None</code>  <code>property</code>","text":"<p>Pydantic model field name in snake-case.</p>"},{"location":"reference/udf/message_router/#udf.message_router.ModelSerializer.response_model_spark_schema","title":"<code>response_model_spark_schema: StructType | None</code>  <code>property</code>","text":"<p>Response model spark schema.</p>"},{"location":"reference/udf/message_router/#udf.message_router.ModelSerializer.spark_schema","title":"<code>spark_schema: StructType</code>  <code>property</code>","text":"<p>Generate a Spark StructType schema for the Pydantic models.</p> <p>This property creates a Spark schema that includes two fields: one for the main Pydantic model and one for the completion model. The field names are derived from the snake case versions of the model class names.</p> <p>Returns:</p> Name Type Description <code>StructType</code> <code>StructType</code> <p>A Spark StructType containing two StructFields, one for each model.         Each field is named after the snake case version of its model class name         and contains the corresponding Spark schema.</p>"},{"location":"reference/udf/message_router/#udf.message_router.ModelSerializer.to_snake_case","title":"<code>to_snake_case(name)</code>  <code>staticmethod</code>","text":"<p>Convert a string from camel case to snake case.</p> <p>This method takes a camel case string and converts it to snake case. For example, 'CamelCase' becomes 'camel_case'.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The camel case string to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The snake case version of the input string.</p> Source code in <code>spark_instructor/udf/message_router.py</code> <pre><code>@staticmethod\ndef to_snake_case(name: str) -&gt; str:\n    \"\"\"Convert a string from camel case to snake case.\n\n    This method takes a camel case string and converts it to snake case.\n    For example, 'CamelCase' becomes 'camel_case'.\n\n    Args:\n        name (str): The camel case string to convert.\n\n    Returns:\n        str: The snake case version of the input string.\n    \"\"\"\n    name = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name).lower()\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":"<p>General utilities.</p>"},{"location":"reference/utils/env/","title":"env","text":"<p>Module for environment utilities.</p>"},{"location":"reference/utils/env/#utils.env.assert_env_is_set","title":"<code>assert_env_is_set(name)</code>","text":"<p>Assert that an environment variable is set.</p> Source code in <code>spark_instructor/utils/env.py</code> <pre><code>def assert_env_is_set(name: str):\n    \"\"\"Assert that an environment variable is set.\"\"\"\n    assert name in os.environ, f\"``{name}`` is not set!\"\n</code></pre>"},{"location":"reference/utils/env/#utils.env.get_env_variable","title":"<code>get_env_variable(name)</code>","text":"<p>Get environment variable with the given name.</p> Source code in <code>spark_instructor/utils/env.py</code> <pre><code>def get_env_variable(name: str) -&gt; str:\n    \"\"\"Get environment variable with the given name.\"\"\"\n    assert_env_is_set(name)\n    return os.environ[name]\n</code></pre>"},{"location":"reference/utils/image/","title":"image","text":"<p>Utilities for images.</p>"},{"location":"reference/utils/image/#utils.image.convert_image_url_to_image_block_param","title":"<code>convert_image_url_to_image_block_param(image_url)</code>","text":"<p>Convert an ImageURL to an ImageBlockParam.</p> Source code in <code>spark_instructor/utils/image.py</code> <pre><code>def convert_image_url_to_image_block_param(image_url: ImageURL) -&gt; ImageBlockParam:\n    \"\"\"Convert an ImageURL to an ImageBlockParam.\"\"\"\n    url = image_url[\"url\"]\n\n    if is_base64_encoded(url):\n        # Extract the base64 part from the data URL\n        base64_data = url.split(\",\")[1]\n        media_type = cast(\n            Literal[\"image/jpeg\", \"image/png\", \"image/gif\", \"image/webp\"], url.split(\";\")[0].split(\":\")[1]\n        )\n        assert media_type in {\n            \"image/jpeg\",\n            \"image/png\",\n            \"image/gif\",\n            \"image/webp\",\n        }, f\"Unsupported media type {media_type}\"\n    else:\n        base64_data = fetch_and_encode_image(url)\n        media_type = get_media_type(url)\n\n    source: Source = {\"data\": base64_data, \"media_type\": media_type, \"type\": \"base64\"}\n\n    image_block_param: ImageBlockParam = {\"source\": source, \"type\": \"image\"}\n\n    return image_block_param\n</code></pre>"},{"location":"reference/utils/image/#utils.image.fetch_and_encode_image","title":"<code>fetch_and_encode_image(url)</code>","text":"<p>Fetch the image from the URL and encode it in base64.</p> Source code in <code>spark_instructor/utils/image.py</code> <pre><code>def fetch_and_encode_image(url: str) -&gt; str:\n    \"\"\"Fetch the image from the URL and encode it in base64.\"\"\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    image_data = response.content\n    encoded_image = base64.b64encode(image_data).decode(\"utf-8\")\n    return encoded_image\n</code></pre>"},{"location":"reference/utils/image/#utils.image.get_media_type","title":"<code>get_media_type(url)</code>","text":"<p>Determine the media type of the image based on its extension using regex.</p> Source code in <code>spark_instructor/utils/image.py</code> <pre><code>def get_media_type(url: str) -&gt; Literal[\"image/jpeg\", \"image/png\", \"image/gif\", \"image/webp\"]:\n    \"\"\"Determine the media type of the image based on its extension using regex.\"\"\"\n    jpeg_pattern = r\".(jpe?g)\"\n    png_pattern = r\".png\"\n    gif_pattern = r\".gif\"\n    webp_pattern = r\".webp\"\n\n    if re.search(jpeg_pattern, url, re.IGNORECASE):\n        return \"image/jpeg\"\n    elif re.search(png_pattern, url, re.IGNORECASE):\n        return \"image/png\"\n    elif re.search(gif_pattern, url, re.IGNORECASE):\n        return \"image/gif\"\n    elif re.search(webp_pattern, url, re.IGNORECASE):\n        return \"image/webp\"\n    else:\n        raise ValueError(\"Unsupported image format\")\n</code></pre>"},{"location":"reference/utils/image/#utils.image.is_base64_encoded","title":"<code>is_base64_encoded(url)</code>","text":"<p>Check if the given URL is already base64 encoded.</p> Source code in <code>spark_instructor/utils/image.py</code> <pre><code>def is_base64_encoded(url: str) -&gt; bool:\n    \"\"\"Check if the given URL is already base64 encoded.\"\"\"\n    try:\n        if url.startswith(\"data:image/\"):\n            base64_part = url.split(\",\")[1]\n            base64.b64decode(base64_part)\n            return True\n    except Exception as e:\n        warnings.warn(f\"Base64 encoding error encountered: {e}\")\n    return False\n</code></pre>"},{"location":"reference/utils/prompt/","title":"prompt","text":"<p>Utilities for prompt generation.</p>"},{"location":"reference/utils/prompt/#utils.prompt.create_chat_completion_messages","title":"<code>create_chat_completion_messages(messages, strict=True)</code>","text":"<p>Create an array of chat completion message structures from a list of column specifications.</p> <p>This function generates a Spark SQL Column containing an array of structured messages suitable for chat completion tasks. It handles all possible fields of a chat message, including role, content, image URLs, name, tool calls, and tool call IDs. Note that <code>image_urls</code> are NOT included in the <code>content</code> due to spark serialization requiring a static schema.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[SparkChatCompletionColumns]</code> <p>A list of dictionaries, where each dictionary specifies the columns or literal values for different parts of a chat message. The dictionary keys can include: <pre><code>- role (Required[ColumnOrName]): The role of the message (e.g., \"user\", \"assistant\", \"system\").\n- content (ColumnOrName, optional): The text content of the message.\n- image_urls (ColumnOrName, optional): URLs of images associated with the message.\n- name (ColumnOrName, optional): Name associated with the message.\n- tool_calls (ColumnOrName, optional): Tool calls made in the message.\n- tool_call_id (ColumnOrName, optional): ID of the tool call.\n</code></pre></p> required <code>strict</code> <code>bool</code> <p>Whether to make the schema nullability strict. Useful when columns are UDF generated. Recommended when using <code>image_urls</code>.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A Spark SQL Column containing an array of structured messages. Each message</p> <code>Column</code> <p>is cast to the SparkChatCompletionMessage schema.</p> Notes <ul> <li>The function uses the SparkChatCompletionMessage schema to ensure type consistency.</li> <li>Fields not specified in the input will be set to None in the output.</li> <li>This function is particularly useful for creating complex, multi-message prompts   for chat-based language models in a Spark environment.</li> </ul> Example <pre><code>&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; from databricks.connect import DatabricksSession\n&gt;&gt;&gt; spark = DatabricksSession.builder.serverless().getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame([(\"Hello\", \"Be helpful\")], [\"user_msg\", \"sys_msg\"])\n&gt;&gt;&gt; messages = [\n...     {\"role\": f.lit(\"system\"), \"content\": \"sys_msg\"},\n...     {\"role\": f.lit(\"user\"), \"content\": \"user_msg\"}\n... ]\n&gt;&gt;&gt; chat_messages = create_chat_completion_messages(messages)\n&gt;&gt;&gt; df.select(chat_messages.alias(\"messages\")).show(truncate=False)\n+-------------------------------------------------------------------------------------+\n|messages                                                                             |\n+-------------------------------------------------------------------------------------+\n|[{system, Be helpful, NULL, NULL, NULL, NULL}, {user, Hello, NULL, NULL, NULL, NULL}]|\n+-------------------------------------------------------------------------------------+\n&lt;BLANKLINE&gt;\n</code></pre> <p>Raises:     ValueError: If a required field (e.g., 'role') is missing from any message specification.</p> Source code in <code>spark_instructor/utils/prompt.py</code> <pre><code>def create_chat_completion_messages(messages: list[SparkChatCompletionColumns], strict: bool = True) -&gt; Column:\n    \"\"\"Create an array of chat completion message structures from a list of column specifications.\n\n    This function generates a Spark SQL Column containing an array of structured messages\n    suitable for chat completion tasks. It handles all possible fields of a chat message,\n    including role, content, image URLs, name, tool calls, and tool call IDs. Note that ``image_urls``\n    are NOT included in the ``content`` due to spark serialization requiring a static schema.\n\n    Args:\n        messages (list[SparkChatCompletionColumns]): A list of dictionaries, where each dictionary\n            specifies the columns or literal values for different parts of a chat message.\n            The dictionary keys can include:\n            ```markdown\n            - role (Required[ColumnOrName]): The role of the message (e.g., \"user\", \"assistant\", \"system\").\n            - content (ColumnOrName, optional): The text content of the message.\n            - image_urls (ColumnOrName, optional): URLs of images associated with the message.\n            - name (ColumnOrName, optional): Name associated with the message.\n            - tool_calls (ColumnOrName, optional): Tool calls made in the message.\n            - tool_call_id (ColumnOrName, optional): ID of the tool call.\n            ```\n        strict (bool): Whether to make the schema nullability strict. Useful when columns are UDF generated.\n            Recommended when using ``image_urls``.\n\n    Returns:\n        Column: A Spark SQL Column containing an array of structured messages. Each message\n        is cast to the SparkChatCompletionMessage schema.\n\n    Notes:\n        - The function uses the SparkChatCompletionMessage schema to ensure type consistency.\n        - Fields not specified in the input will be set to None in the output.\n        - This function is particularly useful for creating complex, multi-message prompts\n          for chat-based language models in a Spark environment.\n\n    Example:\n        ```python\n\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; from databricks.connect import DatabricksSession\n        &gt;&gt;&gt; spark = DatabricksSession.builder.serverless().getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"Hello\", \"Be helpful\")], [\"user_msg\", \"sys_msg\"])\n        &gt;&gt;&gt; messages = [\n        ...     {\"role\": f.lit(\"system\"), \"content\": \"sys_msg\"},\n        ...     {\"role\": f.lit(\"user\"), \"content\": \"user_msg\"}\n        ... ]\n        &gt;&gt;&gt; chat_messages = create_chat_completion_messages(messages)\n        &gt;&gt;&gt; df.select(chat_messages.alias(\"messages\")).show(truncate=False)\n        +-------------------------------------------------------------------------------------+\n        |messages                                                                             |\n        +-------------------------------------------------------------------------------------+\n        |[{system, Be helpful, NULL, NULL, NULL, NULL}, {user, Hello, NULL, NULL, NULL, NULL}]|\n        +-------------------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        ```\n    Raises:\n        ValueError: If a required field (e.g., 'role') is missing from any message specification.\n    \"\"\"\n    all_keys: list[Literal[\"role\", \"content\", \"image_urls\", \"name\", \"tool_calls\", \"tool_call_id\", \"cache_control\"]] = [\n        \"role\",\n        \"content\",\n        \"image_urls\",\n        \"name\",\n        \"tool_calls\",\n        \"tool_call_id\",\n        \"cache_control\",\n    ]\n    cast_schema = create_spark_schema(SparkChatCompletionMessage)\n    image_urls_type = ArrayType(create_spark_schema(ImageURLPD))\n    tool_calls_type = ArrayType(create_spark_schema(ChatCompletionMessageToolCallParamPD))\n    if not strict:\n        image_urls_type = make_spark_schema_nullable(image_urls_type)\n        tool_calls_type = make_spark_schema_nullable(tool_calls_type)\n        cast_schema = make_spark_schema_nullable(cast_schema)\n\n    def create_struct(message: SparkChatCompletionColumns):\n        struct_fields = []\n        for key in all_keys:\n            if key in message:\n                val = message[key]\n                struct_fields.append(f.col(val).alias(key) if isinstance(val, str) else val.alias(key))\n            else:\n                if key == \"image_urls\":\n                    struct_fields.append(f.lit(None).cast(image_urls_type).alias(key))\n                elif key == \"tool_calls\":\n                    struct_fields.append(f.lit(None).cast(tool_calls_type).alias(key))\n                elif key == \"cache_control\":\n                    struct_fields.append(f.lit(None).cast(BooleanType()).alias(key))\n                else:\n                    struct_fields.append(f.lit(None).cast(StringType()).alias(key))\n        return f.struct(*struct_fields).cast(cast_schema)\n\n    return f.array(*[create_struct(message) for message in messages])\n</code></pre>"},{"location":"reference/utils/prompt/#utils.prompt.get_column_or_null","title":"<code>get_column_or_null(column=None)</code>","text":"<p>Format optional column.</p> Source code in <code>spark_instructor/utils/prompt.py</code> <pre><code>def get_column_or_null(column: ColumnOrName | None = None) -&gt; Column:\n    \"\"\"Format optional column.\"\"\"\n    if column is None:\n        return f.lit(None)\n    if isinstance(column, str):\n        return f.col(column)\n    return column\n</code></pre>"},{"location":"reference/utils/prompt/#utils.prompt.zero_shot_prompt","title":"<code>zero_shot_prompt(user_message_column, system_message_column=None)</code>","text":"<p>Generate a zero-shot prompt for language models in Spark DataFrames.</p> <p>This function creates a structured array of messages suitable for zero-shot prompting in language models. It always includes a user message and optionally a system message.</p> <p>Parameters:</p> Name Type Description Default <code>user_message_column</code> <code>Union[Column, str]</code> <p>The column containing user messages. Can be either a Column object or a string column name.</p> required <code>system_message_column</code> <code>Optional[Union[Column, str]]</code> <p>The column containing system messages. Can be either a Column object or a string column name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A Spark SQL Column containing an array of message structures. Each structure is a map with 'role' and 'content' keys.</p> Notes <ul> <li>If system_message_column is None, only the user message is included.</li> <li>If system_message_column is provided, the system message and user message are included.</li> </ul> Example <pre><code>&gt;&gt;&gt; from databricks.connect import DatabricksSession\n&gt;&gt;&gt; spark = DatabricksSession.builder.serverless().getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame([(\"Hello\", \"Be helpful\")], [\"user_msg\", \"sys_msg\"])\n&gt;&gt;&gt; prompt_col = zero_shot_prompt(\"user_msg\", system_message_column=\"sys_msg\")\n&gt;&gt;&gt; df.select(prompt_col.alias(\"prompt\")).show(truncate=False)\n+---------------------------------------------------------------------------+\n|prompt                                                                     |\n+---------------------------------------------------------------------------+\n|[{role -&gt; system, content -&gt; Be helpful}, {role -&gt; user, content -&gt; Hello}]|\n+---------------------------------------------------------------------------+\n&lt;BLANKLINE&gt;\n</code></pre> Source code in <code>spark_instructor/utils/prompt.py</code> <pre><code>def zero_shot_prompt(\n    user_message_column: ColumnOrName,\n    system_message_column: Optional[ColumnOrName] = None,\n) -&gt; Column:\n    \"\"\"Generate a zero-shot prompt for language models in Spark DataFrames.\n\n    This function creates a structured array of messages suitable for zero-shot prompting\n    in language models. It always includes a user message and optionally a system message.\n\n    Args:\n        user_message_column (Union[Column, str]): The column containing user messages.\n            Can be either a Column object or a string column name.\n        system_message_column (Optional[Union[Column, str]], optional): The column containing\n            system messages. Can be either a Column object or a string column name. Defaults to None.\n\n    Returns:\n        Column: A Spark SQL Column containing an array of message structures.\n            Each structure is a map with 'role' and 'content' keys.\n\n    Notes:\n        - If system_message_column is None, only the user message is included.\n        - If system_message_column is provided, the system message and user message are included.\n\n    Example:\n        ```python\n\n        &gt;&gt;&gt; from databricks.connect import DatabricksSession\n        &gt;&gt;&gt; spark = DatabricksSession.builder.serverless().getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"Hello\", \"Be helpful\")], [\"user_msg\", \"sys_msg\"])\n        &gt;&gt;&gt; prompt_col = zero_shot_prompt(\"user_msg\", system_message_column=\"sys_msg\")\n        &gt;&gt;&gt; df.select(prompt_col.alias(\"prompt\")).show(truncate=False)\n        +---------------------------------------------------------------------------+\n        |prompt                                                                     |\n        +---------------------------------------------------------------------------+\n        |[{role -&gt; system, content -&gt; Be helpful}, {role -&gt; user, content -&gt; Hello}]|\n        +---------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        ```\n    \"\"\"\n    role_column = f.lit(\"role\")\n    content_column = f.lit(\"content\")\n    user_map = f.create_map(role_column, f.lit(\"user\"), content_column, user_message_column)\n\n    if system_message_column is not None:\n        system_map = f.create_map(role_column, f.lit(\"system\"), content_column, system_message_column)\n        return f.array(system_map, user_map)\n    return f.array(user_map)\n</code></pre>"},{"location":"reference/utils/types/","title":"types","text":"<p>Module for handling type conversion.</p>"},{"location":"reference/utils/types/#utils.types.make_nullable_array","title":"<code>make_nullable_array(schema)</code>","text":"<p>Make an array nullable.</p> Source code in <code>spark_instructor/utils/types.py</code> <pre><code>def make_nullable_array(schema: ArrayType) -&gt; ArrayType:\n    \"\"\"Make an array nullable.\"\"\"\n    return ArrayType(make_spark_schema_nullable(schema.elementType), True)\n</code></pre>"},{"location":"reference/utils/types/#utils.types.make_nullable_field","title":"<code>make_nullable_field(schema)</code>","text":"<p>Make a field nullable.</p> Source code in <code>spark_instructor/utils/types.py</code> <pre><code>def make_nullable_field(schema: StructField) -&gt; StructField:\n    \"\"\"Make a field nullable.\"\"\"\n    return StructField(schema.name, make_spark_schema_nullable(schema.dataType), nullable=True)\n</code></pre>"},{"location":"reference/utils/types/#utils.types.make_nullable_struct","title":"<code>make_nullable_struct(schema)</code>","text":"<p>Make a struct nullable.</p> Source code in <code>spark_instructor/utils/types.py</code> <pre><code>def make_nullable_struct(schema: StructType) -&gt; StructType:\n    \"\"\"Make a struct nullable.\"\"\"\n    return StructType([make_spark_schema_nullable(field) for field in schema.fields])\n</code></pre>"},{"location":"reference/utils/types/#utils.types.make_spark_schema_nullable","title":"<code>make_spark_schema_nullable(schema)</code>","text":"<p>Make a spark type nullable.</p> Source code in <code>spark_instructor/utils/types.py</code> <pre><code>def make_spark_schema_nullable(schema: T_Spark) -&gt; T_Spark:\n    \"\"\"Make a spark type nullable.\"\"\"\n    if isinstance(schema, StructType):\n        return make_nullable_struct(schema)\n    elif isinstance(schema, StructField):\n        return make_nullable_field(schema)\n    elif isinstance(schema, ArrayType):\n        return make_nullable_array(schema)\n    else:\n        return schema\n</code></pre>"},{"location":"reference/utils/types/#utils.types.pydantic_to_typeddict","title":"<code>pydantic_to_typeddict(pydantic_model, return_type, all_required=False)</code>","text":"<p>Convert a pydantic model to a typed dict.</p> Source code in <code>spark_instructor/utils/types.py</code> <pre><code>def pydantic_to_typeddict(pydantic_model: BaseModel, return_type: Type[T], all_required: bool = False) -&gt; T:\n    \"\"\"Convert a pydantic model to a typed dict.\"\"\"\n    result = pydantic_model.model_dump()\n    if not all_required:\n        result = _remove_none_from_dict(result)\n\n    return return_type(**result)\n</code></pre>"},{"location":"reference/utils/types/#utils.types.typeddict_to_pydantic","title":"<code>typeddict_to_pydantic(typeddict_class)</code>","text":"<p>Convert a TypedDict to a Pydantic model using TypeAdapter and core_schema.</p> <p>Parameters:</p> Name Type Description Default <code>typeddict_class</code> <code>Type[Any]</code> <p>The TypedDict class to convert.</p> required <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>Type[BaseModel]: A Pydantic model class equivalent to the input TypedDict.</p> Example <pre><code>&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt; from typing import TypedDict\n&gt;&gt;&gt; class MyTypedDict(TypedDict):\n...     name: str\n...     age: int\n&gt;&gt;&gt; PydanticModel = typeddict_to_pydantic(MyTypedDict)\n&gt;&gt;&gt; isinstance(PydanticModel(name=\"John\", age=30), BaseModel)\nTrue\n</code></pre> Source code in <code>spark_instructor/utils/types.py</code> <pre><code>def typeddict_to_pydantic(typeddict_class: Type[Any]) -&gt; Type[BaseModel]:\n    \"\"\"Convert a TypedDict to a Pydantic model using TypeAdapter and core_schema.\n\n    Args:\n        typeddict_class (Type[Any]): The TypedDict class to convert.\n\n    Returns:\n        Type[BaseModel]: A Pydantic model class equivalent to the input TypedDict.\n\n    Example:\n        ```python\n\n        &gt;&gt;&gt; from pydantic import BaseModel\n        &gt;&gt;&gt; from typing import TypedDict\n        &gt;&gt;&gt; class MyTypedDict(TypedDict):\n        ...     name: str\n        ...     age: int\n        &gt;&gt;&gt; PydanticModel = typeddict_to_pydantic(MyTypedDict)\n        &gt;&gt;&gt; isinstance(PydanticModel(name=\"John\", age=30), BaseModel)\n        True\n\n        ```\n    \"\"\"\n    adapter = TypeAdapter(typeddict_class)\n    core_schema = adapter.core_schema\n\n    fields: Dict[str, tuple[Any, FieldInfo]] = {}\n    _process_schema(core_schema, fields)\n\n    return create_model(f\"{typeddict_class.__name__}PD\", __doc__=typeddict_class.__doc__, **fields)  # type: ignore\n</code></pre>"}]}